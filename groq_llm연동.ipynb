{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5434d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4b78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70202ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"human\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"LangServe는 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff33d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002078E36A510> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002078E36AF90> root_client=<openai.OpenAI object at 0x000002078E207620> root_async_client=<openai.AsyncOpenAI object at 0x000002078E36ACF0> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1a5303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangServe는 개발자가 대규모 언어 모델(LLM)을 쉽게 배포하고 관리할 수 있도록 지원하는 오픈 소스 라이브러리입니다. LangServe를 사용하면 개발자는 언어 모델을 API 서버로 쉽게 전환하여 다양한 애플리케이션에서 모델을 사용할 수 있습니다.\\n\\nLangServe의 주요 기능은 다음과 같습니다:\\n\\n1. **모델 배포**: LangServe를 사용하면 개발자는 훈련된 언어 모델을 쉽게 배포할 수 있습니다. 모델을 API 서버로 전환하면 다양한 애플리케이션에서 모델을 사용할 수 있습니다.\\n\\n2. **API 서버**: LangServe는 언어 모델을 API 서버로 전환하여 클라이언트 애플리케이션이 모델에 쉽게 접근할 수 있도록 합니다. 이를 통해 개발자는 모델을 웹 애플리케이션, 모바일 애플리케이션, 또는 다른 시스템과 통합할 수 있습니다.\\n\\n3. **모델 관리**: LangServe는 모델의 버전 관리, 업데이트, 모니터링 등을 지원하여 모델을 효율적으로 관리할 수 있습니다.\\n\\n4. **확장성**: LangServe는 수평 확장성을 지원하여 대규모 트래픽을 처리할 수 있습니다.\\n\\n5. **보안**: LangServe는 데이터 암호화, 접근 제어 등의 보안 기능을 제공하여 모델과 데이터를 안전하게 보호합니다.\\n\\n6. **다양한 모델 지원**: LangServe는 다양한 언어 모델을 지원하며, 특히 Hugging Face의 Transformers 라이브러리와 긴밀하게 통합됩니다.\\n\\nLangServe를 사용하는 이유는 다음과 같습니다:\\n\\n* 대규모 언어 모델을 쉽게 배포하고 관리할 수 있습니다.\\n* 언어 모델을 API 서버로 전환하여 다양한 애플리케이션에서 사용할 수 있습니다.\\n* 모델의 버전 관리, 업데이트, 모니터링 등을 쉽게 할 수 있습니다.\\n* 확장성과 보안이 뛰어납니다.\\n\\nLangServe의 사용 사례는 다음과 같습니다:\\n\\n* 챗봇 개발: LangServe를 사용하여 언어 모델을 배포하고 관리함으로써 챗봇을 개발할 수 있습니다.\\n* 언어 번역: LangServe를 사용하여 언어 번역 모델을 배포하고 관리함으로써 번역 서비스를 제공할 수 있습니다.\\n* 텍스트 요약: LangServe를 사용하여 텍스트 요약 모델을 배포하고 관리함으로써 기사 또는 문서의 요약을 제공할 수 있습니다.\\n\\n결론적으로, LangServe는 대규모 언어 모델을 쉽게 배포하고 관리할 수 있는 오픈 소스 라이브러리입니다. 이를 통해 개발자는 언어 모델을 API 서버로 전환하여 다양한 애플리케이션에서 사용할 수 있으며, 모델의 버전 관리, 업데이트, 모니터링 등을 쉽게 할 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 475, 'prompt_tokens': 30, 'total_tokens': 505, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.300551306, 'prompt_time': 0.003034687, 'completion_time': 1.154676738, 'total_time': 1.157711425}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-ac5bad85-9f10-4e4b-b446-5a908d65bdb7', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--3b699040-9e27-47c8-8a24-698fbeed6ec9-0' usage_metadata={'input_tokens': 30, 'output_tokens': 475, 'total_tokens': 505, 'input_token_details': {}, 'output_token_details': {}}\n",
      "응답: LangServe는 개발자가 대규모 언어 모델(LLM)을 쉽게 배포하고 관리할 수 있도록 지원하는 오픈 소스 라이브러리입니다. LangServe를 사용하면 개발자는 언어 모델을 API 서버로 쉽게 전환하여 다양한 애플리케이션에서 모델을 사용할 수 있습니다.\n",
      "\n",
      "LangServe의 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **모델 배포**: LangServe를 사용하면 개발자는 훈련된 언어 모델을 쉽게 배포할 수 있습니다. 모델을 API 서버로 전환하면 다양한 애플리케이션에서 모델을 사용할 수 있습니다.\n",
      "\n",
      "2. **API 서버**: LangServe는 언어 모델을 API 서버로 전환하여 클라이언트 애플리케이션이 모델에 쉽게 접근할 수 있도록 합니다. 이를 통해 개발자는 모델을 웹 애플리케이션, 모바일 애플리케이션, 또는 다른 시스템과 통합할 수 있습니다.\n",
      "\n",
      "3. **모델 관리**: LangServe는 모델의 버전 관리, 업데이트, 모니터링 등을 지원하여 모델을 효율적으로 관리할 수 있습니다.\n",
      "\n",
      "4. **확장성**: LangServe는 수평 확장성을 지원하여 대규모 트래픽을 처리할 수 있습니다.\n",
      "\n",
      "5. **보안**: LangServe는 데이터 암호화, 접근 제어 등의 보안 기능을 제공하여 모델과 데이터를 안전하게 보호합니다.\n",
      "\n",
      "6. **다양한 모델 지원**: LangServe는 다양한 언어 모델을 지원하며, 특히 Hugging Face의 Transformers 라이브러리와 긴밀하게 통합됩니다.\n",
      "\n",
      "LangServe를 사용하는 이유는 다음과 같습니다:\n",
      "\n",
      "* 대규모 언어 모델을 쉽게 배포하고 관리할 수 있습니다.\n",
      "* 언어 모델을 API 서버로 전환하여 다양한 애플리케이션에서 사용할 수 있습니다.\n",
      "* 모델의 버전 관리, 업데이트, 모니터링 등을 쉽게 할 수 있습니다.\n",
      "* 확장성과 보안이 뛰어납니다.\n",
      "\n",
      "LangServe의 사용 사례는 다음과 같습니다:\n",
      "\n",
      "* 챗봇 개발: LangServe를 사용하여 언어 모델을 배포하고 관리함으로써 챗봇을 개발할 수 있습니다.\n",
      "* 언어 번역: LangServe를 사용하여 언어 번역 모델을 배포하고 관리함으로써 번역 서비스를 제공할 수 있습니다.\n",
      "* 텍스트 요약: LangServe를 사용하여 텍스트 요약 모델을 배포하고 관리함으로써 기사 또는 문서의 요약을 제공할 수 있습니다.\n",
      "\n",
      "결론적으로, LangServe는 대규모 언어 모델을 쉽게 배포하고 관리할 수 있는 오픈 소스 라이브러리입니다. 이를 통해 개발자는 언어 모델을 API 서버로 전환하여 다양한 애플리케이션에서 사용할 수 있으며, 모델의 버전 관리, 업데이트, 모니터링 등을 쉽게 할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(response)\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafbd5a1",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c34c649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}에 대해 쉽게 반드시 한글로 설명해주세요.\")\\n    ')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 반드시 한글로 설명해주세요.\")\n",
    "    \"\"\")                                     \n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d392dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbba4772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain 연결 (LCEL)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084bfec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "인공지능 모델의 학습 원리는 컴퓨터가 데이터를 통해 스스로 학습하고, 이를 바탕으로 미래의 새로운 데이터에 대해 예측하거나 결정을 내릴 수 있도록 하는 것입니다. 이를 위해 인공지능 모델은 주어진 데이터를 분석하고, 패턴을 발견하며, 규칙을 학습합니다.\n",
      "\n",
      "가장 기본적인 학습 원리는 다음과 같습니다.\n",
      "\n",
      "1. **데이터 수집**: 인공지능 모델을 학습시키기 위해서는 많은 양의 데이터가 필요합니다. 이 데이터는 문제에 따라 달라지며, 이미지, 텍스트, 오디오 등 다양한 형태일 수 있습니다.\n",
      "\n",
      "2. **데이터 전처리**: 수집된 데이터는 모델에 입력되기 전에 전처리 과정을 거칩니다. 이 과정에서는 데이터의 잡음을 제거하거나, 데이터를 정규화하는 등의 작업이 이루어집니다.\n",
      "\n",
      "3. **모델 선택**: 인공지능 모델에는 여러 가지 종류가 있습니다. 예를 들어, 이미지 인식에는 합성곱 신경망(CNN), 자연어 처리에는 순환 신경망(RNN) 또는 트랜스포머 등이 사용됩니다. 적합한 모델을 선택하는 것이 중요합니다.\n",
      "\n",
      "4. **학습**: 선택된 모델에 전처리된 데이터를 입력하여 학습을 시작합니다. 이 과정에서는 모델이 데이터의 패턴을 인식하고, 이에 맞춰 파라미터를 조정합니다. 학습 과정은 보통 최적화 알고리즘을 통해 이루어지며, 모델의 예측 결과와 실제 값 사이의 오류를 최소화하는 방향으로 파라미터가 조정됩니다.\n",
      "\n",
      "5. **평가**: 학습이 완료된 후, 모델의 성능을 평가합니다. 이를 위해 별도의 테스트 데이터를 사용하며, 모델의 예측 정확도, 오차율 등을 계산합니다.\n",
      "\n",
      "6. **튜닝**: 모델의 성능이 만족스럽지 않을 경우, 하이퍼파라미터를 조정하거나 모델 구조를 변경하는 등의 튜닝 과정을 거칩니다.\n",
      "\n",
      "예를 들어, 어린 아이에게 사과와 바나나의 사진을 보여주고 이 둘을 구별하라고 하면, 처음에는 구별하지 못할 것입니다. 하지만 여러 번 사과와 바나나를 보여주고, 이것이 사과이고, 이것이 바나나라고 설명해 주면, 아이는 어느 순간부터는 사과와 바나나를 구별할 수 있게 됩니다. 인공지능 모델의 학습 원리도 이와 유사합니다. 모델에게 많은 데이터를 제공하고, 이것이 어떤 것인지를 설명해 주면, 모델은 스스로 학습하여 새로운 사과와 바나나의 사진을 구별할 수 있게 됩니다.\n",
      "\n",
      "이러한 학습 원리는 다양한 분야에서 활용되고 있으며, 자율 주행 자동차, 의료 진단, 언어 번역 등 많은 응용 분야에서 인공지능 기술이 사용되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33153ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \": LangChain의 Products(제품)는 어떤 것들이 있나요? 예를 들어 LangSmith, LangServe 같은 Product가 있어\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d50087",
   "metadata": {},
   "source": [
    "\n",
    "###Runnable의 stream() 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfabd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해 주세요.\"})\n",
    "    \n",
    "    # 스트리밍 출력\n",
    "    #print(answer)\n",
    "    for token in answer:\n",
    "        # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "        print(token, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a759f26",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "* 첫번째 Chain의 출력이, 두번째 Chain의 입력이 된다.\n",
    "* 두개의 Chain과 Prompt + OutputParser를 LCEL로 연결하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acf368a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001BD5B7D96D0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001BD5B7DB750> root_client=<openai.OpenAI object at 0x000001BD5B7DB110> root_async_client=<openai.AsyncOpenAI object at 0x000001BD5B7DB890> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 15문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbbc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1의 출력을 movie 입력 변수로 전달\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "    response = chain2.invoke({\"genre\": \"성인\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eab14",
   "metadata": {},
   "source": [
    "## PromptTemplate 여러개 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1595b579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 2 문장으로 한국어로 답변해 주세요.', 'Gemma 모델의 학습 원리를 3 문장으로 한국어로 답변해 주세요.', 'llama*4 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama*4\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3747ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt) #AIMessage\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da13a8",
   "metadata": {},
   "source": [
    "### System_messagePromptTemplate\n",
    "* HumanMessagePromptTemplate, AIMessagePromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092be190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"당신은 {topic} 전문가 입니다다. 명확하고 자세하게 설명해 주세요.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"딥러닝은 무엇인가요?\")\n",
    "\n",
    "# LLM 호출\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efed56",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "* 프롬프트에서 예시를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39112b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 태양계의 행성\n",
      "\n",
      "1. **수성**: 태양과 가장 가까운 행성으로, 매우 작은 크기와 높은 온도를 가지고 있습니다.\n",
      "2. **금성**: 밝고 뜨거운 행성으로, 강한 온실 효과로 인해 표면 온도가 매우 높습니다.\n",
      "3. **지구**: 생명체가 존재하는 유일한 행성으로, 대기 구성과 물이 있어 생명 유지에 적합합니다.\n",
      "4. **화성**: 붉은 행성으로, 과거에 물이 있었을 것으로 추정되며, 현재는 암석과 모래로 덮여 있습니다.\n",
      "5. **목성**: 태양계에서 가장 큰 행성으로, 가스 거인이며 수많은 위성을 가지고 있습니다.\n",
      "6. **토성**: 아름다운 반지로 유명한 가스 거인으로, 많은 위성을 가지고 있습니다.\n",
      "7. **천왕성**: 얼음 거인으로, 자전축이 기울어져 있어 극단적인 계절 변화를 경험합니다.\n",
      "8. **해왕성**: 가장 먼 행성으로, 강한 바람과 깊은 푸른색을 띠는 대기층을 가지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"뉴턴의 운동 법칙을 요약해 주세요.\",\n",
    "        \"output\": \"\"\"### 뉴턴의 운동 법칙\n",
    "1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\n",
    "2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\n",
    "3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"지구의 대기 구성 요소를 알려주세요.\",\n",
    "        \"output\": \"\"\"### 지구 대기의 구성\n",
    "- **질소 (78%)**: 대기의 대부분을 차지합니다.\n",
    "- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\n",
    "- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\n",
    "- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate 적용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 구성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 초등학생도 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 생성 및 체인 구성\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# 테스트 실행\n",
    "result = chain.invoke({\"input\": \"태양계의 행성들을 간략히 정리해 주세요.\"})\n",
    "#result = chain.invoke({\"input\": \"양자 얽힘이 무엇인가요?\"})\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
