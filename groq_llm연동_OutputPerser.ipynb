{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574aa460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "%pip install -q langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9d955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f81ddb",
   "metadata": {},
   "source": [
    "# CommaSeparatedListOutputParser\n",
    "\n",
    "**CommaSeparatedListOutputParser**ëŠ” **ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ëª©ë¡ì„ ë°˜í™˜í•  ë•Œ ìœ ìš©í•œ ì¶œë ¥ íŒŒì„œ** ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë¥¼ í™œìš©í•˜ë©´ **í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‰½ê²Œ ë³€í™˜**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b862edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "# ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# ì¶œë ¥ í˜•ì‹ ì§€ì¹¨ ê°€ì ¸ì˜¤ê¸°\n",
    "format_instructions = output_parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b61675f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI ê´€ë ¨ ê¸°ìˆ  ëª©ë¡:\n",
      "['Machine Learning', 'Deep Learning', 'Natural Language Processing', 'Computer Vision', 'Robotics']\n",
      " './data/ai_technologies.csv' íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì„¤ì •\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸, ëª¨ë¸, ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ì—¬ ì²´ì¸ ìƒì„±\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# \"AI ê´€ë ¨ ê¸°ìˆ \"ì— ëŒ€í•œ ì²´ì¸ í˜¸ì¶œ ì‹¤í–‰\n",
    "result = chain.invoke({\"subject\": \"AI ê´€ë ¨ ê¸°ìˆ \"})\n",
    "\n",
    "# ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\n",
    "print(\" AI ê´€ë ¨ ê¸°ìˆ  ëª©ë¡:\")\n",
    "print(result)\n",
    "\n",
    "# ê²°ê³¼ í™œìš© ì˜ˆì‹œ: CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "csv_filename = \"./data/ai_technologies.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"AI ê¸°ìˆ \"])  # í—¤ë” ì¶”ê°€\n",
    "    for item in result:\n",
    "        writer.writerow([item])\n",
    "\n",
    "print(f\" '{csv_filename}' íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62623f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "\n",
    "# JSON ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ê³¼í•™ ë¶„ì•¼ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì²´ê³„ì ì´ê³  ê°„ê²°í•œ ë‹µë³€ì„ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.\"),\n",
    "        (\"user\", \"#Format: {format_instructions}\\n\\n#Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# JSON ì¶œë ¥ í˜•ì‹ ì§€ì¹¨ì„ í”„ë¡¬í”„íŠ¸ì— ì ìš©\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f122682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"mission_name\": \"ë‰´í˜¸ë¼ì´ì¦ŒìŠ¤\",\n",
      "        \"goal\": \"ëª…ì™•ì„± íƒì‚¬\",\n",
      "        \"agency\": \"NASA\"\n",
      "    },\n",
      "    {\n",
      "        \"mission_name\": \"ì¹´ì‹œë‹ˆ-í˜¸ì´ê²ìŠ¤\",\n",
      "        \"goal\": \"í† ì„±ì˜ ìœ„ì„± íƒ€ì´íƒ„ íƒì‚¬\",\n",
      "        \"agency\": \"NASA, ESA, ì´íƒˆë¦¬ì•„ ìš°ì£¼êµ­\"\n",
      "    },\n",
      "    {\n",
      "        \"mission_name\": \"ì°½ì–´ 4í˜¸\",\n",
      "        \"goal\": \"ë‹¬ì˜ ë’·ë©´ íƒì‚¬\",\n",
      "        \"agency\": \"ì¤‘êµ­ ìš°ì£¼êµ­\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# OpenAI ëª¨ë¸ ì„¤ì •\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸, ëª¨ë¸, ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ëŠ” ì²´ì¸ ìƒì„±\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# ì§ˆë¬¸ ì„¤ì • (ìš°ì£¼ íƒì‚¬ ê´€ë ¨ ì§ˆë¬¸)\n",
    "question = \"ìµœê·¼ 10ë…„ê°„ ì§„í–‰ëœ ì£¼ìš” ìš°ì£¼ íƒì‚¬ ë¯¸ì…˜ 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. \\\n",
    "ê° ë¯¸ì…˜ì˜ ì´ë¦„ì€ `mission_name`ì—, ëª©í‘œëŠ” `goal`ì—, ì£¼ê´€ ê¸°ê´€ì€ `agency`ì— ë‹´ì•„ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ ë° JSON ì‘ë‹µ ë°›ê¸°\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# JSON ë°ì´í„° ì¶œë ¥\n",
    "print(json.dumps(response, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557db08",
   "metadata": {},
   "source": [
    "\n",
    "### PandasDataFrameOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be7783cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format Instructions:\n",
      " The output should be formatted as a string as the operation, followed by a colon, followed by the column or row to be queried on, followed by optional array parameters.\n",
      "1. The column names are limited to the possible columns below.\n",
      "2. Arrays must either be a comma-separated list of numbers formatted as [1,3,5], or it must be in range of numbers formatted as [0..4].\n",
      "3. Remember that arrays are optional and not necessarily required.\n",
      "4. If the column is not in the possible columns or the operation is not a valid Pandas DataFrame operation, return why it is invalid as a sentence starting with either \"Invalid column\" or \"Invalid operation\".\n",
      "\n",
      "As an example, for the formats:\n",
      "1. String \"column:num_legs\" is a well-formatted instance which gets the column num_legs, where num_legs is a possible column.\n",
      "2. String \"row:1\" is a well-formatted instance which gets row 1.\n",
      "3. String \"column:num_legs[1,2]\" is a well-formatted instance which gets the column num_legs for rows 1 and 2, where num_legs is a possible column.\n",
      "4. String \"row:1[num_legs]\" is a well-formatted instance which gets row 1, but for just column num_legs, where num_legs is a possible column.\n",
      "5. String \"mean:num_legs[1..3]\" is a well-formatted instance which takes the mean of num_legs from rows 1 to 3, where num_legs is a possible column and mean is a valid Pandas DataFrame operation.\n",
      "6. String \"do_something:num_legs\" is a badly-formatted instance, where do_something is not a valid Pandas DataFrame operation.\n",
      "7. String \"mean:invalid_col\" is a badly-formatted instance, where invalid_col is not a possible column.\n",
      "\n",
      "Here are the possible columns:\n",
      "```\n",
      "Survived, Pclass, Name, Sex, Age, Siblings/Spouses Aboard, Parents/Children Aboard, Fare\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import re\n",
    "\n",
    "# Titanic ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "# Pandas DataFrame Output Parser ì„¤ì •\n",
    "parser = PandasDataFrameOutputParser(dataframe=df)\n",
    "\n",
    "# í˜•ì‹ ì§€ì¹¨ ì¶œë ¥\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(\"Format Instructions:\\n\", format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d49127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived, Pclass, Name, Sex, Age, Siblings/Spouses Aboard, Parents/Children Aboard, Fare\n"
     ]
    }
   ],
   "source": [
    "# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "# model = ChatOpenAI(\n",
    "#     base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "#     model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "#     temperature=0\n",
    "# )\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" \n",
    "    You are a helpful assistant that interacts with a Pandas DataFrame.\n",
    "    The DataFrame contains the following columns: {columns}.\n",
    "    \n",
    "    Your task is to answer the user's query by generating a command in the following format:\n",
    "    {format_instructions}\n",
    "    \n",
    "    User Query: {query}    \n",
    "    \"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": format_instructions,\n",
    "        \"columns\": \", \".join(df.columns)\n",
    "    },\n",
    ")\n",
    "print(prompt.partial_variables['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a318672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name ì»¬ëŸ¼ ì¶œë ¥\n",
      "<class 'dict'>\n",
      "{'Name': 0                                 Mr. Owen Harris Braund\n",
      "1      Mrs. John Bradley (Florence Briggs Thayer) Cum...\n",
      "2                                  Miss. Laina Heikkinen\n",
      "3            Mrs. Jacques Heath (Lily May Peel) Futrelle\n",
      "4                                Mr. William Henry Allen\n",
      "                             ...                        \n",
      "882                                 Rev. Juozas Montvila\n",
      "883                          Miss. Margaret Edith Graham\n",
      "884                       Miss. Catherine Helen Johnston\n",
      "885                                 Mr. Karl Howell Behr\n",
      "886                                   Mr. Patrick Dooley\n",
      "Name: Name, Length: 887, dtype: object}\n",
      "ì²«ë²ˆì§¸ í–‰ ì¶œë ¥\n",
      "{'0': Survived                                        0\n",
      "Pclass                                          3\n",
      "Name                       Mr. Owen Harris Braund\n",
      "Sex                                          male\n",
      "Age                                          22.0\n",
      "Siblings/Spouses Aboard                         1\n",
      "Parents/Children Aboard                         0\n",
      "Fare                                         7.25\n",
      "Name: 0, dtype: object}\n"
     ]
    }
   ],
   "source": [
    "# ì²´ì¸ ìƒì„±\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# ëª¨ë¸ ì‘ë‹µ ë°›ê¸°\n",
    "try:\n",
    "    # **Name ì—´ì„ í‘œì‹œí•˜ì‹­ì‹œì˜¤.**\n",
    "    print('Name ì»¬ëŸ¼ ì¶œë ¥')\n",
    "    df_query = \"Show the Name column\"\n",
    "\n",
    "    parser_output = chain.invoke({\"query\": df_query})\n",
    "    print(type(parser_output))\n",
    "    print(parser_output)\n",
    "\n",
    "        # **ì²«ë²ˆì§¸ í–‰ì„ í‘œì‹œí•˜ì‹­ì‹œì˜¤.**\n",
    "    print('ì²«ë²ˆì§¸ í–‰ ì¶œë ¥')\n",
    "    df_query2 = \"ì²«ë²ˆìŸ¤ í–‰(row)ì„ ë³´ì—¬ì£¼ì„¸ìš”.\"\n",
    "\n",
    "    parser_output2 = chain.invoke({\"query\": df_query2})\n",
    "    print(parser_output2)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89271929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] input_types={} partial_variables={'format_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"data\": string  // A list of dictionaries representing table rows.\\n}\\n```'} template='\\n    You are an AI assistant that generates tabular data. \\n    You must return the data in JSON format that follows this schema:\\n\\n    {format_instructions}\\n\\n    **User Query:**\\n    {query}\\n    '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "# model = ChatOpenAI(\n",
    "#     base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "#     model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "#     temperature=0\n",
    "# )\n",
    "\n",
    "# ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜  (data : {}, {},{} )\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"data\", description=\"A list of dictionaries representing table rows.\"),\n",
    "]\n",
    "\n",
    "# Output Parser ì„¤ì •\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an AI assistant that generates tabular data. \n",
    "    You must return the data in JSON format that follows this schema:\n",
    "    \n",
    "    {format_instructions}\n",
    "        \n",
    "    **User Query:**\n",
    "    {query}\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a925ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ì²´ì¸ ìƒì„± (í”„ë¡¬í”„íŠ¸ â†’ ëª¨ë¸ â†’ OutputParser)\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# ì‹¤í–‰ í•¨ìˆ˜\n",
    "def generate_dataframe(user_query):\n",
    "    try:\n",
    "        # ëª¨ë¸ í˜¸ì¶œ\n",
    "        json_response = chain.invoke({\"query\": user_query})\n",
    "\n",
    "        # ëª¨ë¸ì´ ë°˜í™˜í•œ JSONì„ Pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        df = pd.DataFrame(json_response[\"data\"])\n",
    "\n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(\"\\nğŸ”¹ Generated DataFrame:\\n\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75be0104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024ë…„ í•˜ë°˜ê¸° ì„œìš¸ìš¸ ì•„íŒŒíŠ¸ í‰ê·  ë§¤ë§¤ ê°€ê²© ë°ì´í„° ìƒì„±\n",
      "\n",
      "ğŸ”¹ Generated DataFrame:\n",
      "\n",
      "(10, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>District</th>\n",
       "      <th>Average Price</th>\n",
       "      <th>Number of Transactions</th>\n",
       "      <th>Year-over-Year Change (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gangnam-gu</td>\n",
       "      <td>1500000000</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jongno-gu</td>\n",
       "      <td>950000000</td>\n",
       "      <td>800</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mapo-gu</td>\n",
       "      <td>1100000000</td>\n",
       "      <td>950</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Songpa-gu</td>\n",
       "      <td>1300000000</td>\n",
       "      <td>1100</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yongsan-gu</td>\n",
       "      <td>1400000000</td>\n",
       "      <td>700</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     District  Average Price  Number of Transactions  \\\n",
       "0  Gangnam-gu     1500000000                    1200   \n",
       "1   Jongno-gu      950000000                     800   \n",
       "2     Mapo-gu     1100000000                     950   \n",
       "3   Songpa-gu     1300000000                    1100   \n",
       "4  Yongsan-gu     1400000000                     700   \n",
       "\n",
       "   Year-over-Year Change (%)  \n",
       "0                        3.5  \n",
       "1                        2.1  \n",
       "2                        4.0  \n",
       "3                        3.8  \n",
       "4                        2.5  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [ì˜ˆì œ 1] 2024ë…„ ìƒë°˜ê¸° ì„œìš¸ ì•„íŒŒíŠ¸ í‰ê·  ë§¤ë§¤ ê°€ê²© ë°ì´í„° ìƒì„±\n",
    "print('2024ë…„ í•˜ë°˜ê¸° ì„œìš¸ìš¸ ì•„íŒŒíŠ¸ í‰ê·  ë§¤ë§¤ ê°€ê²© ë°ì´í„° ìƒì„±')\n",
    "df_seoul_housing = generate_dataframe(\n",
    "    \"Create a dataset of the average apartment sale prices in Seoul for the Second half of 2024 with columns: District (êµ¬), Average Price (in KRW), Number of Transactions, and Year-over-Year Change (%).\"\n",
    ")\n",
    "print(df_seoul_housing.shape)\n",
    "df_seoul_housing.head()\n",
    "\n",
    "# gyeonggido\n",
    "# seoul ì„œìš¸\n",
    "# first half <- ìƒë°˜ê¸° // second half <- í•˜ë°˜ê¸°ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bba9e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024ë…„ ì„œìš¸ ì§€í•˜ì² ì—­ë³„ ìœ ë™ ì¸êµ¬ ë°ì´í„°\n",
      "\n",
      "ğŸ”¹ Generated DataFrame:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('2024ë…„ ì„œìš¸ ì§€í•˜ì² ì—­ë³„ ìœ ë™ ì¸êµ¬ ë°ì´í„°')\n",
    "# [ì˜ˆì œ 2] 2024ë…„ ì„œìš¸ ì§€í•˜ì² ì—­ë³„ ìœ ë™ ì¸êµ¬ ë°ì´í„°\n",
    "df_seoul_subway = generate_dataframe(\n",
    "    \"Generate a dataset of the top 10 busiest subway stations in Seoul in 2024 with columns: Station Name, Line Number, Daily Passenger Volume, and Weekday vs Weekend Ratio.\"\n",
    ")\n",
    "if df_seoul_subway is not None:\n",
    "    # print(df_seoul_subway.shape)\n",
    "    df_seoul_subway.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e6e4e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ 5ëŒ€ í¸ì˜ì  ë¸Œëœë“œë³„ 2024ë…„ ë§¤ì¶œ ë° ì í¬ ìˆ˜\n",
      "\n",
      "ğŸ”¹ Generated DataFrame:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Number of Stores</th>\n",
       "      <th>Total Revenue (in billion KRW)</th>\n",
       "      <th>Market Share (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CU</td>\n",
       "      <td>15000</td>\n",
       "      <td>5000</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GS25</td>\n",
       "      <td>14000</td>\n",
       "      <td>4800</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>11000</td>\n",
       "      <td>3200</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emart24</td>\n",
       "      <td>5000</td>\n",
       "      <td>1500</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ministop</td>\n",
       "      <td>3000</td>\n",
       "      <td>800</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Brand Name  Number of Stores  Total Revenue (in billion KRW)  \\\n",
       "0         CU             15000                            5000   \n",
       "1       GS25             14000                            4800   \n",
       "2   7-Eleven             11000                            3200   \n",
       "3    Emart24              5000                            1500   \n",
       "4   Ministop              3000                             800   \n",
       "\n",
       "   Market Share (%)  \n",
       "0              35.0  \n",
       "1              33.5  \n",
       "2              20.0  \n",
       "3               7.5  \n",
       "4               4.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('í•œêµ­ 5ëŒ€ í¸ì˜ì  ë¸Œëœë“œë³„ 2024ë…„ ë§¤ì¶œ ë° ì í¬ ìˆ˜')\n",
    "# [ì˜ˆì œ 3] í•œêµ­ 5ëŒ€ í¸ì˜ì  ë¸Œëœë“œë³„ 2024ë…„ ë§¤ì¶œ ë° ì í¬ ìˆ˜\n",
    "df_korean_convenience_stores = generate_dataframe(\n",
    "    \"Create a dataset of the top 5 convenience store brands in Korea in 2024 with columns: Brand Name, Number of Stores, Total Revenue (in billion KRW), and Market Share (%).\"\n",
    ")\n",
    "df_korean_convenience_stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63affb",
   "metadata": {},
   "source": [
    "### PydnticOutputPerser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14be6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026b1792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"movie_title\": {\"description\": \"ì¶”ì²œ ì˜í™” ì œëª©\", \"title\": \"Movie Title\", \"type\": \"string\"}, \"reason\": {\"description\": \"ì¶”ì²œ ì´ìœ \", \"title\": \"Reason\", \"type\": \"string\"}, \"genre\": {\"description\": \"ì˜í™” ì¥ë¥´\", \"items\": {\"type\": \"string\"}, \"title\": \"Genre\", \"type\": \"array\"}, \"estimated_rating\": {\"description\": \"10ì  ë§Œì ì—ì„œ ì˜ˆìƒ í‰ì \", \"title\": \"Estimated Rating\", \"type\": \"number\"}}, \"required\": [\"movie_title\", \"reason\", \"genre\", \"estimated_rating\"]}\\n```'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'query'], input_types={}, partial_variables={}, template='\\në‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì˜í™”ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.\\nìš”ì²­: {query}\\n\\n{format_instructions}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# ì¶œë ¥ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” Pydantic ëª¨ë¸\n",
    "class MovieRecommendation(BaseModel):\n",
    "    movie_title: str = Field(description=\"ì¶”ì²œ ì˜í™” ì œëª©\")\n",
    "    reason: str = Field(description=\"ì¶”ì²œ ì´ìœ \")\n",
    "    genre: List[str] = Field(description=\"ì˜í™” ì¥ë¥´\")\n",
    "    estimated_rating: float = Field(description=\"10ì  ë§Œì ì—ì„œ ì˜ˆìƒ í‰ì \")\n",
    "    \n",
    "# Pydantic ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”\n",
    "parser = PydanticOutputParser(pydantic_object=MovieRecommendation)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "template = \"\"\"\n",
    "ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì˜í™”ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.\n",
    "ìš”ì²­: {query}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# íŒŒì„œì˜ ì§€ì‹œì‚¬í•­ì„ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…\n",
    "prompt = prompt.partial(\n",
    "    format_instructions=parser.get_format_instructions()\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab408ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶”ì²œ ì˜í™”: The Silence of the Lambs\n",
      "ì¶”ì²œ ì´ìœ : 1990ë…„ëŒ€ í´ë˜ì‹í•œ ëŠë‚Œì˜ ê³µí¬ ì˜í™”ë¡œ, ê¸´ì¥ê°ì´ ë„˜ì¹˜ëŠ” ìŠ¤ë¦´ëŸ¬ ì˜í™”ì…ë‹ˆë‹¤.\n",
      "ì¥ë¥´: ê³µí¬, ìŠ¤ë¦´ëŸ¬, ë¯¸ìŠ¤í„°ë¦¬\n",
      "ì˜ˆìƒ í‰ì : 9.5/10\n"
     ]
    }
   ],
   "source": [
    "# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "#model = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰\n",
    "query = \"1990ë…„ëŒ€ í´ë˜ì‹í•œ ëŠë‚Œì˜ ê³µí¬ ì˜í™” ì¶”ì²œí•´ì¤˜\"\n",
    "chain = prompt | model | parser\n",
    "output = chain.invoke({\"query\": query})\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ì¶”ì²œ ì˜í™”: {output.movie_title}\")\n",
    "print(f\"ì¶”ì²œ ì´ìœ : {output.reason}\")\n",
    "print(f\"ì¥ë¥´: {', '.join(output.genre)}\")\n",
    "print(f\"ì˜ˆìƒ í‰ì : {output.estimated_rating}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8a06a",
   "metadata": {},
   "source": [
    "# StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c492c2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶œë ¥ í˜•ì‹ ì§€ì‹œì‚¬í•­:\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"rating\": string  // 5ì  ë§Œì ì—ì„œ ì˜ˆìƒ í‰ì \n",
      "\t\"pros\": string  // ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ ì¥ì  3ê°€ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥\n",
      "\t\"cons\": string  // ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ ë‹¨ì  3ê°€ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥\n",
      "\t\"summary\": string  // ë¦¬ë·°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# ì¶œë ¥ êµ¬ì¡° ì •ì˜ (í‰ì , ì¥ì , ë‹¨ì , ìš”ì•½)\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"rating\", description=\"5ì  ë§Œì ì—ì„œ ì˜ˆìƒ í‰ì \"),\n",
    "    ResponseSchema(name=\"pros\", description=\"ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ ì¥ì  3ê°€ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥\"),\n",
    "    ResponseSchema(name=\"cons\", description=\"ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ ë‹¨ì  3ê°€ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥\"),\n",
    "    ResponseSchema(name=\"summary\", description=\"ë¦¬ë·°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\")\n",
    "]\n",
    "\n",
    "# íŒŒì„œ ì´ˆê¸°í™”\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "print(\"ì¶œë ¥ í˜•ì‹ ì§€ì‹œì‚¬í•­:\")\n",
    "print(format_instructions)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "template = \"\"\"\n",
    "ë‹¤ìŒ ì œí’ˆ ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì„¸ìš”. ë¦¬ë·° ë‚´ìš©: {review}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(format_instructions=format_instructions)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™” (temperature=0.5ë¡œ ì„¤ì •í•´ ì¼ê´€ì„± ìˆëŠ” ì¶œë ¥)\n",
    "#model = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    temperature=0.7\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ef38b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ë¶„ì„ ê²°ê³¼ =====\n",
      "{'cons': '',\n",
      " 'pros': ['ë””ìì¸, ë””ìŠ¤í”Œë ˆì´, ì¹´ë©”ë¼, ì„±ëŠ¥, ë°°í„°ë¦¬, ì˜¤ë””ì˜¤ ë“± ëª¨ë“  ì¸¡ë©´ì—ì„œ ê· í˜• ì¡íŒ ì™„ì„±ë„ë¥¼ ë³´ì—¬ì¤Œ',\n",
      "          'ê°€ê²© ëŒ€ë¹„ ê¸°ëŠ¥ì„±ê³¼ ì„±ëŠ¥ì´ ë§¤ìš° ë›°ì–´ë‚¨',\n",
      "          'ì‚¬ìš©ì ê²½í—˜ì´ ê¹”ë”í•˜ê³  ì§ê´€ì ì¸ ì¸í„°í˜ì´ìŠ¤ ì œê³µ'],\n",
      " 'rating': '5',\n",
      " 'summary': 'ìƒˆë¡œ êµ¬ë§¤í•œ ìŠ¤ë§ˆíŠ¸í°ì€ ë””ìì¸, ì„±ëŠ¥, ì¹´ë©”ë¼ ë“± ëª¨ë“  ë©´ì—ì„œ ìš°ìˆ˜í•˜ë©°, ê°€ê²© ëŒ€ë¹„ ë›°ì–´ë‚œ ê¸°ëŠ¥ì„±ê³¼ ì„±ëŠ¥ì„ ì œê³µí•˜ì—¬ '\n",
      "            'ë§¤ìš° ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë¦¬ë·° ë°ì´í„°\n",
    "review = \"\"\"ìƒˆë¡œ êµ¬ë§¤í•œ ìŠ¤ë§ˆíŠ¸í°ì„ ì•½ 2ì£¼ê°„ ì‚¬ìš©í•´ë³¸ ê²°ê³¼, ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ê³¼ ì™„ì„±ë„ê°€ ë§¤ìš° ì¸ìƒì ì…ë‹ˆë‹¤. ìš°ì„  ì™¸ê´€ ë””ìì¸ì€ ë©”íƒˆ í”„ë ˆì„ê³¼ ìœ ê´‘ ê¸€ë˜ìŠ¤ ë°± ì»¤ë²„ ì¡°í•©ìœ¼ë¡œ ê³ ê¸‰ìŠ¤ëŸ¬ì›€ì„ ì˜ ì‚´ë ¸ê³ , ê³¡ë©´ ì²˜ë¦¬ëœ ì—£ì§€ ë¼ì¸ì´ ì†ì— ì°© ê°ê¸°ëŠ” ê·¸ë¦½ê°ì„ ì œê³µí•©ë‹ˆë‹¤. ë¬´ê²ŒëŠ” ì•½ 180g ì •ë„ë¡œ ê°€ë³ê³ , ë‘ê»˜ë„ ì–‡ì€ í¸ì´ë¼ íœ´ëŒ€ì„± ë©´ì—ì„œë„ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.\n",
    "ë””ìŠ¤í”Œë ˆì´ëŠ” 6.7ì¸ì¹˜ AMOLED íŒ¨ë„ì„ íƒ‘ì¬í•˜ê³  ìˆìœ¼ë©°, 120Hzì˜ ê³ ì£¼ì‚¬ìœ¨ì„ ì§€ì›í•´ ìŠ¤í¬ë¡¤ì´ë‚˜ ì• ë‹ˆë©”ì´ì…˜ì—ì„œ ë§¤ìš° ë¶€ë“œëŸ¬ìš´ ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤. FHD+ í•´ìƒë„ì— HDR10+ë¥¼ ì§€ì›í•´ ìƒ‰ í‘œí˜„ë ¥ê³¼ ëª…ì•”ë¹„ê°€ ë›°ì–´ë‚˜ê³ , ì‹¤ì™¸ ê°€ì‹œì„±ì„ ë†’ì´ê¸° ìœ„í•´ ìµœëŒ€ ë°ê¸° ì•½ 1,500ë‹ˆíŠ¸ê¹Œì§€ ì§€ì›ë˜ëŠ” ì ë„ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "ì¹´ë©”ë¼ ì‹œìŠ¤í…œì€ ê´‘ê°, ì´ˆê´‘ê°, ë§ì› ë“± ë‹¤ì¤‘ ì¹´ë©”ë¼ êµ¬ì„±ì„ ê°–ì¶”ê³  ìˆê³ , ë©”ì¸ ì„¼ì„œëŠ” 5,000ë§Œ í™”ì†Œë¡œ ë””í…Œì¼ í‘œí˜„ë ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤. íŠ¹íˆ ì•¼ê°„ ëª¨ë“œì—ì„œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì‹±ì´ íƒì›”í•˜ê²Œ ì‘ë™í•´, ì €ì¡°ë„ í™˜ê²½ì—ì„œë„ ë…¸ì´ì¦ˆê°€ ì ê³  ìƒ‰ê°ì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. OIS(ê´‘í•™ ì†ë–¨ë¦¼ ë³´ì •) ê¸°ëŠ¥ì´ ì ìš©ë˜ì–´ ë™ì˜ìƒ ì´¬ì˜ ì‹œ ì•ˆì •ê°ì´ í¬ë©°, 4K 60fps ì˜ìƒ ì´¬ì˜ë„ ê°€ëŠ¥í•´ ì½˜í…ì¸  ì œì‘ìš©ìœ¼ë¡œë„ ì¶©ë¶„í•©ë‹ˆë‹¤. ì „ë©´ ì¹´ë©”ë¼ë„ 1,200ë§Œ í™”ì†Œë¡œ, í”¼ë¶€í†¤ ë³´ì •ì´ ê³¼í•˜ì§€ ì•Šì•„ ìì—°ìŠ¤ëŸ½ê³  ê¹”ë”í•œ ê²°ê³¼ë¬¼ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "í¼í¬ë¨¼ìŠ¤ëŠ” ìµœì‹  Snapdragon 8 Gen 2 ì¹©ì…‹ ê¸°ë°˜ìœ¼ë¡œ, ì•± ì‹¤í–‰ ì†ë„ë‚˜ ë©€í‹°íƒœìŠ¤í‚¹, ê²Œì„ í”Œë ˆì´ ì‹œ ì „í˜€ ëŠê¹€ ì—†ì´ ë§¤ìš° ì¾Œì í•©ë‹ˆë‹¤. RAMì€ 12GBë¡œ ë„‰ë„‰í•˜ê³ , UFS 4.0 ìŠ¤í† ë¦¬ì§€ë¥¼ ì±„íƒí•´ íŒŒì¼ ì „ì†¡ ì†ë„ë‚˜ ì•± ë¡œë”© ì†ë„ë„ ë¹ ë¦…ë‹ˆë‹¤. ì¥ì‹œê°„ ê³ ë¶€í•˜ ì‘ì—… ì‹œì—ë„ ë°œì—´ ì œì–´ê°€ ì˜ ë˜ì–´ ìˆê³ , ì¨ë©€ ìŠ¤ë¡œí‹€ë§ í˜„ìƒë„ ê±°ì˜ ëŠê»´ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
    "ë°°í„°ë¦¬ëŠ” 5,000mAh ìš©ëŸ‰ìœ¼ë¡œ ì¼ë°˜ì ì¸ ì‚¬ìš© í™˜ê²½ì—ì„œ í•˜ë£¨ ì´ìƒì€ ì¶©ë¶„íˆ ë²„íŒë‹ˆë‹¤. íŠ¹íˆ 65W ê³ ì† ìœ ì„  ì¶©ì „ê³¼ 30W ë¬´ì„  ì¶©ì „ì„ ëª¨ë‘ ì§€ì›í•´ ì¶©ì „ íš¨ìœ¨ì´ ìš°ìˆ˜í•˜ë©°, 30ë¶„ ë‚´ì™¸ë¡œ ì•½ 70% ì´ìƒ ì¶©ì „ë˜ëŠ” ê²ƒì´ ì¸ìƒì ì…ë‹ˆë‹¤. ë°°í„°ë¦¬ ìˆ˜ëª…ë„ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ ì¶©ì „ ì•Œê³ ë¦¬ì¦˜ì´ ë‚´ì¥ë˜ì–´ ì¥ê¸°ì ì¸ ë°°í„°ë¦¬ íš¨ìœ¨ë„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì‚¬ìš´ë“œ í’ˆì§ˆ ì—­ì‹œ ìƒìœ„ê¶Œì…ë‹ˆë‹¤. ìŠ¤í…Œë ˆì˜¤ ë“€ì–¼ ìŠ¤í”¼ì»¤ë¥¼ íƒ‘ì¬í•´ ì‚¬ìš´ë“œ ë°¸ëŸ°ìŠ¤ê°€ ì¢‹ê³ , Dolby Atmos ì§€ì›ìœ¼ë¡œ ì…ì²´ì ì¸ ìŒí–¥ íš¨ê³¼ë„ ì˜ êµ¬í˜„ë©ë‹ˆë‹¤. í†µí™” ìŒì§ˆë„ ê¹¨ë—í•˜ê³ , ë…¸ì´ì¦ˆ ìº”ìŠ¬ë§ ê¸°ëŠ¥ì´ í¬í•¨ëœ ë§ˆì´í¬ ë•ë¶„ì— ì£¼ë³€ ì†ŒìŒì´ ë§ì€ í™˜ê²½ì—ì„œë„ ì›í™œí•œ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì´ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤.\n",
    "ì§€ë¬¸ ì¸ì‹ ì„¼ì„œëŠ” í™”ë©´ ë‚´ì¥í˜•ì´ë©° ë°˜ì‘ ì†ë„ê°€ ë¹ ë¥´ê³  ì •í™•ë„ë„ ë†’ì€ í¸ì…ë‹ˆë‹¤. ì–¼êµ´ ì¸ì‹ ê¸°ëŠ¥ë„ ë³‘í–‰ ì§€ì›ë˜ë©°, ë³´ì•ˆ ìˆ˜ì¤€ë„ ë‚˜ì˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš´ì˜ì²´ì œëŠ” ìµœì‹  ì•ˆë“œë¡œì´ë“œ ë²„ì „ì— ê¸°ë°˜í•œ ì»¤ìŠ¤í…€ UIë¡œ, ì¸í„°í˜ì´ìŠ¤ê°€ ì§ê´€ì ì´ê³  êµ°ë”ë”ê¸° ì—†ì´ ì •ë¦¬ë¼ ìˆì–´ ì‚¬ìš©ì ê²½í—˜ì´ ê¹”ë”í•©ë‹ˆë‹¤. íŠ¹íˆ ë¶ˆí•„ìš”í•œ ê¸°ë³¸ ì•±ì´ ê±°ì˜ ì—†ê³ , ì‹œìŠ¤í…œ ìµœì í™”ê°€ ì˜ ë˜ì–´ìˆë‹¤ëŠ” ì¸ìƒì„ ë°›ì•˜ìŠµë‹ˆë‹¤.\n",
    "ë°©ìˆ˜Â·ë°©ì§„ì€ IP68 ë“±ê¸‰ì„ ì§€ì›í•´ ì¼ìƒì ì¸ ë¬¼ íŠ€ê¹€ì´ë‚˜ ë¨¼ì§€ ê±±ì • ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆê³ , ë¸”ë£¨íˆ¬ìŠ¤ 5.3 ê¸°ë°˜ì˜ ì—°ê²°ë„ ì•ˆì •ì ì´ë©° ë¹ ë¦…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì„œë“œíŒŒí‹° ì•¡ì„¸ì„œë¦¬ì™€ì˜ í˜¸í™˜ì„±ë„ ìš°ìˆ˜í•´ í™•ì¥ì„± ì¸¡ë©´ì—ì„œë„ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.\n",
    "ì „ë°˜ì ìœ¼ë¡œ ë´¤ì„ ë•Œ, ì´ ìŠ¤ë§ˆíŠ¸í°ì€ ë””ìì¸, ë””ìŠ¤í”Œë ˆì´, ì¹´ë©”ë¼, ì„±ëŠ¥, ë°°í„°ë¦¬, ì˜¤ë””ì˜¤ ë“± ê±°ì˜ ëª¨ë“  ì¸¡ë©´ì—ì„œ ê· í˜• ì¡íŒ ì™„ì„±ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê°€ê²© ëŒ€ë¹„ ê¸°ëŠ¥ì„±ê³¼ ì„±ëŠ¥ì´ ë§¤ìš° ë›°ì–´ë‚˜ë©°, í”Œë˜ê·¸ì‹­ ëª¨ë¸ë‹¤ìš´ ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤. \n",
    "ê°œì¸ì ìœ¼ë¡œëŠ” í˜„ì‹œì ì—ì„œ ê°€ì¥ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ìŠ¤ë§ˆíŠ¸í° ì¤‘ í•˜ë‚˜ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ì´ ìŠ¤ë§ˆíŠ¸í°ì€ ë°°í„°ë¦¬ ìˆ˜ëª…ì´ ì •ë§ ì¢‹ì•„ì„œ í•˜ë£¨ ì¢…ì¼ ì‚¬ìš©í•´ë„ ì¶©ì „ì´ í•„ìš” ì—†ì—ˆì–´ìš”. \n",
    "# ì¹´ë©”ë¼ í™”ì§ˆë„ ì„ ëª…í•˜ê³ , íŠ¹íˆ ì•¼ê°„ ëª¨ë“œê°€ í›Œë¥­í•©ë‹ˆë‹¤. \n",
    "# ë‹¤ë§Œ ê°€ê²©ì´ ì¡°ê¸ˆ ë¹„ì‹¸ê³ , ë¬´ê²Œê°€ 200gì´ ë„˜ì–´ì„œ ì†ì´ í”¼ê³¤í•  ìˆ˜ ìˆì–´ìš”.\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "chain = prompt | model | parser\n",
    "\n",
    "output = chain.invoke({\"review\": review})\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥ (Pretty Print)\n",
    "print(\"===== ë¶„ì„ ê²°ê³¼ =====\")\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c66b8f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 14:38:57.564765\n",
      "ë‚ ì§œ ì¶œë ¥ í˜•ì‹ ì§€ì‹œì‚¬í•­:\n",
      "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
      "\n",
      "Examples: 0026-05-18T01:29:55.659408Z, 1998-02-11T11:56:11.614090Z, 1701-12-24T23:49:21.587067Z\n",
      "\n",
      "Return ONLY this string, no other words!\n",
      "prompt = input_variables=['text'] input_types={} partial_variables={'format_instructions': \"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 0026-05-18T01:29:55.659408Z, 1998-02-11T11:56:11.614090Z, 1701-12-24T23:49:21.587067Z\\n\\nReturn ONLY this string, no other words!\"} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'text'], input_types={}, partial_variables={}, template=\"\\ní˜„ì¬ ë‚ ì§œ: 2025-06-11\\në‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ/ì‹œê°„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ìƒëŒ€ì  í‘œí˜„(ì˜ˆ: 'ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼')ì€ í˜„ì¬ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.\\ní…ìŠ¤íŠ¸: {text}\\n\\n{format_instructions}\\n\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "#%pip install python-dateutil\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(datetime.today())\n",
    "\n",
    "# ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™” (ì‹œê°„ëŒ€ í¬í•¨ ê°€ëŠ¥)\n",
    "datetime_parser = DatetimeOutputParser()\n",
    "format_instructions = datetime_parser.get_format_instructions()\n",
    "\n",
    "print(\"ë‚ ì§œ ì¶œë ¥ í˜•ì‹ ì§€ì‹œì‚¬í•­:\")\n",
    "print(format_instructions)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "# í˜„ì¬ ë‚ ì§œë¥¼ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "template = f\"\"\"\n",
    "í˜„ì¬ ë‚ ì§œ: {current_date}\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ/ì‹œê°„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ìƒëŒ€ì  í‘œí˜„(ì˜ˆ: 'ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼')ì€ í˜„ì¬ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸: {{text}}\n",
    "\n",
    "{{format_instructions}}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(format_instructions=format_instructions)\n",
    "\n",
    "print(f'prompt = {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a2e1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì›ë³¸ í…ìŠ¤íŠ¸: íšŒì˜ëŠ” 2025ë…„ 6ì›” 15ì¼ ì˜¤í›„ 2ì‹œì— ì˜ˆì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
      "ì¶”ì¶œëœ ë‚ ì§œ: 2025-06-15 14:00:00 \n",
      "\n",
      "ì›ë³¸ í…ìŠ¤íŠ¸: í”„ë¡œì íŠ¸ ë§ˆê°ì¼ì€ ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼ì…ë‹ˆë‹¤.\n",
      "ì¶”ì¶œëœ ë‚ ì§œ: 2025-06-20 00:00:00 \n",
      "\n",
      "ì›ë³¸ í…ìŠ¤íŠ¸: í–‰ì‚¬ ì‹œì‘: 7/25/2025 18:00 KST\n",
      "ì¶”ì¶œëœ ë‚ ì§œ: 2025-07-25 18:00:00 \n",
      "\n",
      "ì›ë³¸ í…ìŠ¤íŠ¸: 3ì¼ í›„ì— ì‹œìŠ¤í…œ ì ê²€ì´ ì§„í–‰ë©ë‹ˆë‹¤.\n",
      "ì¶”ì¶œëœ ë‚ ì§œ: 2025-06-14 00:00:00 \n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™” (temperature=0.1ë¡œ ì„¤ì •í•´ ì •í™•í•œ ë‚ ì§œ ì¶œë ¥ ê°•ì¡°)\n",
    "#model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° (ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ í¬í•¨)\n",
    "texts = [\n",
    "    \"íšŒì˜ëŠ” 2025ë…„ 6ì›” 15ì¼ ì˜¤í›„ 2ì‹œì— ì˜ˆì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"í”„ë¡œì íŠ¸ ë§ˆê°ì¼ì€ ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼ì…ë‹ˆë‹¤.\",\n",
    "    \"í–‰ì‚¬ ì‹œì‘: 7/25/2025 18:00 KST\",\n",
    "    \"3ì¼ í›„ì— ì‹œìŠ¤í…œ ì ê²€ì´ ì§„í–‰ë©ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "chain = prompt | model | datetime_parser\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"\\nì›ë³¸ í…ìŠ¤íŠ¸: {text}\")\n",
    "    output = chain.invoke({\"text\": text})\n",
    "    print(f\"ì¶”ì¶œëœ ë‚ ì§œ: {output.strftime('%Y-%m-%d %H:%M:%S %Z')}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2352454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ë‚ ì§œ: 2025-06-11\n",
      "\n",
      "ì´ë²¤íŠ¸ ëª©ë¡:\n",
      "\n",
      "- ì´ë²¤íŠ¸ëª…: í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ë§ˆì¼“\n",
      "- ë‚ ì§œ: 2025-12-10 00:00:00\n",
      "\n",
      "- ì´ë²¤íŠ¸ëª…: í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì´ë¸Œ íŒŒí‹°\n",
      "- ë‚ ì§œ: 2025-12-24 00:00:00\n",
      "\n",
      "- ì´ë²¤íŠ¸ëª…: ìƒˆí•´ ì¹´ìš´íŠ¸ë‹¤ìš´\n",
      "- ë‚ ì§œ: 2026-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ì´ë²¤íŠ¸ ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "event_template = \"\"\"\n",
    "í˜„ì¬ ë‚ ì§œ: {current_date}\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ì´ë²¤íŠ¸ì˜ ë‚ ì§œ/ì‹œê°„ì„ ì¶”ì¶œí•˜ì„¸ìš”. ê° ì´ë²¤íŠ¸ëŠ” ì´ë¦„ê³¼ ë‚ ì§œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ê° ì´ë²¤íŠ¸ì˜ ë‚ ì§œëŠ” ë¯¸ë˜ ì¼ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚° í•©ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸: {text}\n",
    "\n",
    "ì¶œë ¥ í˜•ì‹:\n",
    "- ì´ë²¤íŠ¸ëª…: [ì´ë¦„]\n",
    "- ë‚ ì§œ: [YYYY-MM-DD HH:MM:SS]\n",
    "\"\"\"\n",
    "\n",
    "event_prompt = ChatPromptTemplate.from_template(event_template)\n",
    "event_chain = event_prompt | model\n",
    "í…ìŠ¤íŠ¸: {(text)}\n",
    "\n",
    "# ì˜ˆì‹œ í…ìŠ¤íŠ¸ (ì—¬ëŸ¬ ì´ë²¤íŠ¸ í¬í•¨)\n",
    "event_text = \"\"\"\n",
    "12ì›” 10ì¼ì— í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ë§ˆì¼“ì´ ì—´ë¦¬ê³ , 12ì›” 24ì¼ì—ëŠ” í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì´ë¸Œ íŒŒí‹°ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "ë˜í•œ 1ì›” 1ì¼ 00:00ì— ìƒˆí•´ ì¹´ìš´íŠ¸ë‹¤ìš´ì´ ì§„í–‰ë  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "print(event_chain.invoke({\"current_date\":current_date, \"text\": event_text}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ba5ec",
   "metadata": {},
   "source": [
    "# EnumOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20751719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import EnumOutputParser, OutputFixingParser\n",
    "from langchain.schema import OutputParserException\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a2c7df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì • ë¶„ë¥˜ ì¶œë ¥ í˜•ì‹:\n",
      "Select one of the following options: ê¸ì •, ë¶€ì •, ì¤‘ë¦½\n",
      "input_variables=['text'] input_types={} partial_variables={'format_instructions': 'Select one of the following options: ê¸ì •, ë¶€ì •, ì¤‘ë¦½'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'text'], input_types={}, partial_variables={}, template='\\në‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\\në‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ê³ , ë°˜ë“œì‹œ ì•„ë˜ ì„¸ ê°€ì§€ ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\\n\\ní…ìŠ¤íŠ¸: \"{text}\"\\n\\n{format_instructions}\\n\\nì¤‘ìš” ê·œì¹™:\\n1. ë°˜ë“œì‹œ \"ê¸ì •\", \"ë¶€ì •\", \"ì¤‘ë¦½\" ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”\\n2. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ì„¤ëª…ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”\\n3. ì´ëª¨ì§€ë‚˜ íŠ¹ìˆ˜ë¬¸ìë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\\n4. ì˜¤ì§ í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”\\n\\në‹µë³€:'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import EnumOutputParser, OutputFixingParser\n",
    "from langchain.schema import OutputParserException\n",
    "\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "\n",
    "# ê°ì • í´ë˜ìŠ¤ ì •ì˜ (Enum)\n",
    "class Sentiment(str, Enum):\n",
    "    POSITIVE = \"ê¸ì •\"\n",
    "    NEGATIVE = \"ë¶€ì •\"\n",
    "    NEUTRAL = \"ì¤‘ë¦½\"\n",
    "\n",
    "# EnumOutputParser ì´ˆê¸°í™”\n",
    "enumParser = EnumOutputParser(enum=Sentiment)\n",
    "format_instructions = enumParser.get_format_instructions()\n",
    "\n",
    "print(\"ê°ì • ë¶„ë¥˜ ì¶œë ¥ í˜•ì‹:\")\n",
    "print(format_instructions)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "template = \"\"\"\n",
    "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ê³ , ë°˜ë“œì‹œ ì•„ë˜ ì„¸ ê°€ì§€ ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "í…ìŠ¤íŠ¸: \"{text}\"\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "ì¤‘ìš” ê·œì¹™:\n",
    "1. ë°˜ë“œì‹œ \"ê¸ì •\", \"ë¶€ì •\", \"ì¤‘ë¦½\" ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”\n",
    "2. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ì„¤ëª…ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”\n",
    "3. ì´ëª¨ì§€ë‚˜ íŠ¹ìˆ˜ë¬¸ìë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\n",
    "4. ì˜¤ì§ í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(format_instructions=format_instructions)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3fc077a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë° íŒŒì„œ ì„¤ì • ì™„ë£Œ\n",
      "í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ 9ê°œ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0  # ì¼ê´€ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •\n",
    ")\n",
    "\n",
    "# OutputFixingParserë¡œ ì•ˆì •ì„± í–¥ìƒ\n",
    "fixing_parser = OutputFixingParser.from_llm(parser=enumParser, llm=model)\n",
    "\n",
    "print(\"ëª¨ë¸ ë° íŒŒì„œ ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸\n",
    "texts = [\n",
    "    \"ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”! ì™„ì „ ë§Œì¡±ìŠ¤ëŸ¬ì›Œìš”.\",\n",
    "    \"ì„œë¹„ìŠ¤ê°€ ë„ˆë¬´ ëŠë¦¬ê³  ë¶ˆì¹œì ˆí–ˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì˜¤ëŠ˜ì€ ë¹„ê°€ ì˜¨ë‹¤ë„¤ìš”.\",\n",
    "    \"ë°°ì†¡ì€ ë¹ ë¥´ì§€ë§Œ í’ˆì§ˆì´ ì•„ì‰½ìŠµë‹ˆë‹¤.\",\n",
    "    \"ìµœê³ ì˜ ê²½í—˜ì´ì—ˆìŠµë‹ˆë‹¤!\",\n",
    "    \"ì™„ì „ ì‹¤ë§í–ˆì–´ìš”... ìµœì•…ì´ì—ìš”\",\n",
    "    \"ì¼ì´ ë§ì•„ì„œ ë°¤ìƒ˜ì„ í–ˆì–´ìš”.\",\n",
    "    \"ë„ˆë¬´ ë§› ì—†ì–´ìš”.\",\n",
    "    \"ë„ˆë¬´ ë§› ìˆì—ˆì–´ìš” ë‹¤ìŒì— ë˜ ì˜¬ê»˜ìš”.\"\n",
    "    \n",
    "]\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ {len(texts)}ê°œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc4d2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì‹¤ì œ ê°ì • ë¶„ì„ ê²°ê³¼ ===\n",
      "\n",
      "1. í…ìŠ¤íŠ¸: ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”! ì™„ì „ ë§Œì¡±ìŠ¤ëŸ¬ì›Œìš”.\n",
      "   ê°ì •: ê¸ì • \n",
      "\n",
      "2. í…ìŠ¤íŠ¸: ì„œë¹„ìŠ¤ê°€ ë„ˆë¬´ ëŠë¦¬ê³  ë¶ˆì¹œì ˆí–ˆìŠµë‹ˆë‹¤.\n",
      "   ê°ì •: ë¶€ì • \n",
      "\n",
      "3. í…ìŠ¤íŠ¸: ì˜¤ëŠ˜ì€ ë¹„ê°€ ì˜¨ë‹¤ë„¤ìš”.\n",
      "   ê°ì •: ì¤‘ë¦½ \n",
      "\n",
      "4. í…ìŠ¤íŠ¸: ë°°ì†¡ì€ ë¹ ë¥´ì§€ë§Œ í’ˆì§ˆì´ ì•„ì‰½ìŠµë‹ˆë‹¤.\n",
      "   ê°ì •: ë¶€ì • \n",
      "\n",
      "5. í…ìŠ¤íŠ¸: ìµœê³ ì˜ ê²½í—˜ì´ì—ˆìŠµë‹ˆë‹¤!\n",
      "   ê°ì •: ê¸ì • \n",
      "\n",
      "6. í…ìŠ¤íŠ¸: ì™„ì „ ì‹¤ë§í–ˆì–´ìš”... ìµœì•…ì´ì—ìš”\n",
      "   ê°ì •: ë¶€ì • \n",
      "\n",
      "7. í…ìŠ¤íŠ¸: ì¼ì´ ë§ì•„ì„œ ë°¤ìƒ˜ì„ í–ˆì–´ìš”.\n",
      "   ê°ì •: ë¶€ì • \n",
      "\n",
      "8. í…ìŠ¤íŠ¸: ë„ˆë¬´ ë§› ì—†ì–´ìš”.\n",
      "   ê°ì •: ë¶€ì • \n",
      "\n",
      "9. í…ìŠ¤íŠ¸: ë„ˆë¬´ ë§› ìˆì—ˆì–´ìš” ë‹¤ìŒì— ë˜ ì˜¬ê»˜ìš”.\n",
      "   ê°ì •: ê¸ì • \n",
      "\n",
      "=== ê²°ê³¼ ìš”ì•½ ===\n",
      "ì„±ê³µ: 9/9 (100.0%)\n",
      "ì‹¤íŒ¨: 0/9\n"
     ]
    }
   ],
   "source": [
    "# ì•ˆì „í•œ ê°ì • ë¶„ì„ í•¨ìˆ˜ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)\n",
    "def safe_sentiment_analysis(text, use_fixing_parser=True):\n",
    "    \"\"\"ì•ˆì „í•œ ê°ì • ë¶„ì„ í•¨ìˆ˜ - ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨\"\"\"\n",
    "    try:\n",
    "        # ê¸°ë³¸ ì²´ì¸ ìƒì„±\n",
    "        chain = prompt | model | (fixing_parser if use_fixing_parser else enumParser)\n",
    "        \n",
    "        # ë¶„ì„ ì‹¤í–‰\n",
    "        result = chain.invoke({\"text\": text})\n",
    "        return result, None\n",
    "        \n",
    "    except OutputParserException as e:\n",
    "        return None, f\"íŒŒì‹± ì˜¤ë¥˜: {str(e)[:100]}...\"\n",
    "    except Exception as e:\n",
    "        return None, f\"ì¼ë°˜ ì˜¤ë¥˜: {str(e)[:100]}...\"\n",
    "\n",
    "# ì‹¤ì œ ê°ì • ë¶„ì„ ì‹¤í–‰ (API í‚¤ í•„ìš”)\n",
    "def run_sentiment_analysis():\n",
    "    \"\"\"ì‹¤ì œ ê°ì • ë¶„ì„ ì‹¤í–‰\"\"\"\n",
    "    print(\"=== ì‹¤ì œ ê°ì • ë¶„ì„ ê²°ê³¼ ===\")\n",
    "    \n",
    "    success_count = 0\n",
    "    total_count = len(texts)\n",
    "    \n",
    "    for i, text in enumerate(texts, 1):\n",
    "        print(f\"\\n{i}. í…ìŠ¤íŠ¸: {text}\")\n",
    "        \n",
    "        # OutputFixingParser ì‚¬ìš©\n",
    "        result, error = safe_sentiment_analysis(text, use_fixing_parser=True)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"   ê°ì •: {result.value} \")\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"   ì˜¤ë¥˜: {error} \")\n",
    "            \n",
    "            # ê¸°ë³¸ íŒŒì„œë¡œ ì¬ì‹œë„\n",
    "            print(\"   ê¸°ë³¸ íŒŒì„œë¡œ ì¬ì‹œë„...\")\n",
    "            result2, error2 = safe_sentiment_analysis(text, use_fixing_parser=False)\n",
    "            \n",
    "            if result2:\n",
    "                print(f\"   ê°ì •: {result2.value} (ê¸°ë³¸ íŒŒì„œ ì„±ê³µ)\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"   ì¬ì‹œë„ ì‹¤íŒ¨: {error2} \")\n",
    "    \n",
    "    print(f\"\\n=== ê²°ê³¼ ìš”ì•½ ===\")\n",
    "    print(f\"ì„±ê³µ: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)\")\n",
    "    print(f\"ì‹¤íŒ¨: {total_count-success_count}/{total_count}\")\n",
    "\n",
    "# ì‹¤ì œ ë¶„ì„ ì‹¤í–‰ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)\n",
    "try:\n",
    "    run_sentiment_analysis()\n",
    "except Exception as e:\n",
    "    print(\"API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜:\")\n",
    "    print(\"ì‹¤ì œ ì‹¤í–‰ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\")\n",
    "    print(f\"ì˜¤ë¥˜ ìƒì„¸: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f4a6a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['applicant_details', 'min_age', 'min_credit_score', 'min_income'] input_types={} partial_variables={'format_instructions': '\\nì¶œë ¥ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤:\\n- `True`: ëª¨ë“  ì¡°ê±´ ì¶©ì¡± ì‹œ\\n- `False`: í•˜ë‚˜ë¼ë„ ì¡°ê±´ ë¶ˆì¶©ì¡± ì‹œ\\n\\nì˜ˆì‹œ:\\nTrue  # ëª¨ë“  ì¡°ê±´ ë§Œì¡±\\nFalse # ì¡°ê±´ ë¶ˆë§Œì¡±\\n'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['applicant_details', 'format_instructions', 'min_age', 'min_credit_score', 'min_income'], input_types={}, partial_variables={}, template='\\në‹¤ìŒ ëŒ€ì¶œ ì‹ ì²­ìë¥¼ í‰ê°€í•˜ì„¸ìš”. ì¡°ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•˜ë©´ `True`, ì•„ë‹ˆë©´ `False`ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\\n\\n### ì¡°ê±´:\\n1. ë‚˜ì´ >= {min_age}ì„¸\\n2. ì‹ ìš© ì ìˆ˜ >= {min_credit_score}\\n3. ì›” ìˆ˜ì… >= ${min_income}\\n\\n### ì‹ ì²­ì ì •ë³´:\\n{applicant_details}\\n\\n{format_instructions}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import BooleanOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Boolean íŒŒì„œ ì´ˆê¸°í™”\n",
    "boolParser = BooleanOutputParser()\n",
    "\n",
    "# ìˆ˜ë™ìœ¼ë¡œ í¬ë§· ì§€ì‹œì‚¬í•­ ì •ì˜ (LangChain ë²„ì „ ì´ìŠˆ íšŒí”¼)\n",
    "format_instructions = \"\"\"\n",
    "ì¶œë ¥ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤:\n",
    "- `True`: ëª¨ë“  ì¡°ê±´ ì¶©ì¡± ì‹œ\n",
    "- `False`: í•˜ë‚˜ë¼ë„ ì¡°ê±´ ë¶ˆì¶©ì¡± ì‹œ\n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "True  # ëª¨ë“  ì¡°ê±´ ë§Œì¡±\n",
    "False # ì¡°ê±´ ë¶ˆë§Œì¡±\n",
    "\"\"\"\n",
    "\n",
    "# ìŠ¹ì¸/ê±°ë¶€ ê²°ì • í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "template = \"\"\"\n",
    "ë‹¤ìŒ ëŒ€ì¶œ ì‹ ì²­ìë¥¼ í‰ê°€í•˜ì„¸ìš”. ì¡°ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•˜ë©´ `True`, ì•„ë‹ˆë©´ `False`ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "\n",
    "### ì¡°ê±´:\n",
    "1. ë‚˜ì´ >= {min_age}ì„¸\n",
    "2. ì‹ ìš© ì ìˆ˜ >= {min_credit_score}\n",
    "3. ì›” ìˆ˜ì… >= ${min_income}\n",
    "\n",
    "### ì‹ ì²­ì ì •ë³´:\n",
    "{applicant_details}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt.partial(format_instructions=format_instructions)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ac3f511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜ ë°œìƒ: BooleanOutputParser expected output value to include either YES or NO. Received ### ëŒ€ì¶œ ì‹ ì²­ì í‰ê°€\n",
      "\n",
      "#### ì¡°ê±´:\n",
      "1. ë‚˜ì´ >= 18ì„¸\n",
      "2. ì‹ ìš© ì ìˆ˜ >= 700\n",
      "3. ì›” ìˆ˜ì… >= $3000\n",
      "\n",
      "#### ì‹ ì²­ì ì •ë³´:\n",
      "- ì´ë¦„: ê¹€ì² ìˆ˜\n",
      "- ë‚˜ì´: 25ì„¸\n",
      "- ì‹ ìš© ì ìˆ˜: 750\n",
      "- ì›” ìˆ˜ì…: $3,500\n",
      "\n",
      "### í‰ê°€ ì½”ë“œ\n",
      "\n",
      "```python\n",
      "def evaluate_loan_application(name, age, credit_score, monthly_income):\n",
      "    # ì¡°ê±´ 1: ë‚˜ì´ >= 18ì„¸\n",
      "    condition1 = age >= 18\n",
      "    \n",
      "    # ì¡°ê±´ 2: ì‹ ìš© ì ìˆ˜ >= 700\n",
      "    condition2 = credit_score >= 700\n",
      "    \n",
      "    # ì¡°ê±´ 3: ì›” ìˆ˜ì… >= $3000\n",
      "    condition3 = monthly_income >= 3000\n",
      "    \n",
      "    # ëª¨ë“  ì¡°ê±´ ì¶©ì¡± ì‹œ True ë°˜í™˜\n",
      "    return condition1 and condition2 and condition3\n",
      "\n",
      "# ì‹ ì²­ì ì •ë³´\n",
      "name = \"ê¹€ì² ìˆ˜\"\n",
      "age = 25\n",
      "credit_score = 750\n",
      "monthly_income = 3500\n",
      "\n",
      "# í‰ê°€ ê²°ê³¼\n",
      "result = evaluate_loan_application(name, age, credit_score, monthly_income)\n",
      "\n",
      "print(result)  # True\n",
      "```\n",
      "\n",
      "### ê²°ê³¼ ì„¤ëª…\n",
      "\n",
      "- ë‚˜ì´: 25ì„¸ (ì¡°ê±´ ë§Œì¡±)\n",
      "- ì‹ ìš© ì ìˆ˜: 750 (ì¡°ê±´ ë§Œì¡±)\n",
      "- ì›” ìˆ˜ì…: $3,500 (ì¡°ê±´ ë§Œì¡±)\n",
      "\n",
      "ëª¨ë“  ì¡°ê±´ì„ ì¶©ì¡±í•˜ë¯€ë¡œ ê²°ê³¼ëŠ” `True`ì…ë‹ˆë‹¤..\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "chain = prompt | model | boolParser\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "test_cases = [\n",
    "    {\n",
    "        \"min_age\": 18,\n",
    "        \"min_credit_score\": 700,\n",
    "        \"min_income\": 3000,\n",
    "        \"applicant_details\": \"\"\"\n",
    "        - ì´ë¦„: ê¹€ì² ìˆ˜\n",
    "        - ë‚˜ì´: 25ì„¸\n",
    "        - ì‹ ìš© ì ìˆ˜: 750\n",
    "        - ì›” ìˆ˜ì…: $3,500\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"min_age\": 18,\n",
    "        \"min_credit_score\": 700,\n",
    "        \"min_income\": 3000,\n",
    "        \"applicant_details\": \"\"\"\n",
    "        - ì´ë¦„: ì´ì˜í¬\n",
    "        - ë‚˜ì´: 17ì„¸\n",
    "        - ì‹ ìš© ì ìˆ˜: 680\n",
    "        - ì›” ìˆ˜ì…: $2,800\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ê±°ë¶€ ì‚¬ìœ  ìƒì„± í”„ë¡¬í”„íŠ¸ (ì¶œë ¥ í˜•ì‹ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •)\n",
    "reason_template = \"\"\"\n",
    "ë‹¤ìŒ ëŒ€ì¶œ ì‹ ì²­ ê±°ë¶€ ì‚¬ìœ ë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”. ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "[ê±°ë¶€ ì‚¬ìœ ]: [ì‚¬ìœ  ë‚´ìš©]\n",
    "\n",
    "### ì‹ ì²­ì ì •ë³´:\n",
    "{applicant_details}\n",
    "\n",
    "### ì¡°ê±´:\n",
    "- ìµœì†Œ ë‚˜ì´: {min_age}ì„¸\n",
    "- ìµœì†Œ ì‹ ìš© ì ìˆ˜: {min_credit_score}\n",
    "- ìµœì†Œ ì›” ìˆ˜ì…: ${min_income}\n",
    "\"\"\"\n",
    "reason_prompt = ChatPromptTemplate.from_template(reason_template)\n",
    "reason_chain = reason_prompt | model | StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ í™•ì¥ (Boolean íŒŒì„œì™€ ë¶„ë¦¬)\n",
    "def get_decision_with_reason(input_dict):\n",
    "    # 1. ë¨¼ì € Boolean ê²°ì •\n",
    "    decision = chain.invoke(input_dict)\n",
    "    \n",
    "    # 2. ê±°ë¶€ ì‹œì—ë§Œ ì‚¬ìœ  ìƒì„±\n",
    "    if not decision:\n",
    "        try:\n",
    "            reason = reason_chain.invoke({\n",
    "                \"applicant_details\": input_dict[\"applicant_details\"],\n",
    "                \"min_age\": input_dict[\"min_age\"],\n",
    "                \"min_credit_score\": input_dict[\"min_credit_score\"],\n",
    "                \"min_income\": input_dict[\"min_income\"]\n",
    "            })\n",
    "            return decision, reason\n",
    "        except Exception as e:\n",
    "            return decision, f\"ê±°ë¶€ ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨: {str(e)}\"\n",
    "    return decision, \"ëª¨ë“  ì¡°ê±´ì„ ì¶©ì¡±í–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ (ì•ˆì „í•œ ì‹¤í–‰)\n",
    "try:\n",
    "    decision, reason = get_decision_with_reason(test_cases[0])\n",
    "    print(f\"\\nê²°ê³¼: {'ìŠ¹ì¸' if decision else 'ê±°ë¶€'}\")\n",
    "    print(f\"ì‚¬ìœ : {reason}\")\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
