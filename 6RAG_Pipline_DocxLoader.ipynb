{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5a6fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bea63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # 특정 경고 유형만 무시\n",
    "\n",
    "#  1. 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API 키 확인\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OpenAI API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "#  3. DOCX 파일 로드 및 텍스트 추출 (Docx2txtLoader 활용)\n",
    "def load_docx(file_path):\n",
    "    \"\"\"DOCX 파일에서 텍스트를 추출하는 함수.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "#  4. 문서 분할 함수\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"텍스트를 지정된 크기의 청크로 분할하는 함수.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "#  5. 벡터 데이터베이스(FAISS) 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"텍스트 청크를 임베딩하고 FAISS 벡터 저장소에 저장.\"\"\"\n",
    "    try:\n",
    "        documents = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "#  6. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store):\n",
    "    \"\"\"LLM을 사용하여 검색된 문서 기반으로 답변 생성.\"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "        \n",
    "        # 프롬프트 로드 (RAG 최적화된 LangChain Hub 프롬프트 사용)\n",
    "        #prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "        prompt = hub.pull(\"cheorhyeon/rag-prompt-korean\")\n",
    "        print(prompt)\n",
    "\n",
    "        # RetrievalQA 체인 생성\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm, \n",
    "            retriever=vector_store.as_retriever(),\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "\n",
    "        # LLM 응답 생성\n",
    "        ai_message = qa_chain.invoke({\"query\": query})\n",
    "        print(ai_message)\n",
    "        return ai_message[\"result\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7c638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 로드 완료\n",
      "문서 분할 완료: 296개 청크 생성\n",
      "벡터 저장소 생성 완료\n",
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'cheorhyeon', 'lc_hub_repo': 'rag-prompt-korean', 'lc_hub_commit_hash': '23289ae9d525acc8cd52f2e7be1b4a2b09f527f7fa5d42287cac0ab824a4b3ca'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"\\n당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다.\\n주어진 문맥(Context)에서 질문(Question)에 답하세요.\\n답을 모르면 '주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다'라고 답하세요.\\n모든 답변은 한글로 작성하되, 기술 용어나 이름은 번역하지 마세요.\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n#Question: {question}\\n\\n#Context: {context}\\n'), additional_kwargs={})]\n",
      "{'query': '총수입금액 불산입에 대하여 설명해 주세요.', 'result': '총수입금액 불산입은 해당 과세기간 동안 개별소비세, 주세, 부가가치세, 석유판매업자가 환급받은 세액, 그 밖의 환급금에 대한 이자와 같은 소득금액을 계산할 때 고려하지 않는 것을 말합니다. 즉, 이러한 수입금액들은 해당 세액을 산정할 때 고려되지 않습니다.'}\n",
      "\n",
      " AI의 답변:\n",
      "총수입금액 불산입은 해당 과세기간 동안 개별소비세, 주세, 부가가치세, 석유판매업자가 환급받은 세액, 그 밖의 환급금에 대한 이자와 같은 소득금액을 계산할 때 고려하지 않는 것을 말합니다. 즉, 이러한 수입금액들은 해당 세액을 산정할 때 고려되지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "#  실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX 파일 경로\n",
    "    docx_path = \"data/tax_with_table_short.docx\"\n",
    "    \n",
    "    # 1. 문서 로드\n",
    "    text = load_docx(docx_path)\n",
    "    print(\"문서 로드 완료\")\n",
    "    \n",
    "    # 2. 문서 분할\n",
    "    text_chunks = split_text(text)\n",
    "    print(f\"문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 3. 임베딩 모델 초기화\n",
    "    embedding_model = OpenAIEmbeddings()\n",
    "    \n",
    "    # 4. 벡터 저장소 생성\n",
    "    vector_store = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5. 질의 실행\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store)\n",
    "    \n",
    "    # 6. AI 응답 출력\n",
    "    print(\"\\n AI의 답변:\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2edce80",
   "metadata": {},
   "source": [
    "### Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d39c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. 환경 변수 로드 및 API 키 설정\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OpenAI API 키가 설정되지 않았습니다. .env 파일에 OPENAI_API_KEY를 추가하세요.\")\n",
    "\n",
    "# 2. 한국어 법률 문서 전용 텍스트 전처리 함수\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"\n",
    "    한국어 법률 문서의 구조를 고려한 텍스트 전처리 함수\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 전처리된 텍스트\n",
    "        \n",
    "    주요 처리 내용:\n",
    "        - 불필요한 공백 및 개행 정리\n",
    "        - 법조문 번호 정규화 (제1조, 제2조 등)\n",
    "        - 항 번호를 아라비아 숫자로 변환 (①→제1항)\n",
    "        - 호 번호 정규화\n",
    "    \"\"\"\n",
    "    # 연속된 공백을 하나의 공백으로 통일\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 조항 번호 정규화: \"제 1 조\" -> \"제1조\" 형태로 통일\n",
    "    text = re.sub(r'제(\\d+)조', r'제\\1조', text)\n",
    "    \n",
    "    # 원문자 항 번호를 아라비아 숫자로 변환하여 검색 정확도 향상\n",
    "    # ①②③④⑤⑥⑦⑧⑨⑩ -> 제1항, 제2항, ... 제10항\n",
    "    text = re.sub(r'①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩', \n",
    "                  lambda m: f\"제{ord(m.group()) - ord('①') + 1}항\", text)\n",
    "    \n",
    "    # 호 번호 정규화: \"1. \" -> \"제1호 \" 형태로 변환\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'제\\1호 ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. 법률 문서에 최적화된 텍스트 분할 함수\n",
    "def advanced_split_text(text, chunk_size=600, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    법률 문서의 구조적 특성을 고려한 지능적 텍스트 분할\n",
    "    \n",
    "    Args:\n",
    "        text (str): 분할할 텍스트\n",
    "        chunk_size (int): 각 청크의 목표 크기 (문자 수)\n",
    "        chunk_overlap (int): 청크 간 중복되는 문자 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 분할된 텍스트 청크들의 리스트\n",
    "        \n",
    "    특징:\n",
    "        - 법률 문서의 계층 구조(조>항>호>목)를 고려한 분할 우선순위\n",
    "        - 의미적 완성도를 유지하면서 분할\n",
    "        - 토큰 한도를 고려한 적절한 크기 설정\n",
    "    \"\"\"\n",
    "    # 텍스트 전처리 수행\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # 법률 문서 구조를 고려한 분할 구분자들을 우선순위대로 설정\n",
    "    # 상위 구조부터 하위 구조 순서로 분할을 시도\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n제\", \"\\n**제\",  # 조항 단위 분할 (가장 우선)\n",
    "            \"\\n①\", \"\\n②\", \"\\n③\", \"\\n④\", \"\\n⑤\",  # 항 단위 분할\n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # 호 단위 분할\n",
    "            \"\\n가.\", \"\\n나.\", \"\\n다.\", \"\\n라.\", \"\\n마.\",  # 목 단위 분할\n",
    "            \"\\n\\n\",  # 문단 단위 분할\n",
    "            \"\\n\",    # 줄 단위 분할\n",
    "            \". \",    # 문장 단위 분할\n",
    "            \" \",     # 단어 단위 분할\n",
    "            \"\"       # 문자 단위 분할 (최후 수단)\n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. DOCX 파일 로딩 및 전처리 함수\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"\n",
    "    DOCX 파일을 로드하고 기본적인 텍스트 정리를 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): DOCX 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        str: 정리된 텍스트\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: 파일 로딩 실패 시\n",
    "        ValueError: 텍스트 추출 실패 시\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Docx2txtLoader를 사용하여 DOCX 파일에서 텍스트 추출\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        # 텍스트가 비어있는지 확인\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다. 파일이 비어있거나 손상되었을 수 있습니다.\")\n",
    "        \n",
    "        # 기본적인 텍스트 정리 작업\n",
    "        # 연속된 빈 줄을 두 개의 줄바꿈으로 통일\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # 연속된 공백과 탭을 하나의 공백으로 통일\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "# 5. 배치 처리를 통한 벡터 저장소 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model, batch_size=30):\n",
    "    \"\"\"\n",
    "    텍스트 청크들을 배치 단위로 처리하여 벡터 저장소 생성\n",
    "    토큰 한도 초과 문제를 해결하기 위해 배치 처리 방식 적용\n",
    "    \n",
    "    Args:\n",
    "        text_chunks (list): 분할된 텍스트 청크들\n",
    "        embedding_model: OpenAI 임베딩 모델 객체\n",
    "        batch_size (int): 한 번에 처리할 청크 수 (기본값: 30)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (FAISS 벡터 저장소, Document 객체들의 리스트)\n",
    "        \n",
    "    처리 과정:\n",
    "        1. 각 청크에 메타데이터 추가 (ID, 길이, 조항 정보 등)\n",
    "        2. 청크 크기가 너무 큰 경우 자동으로 제한\n",
    "        3. 배치 단위로 임베딩 생성하여 토큰 한도 문제 방지\n",
    "        4. FAISS merge 기능을 활용하여 배치별 결과 통합\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"   총 {len(text_chunks)}개 청크를 {batch_size}개씩 배치 처리...\")\n",
    "        \n",
    "        # Document 객체들을 저장할 리스트 초기화\n",
    "        documents = []\n",
    "        \n",
    "        # 각 텍스트 청크를 Document 객체로 변환하면서 메타데이터 추가\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # 청크 크기가 너무 큰 경우 제한 (토큰 한도 방지)\n",
    "            if len(chunk) > 2000:\n",
    "                chunk = chunk[:2000] + \"...\"\n",
    "                print(f\"   경고: 청크 {i}가 너무 커서 2000자로 제한했습니다.\")\n",
    "            \n",
    "            # 각 청크에 추가할 메타데이터 구성\n",
    "            metadata = {\n",
    "                'chunk_id': i,  # 청크 고유 번호\n",
    "                'chunk_length': len(chunk),  # 청크 길이\n",
    "                'chunk_type': 'legal_document'  # 문서 유형\n",
    "            }\n",
    "            \n",
    "            # 청크 내용에서 조항 정보 자동 추출\n",
    "            # \"제n조\" 패턴을 찾아 메타데이터에 추가\n",
    "            if '제' in chunk and '조' in chunk:\n",
    "                article_match = re.search(r'제(\\d+)조', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"제{article_match.group(1)}조\"\n",
    "            \n",
    "            # Document 객체 생성하여 리스트에 추가\n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        \n",
    "        # 배치별 벡터 저장소 생성 및 병합 과정\n",
    "        vector_store = None\n",
    "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # 문서들을 배치 크기만큼 나누어 처리\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            \n",
    "            print(f\"   배치 {batch_num}/{total_batches} 처리 중... ({len(batch_docs)}개 문서)\")\n",
    "            \n",
    "            # 첫 번째 배치인 경우 새로운 벡터 저장소 생성\n",
    "            if vector_store is None:\n",
    "                vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "            else:\n",
    "                # 이후 배치들은 기존 벡터 저장소에 병합\n",
    "                batch_vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "                vector_store.merge_from(batch_vector_store)\n",
    "        \n",
    "        print(\"   모든 배치 처리 완료\")\n",
    "        return vector_store, documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "# 6. 키워드 기반 검색 함수\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"\n",
    "    단순한 키워드 매칭을 통한 문서 검색\n",
    "    벡터 검색과 상호 보완적으로 사용하여 검색 정확도 향상\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 반환할 상위 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 관련도 순으로 정렬된 Document 객체들\n",
    "        \n",
    "    검색 로직:\n",
    "        1. 질의와 문서의 단어 교집합 계산\n",
    "        2. 교집합 크기를 질의 단어 수로 나누어 정규화\n",
    "        3. 정확한 구문 매칭 시 보너스 점수 부여\n",
    "        4. 점수 기준으로 상위 k개 문서 반환\n",
    "    \"\"\"\n",
    "    # 질의를 소문자로 변환하고 단어 단위로 분할\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서의 점수를 계산할 리스트\n",
    "    scores = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # 문서 내용을 소문자로 변환하고 단어 단위로 분할\n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 질의 단어와 문서 단어의 교집합 계산\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        \n",
    "        # 기본 점수: 교집합 크기를 질의 단어 수로 나누어 정규화 (0~1 범위)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # 보너스 점수: 질의 전체가 문서에 정확히 포함된 경우\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        # (점수, 인덱스, 문서) 튜플로 저장\n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # 점수 기준으로 내림차순 정렬하여 상위 k개 반환\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. 하이브리드 검색 함수 (벡터 + 키워드 검색 결합)\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"\n",
    "    벡터 유사도 검색과 키워드 검색을 결합한 하이브리드 검색\n",
    "    두 검색 방법의 장점을 결합하여 더 정확한 검색 결과 제공\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 최종 반환할 문서 수\n",
    "        alpha (float): 벡터 검색 가중치 (0~1, 높을수록 벡터 검색 중시)\n",
    "        \n",
    "    Returns:\n",
    "        list: 종합 점수로 정렬된 상위 k개 Document 객체들\n",
    "        \n",
    "    검색 과정:\n",
    "        1. 벡터 유사도 검색으로 의미적으로 관련된 문서 찾기\n",
    "        2. 키워드 검색으로 정확한 용어 매칭 문서 찾기\n",
    "        3. 두 결과를 alpha 가중치로 결합\n",
    "        4. 중복 문서 처리 및 최종 점수 계산\n",
    "        5. 점수 순으로 정렬하여 상위 k개 반환\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 벡터 유사도 검색 수행\n",
    "    # 더 많은 후보를 가져와서 다양성 확보 (k*2개)\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)\n",
    "    \n",
    "    # 2. 키워드 기반 검색 수행\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. 두 검색 결과를 점수와 함께 통합\n",
    "    combined_results = {}\n",
    "    \n",
    "    # 벡터 검색 결과에 점수 부여 (alpha 가중치 적용)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        # 문서 내용을 고유 키로 사용\n",
    "        doc_id = doc.page_content\n",
    "        # 순위가 높을수록 높은 점수 (1.0에서 시작하여 순위에 따라 감소)\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        \n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1,\n",
    "            'keyword_rank': None\n",
    "        }\n",
    "    \n",
    "    # 키워드 검색 결과에 점수 부여 ((1-alpha) 가중치 적용)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # 이미 벡터 검색에서 찾은 문서인 경우 점수 합산\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # 키워드 검색에서만 찾은 새로운 문서인 경우 추가\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'vector_rank': None,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. 종합 점수 기준으로 정렬하여 상위 k개 반환\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. 한국어 법률 문서 전용 프롬프트 생성 함수\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"\n",
    "    한국어 법률 문서 특성에 맞춘 전용 프롬프트 템플릿 생성\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: 법률 문서 질의응답을 위한 프롬프트 템플릿\n",
    "        \n",
    "    프롬프트 특징:\n",
    "        - 법조문 인용의 정확성 강조\n",
    "        - 전문 용어에 대한 쉬운 설명 요구\n",
    "        - 조항 간 연관성 설명 포함\n",
    "        - 실무적 적용 방법 제시\n",
    "        - 불확실한 내용에 대한 명시적 언급\n",
    "    \"\"\"\n",
    "    template = \"\"\"당신은 한국 세법 전문가입니다. 주어진 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해야 합니다.\n",
    "\n",
    "다음 규칙을 반드시 따르세요:\n",
    "1. 법조문의 조항, 항, 호, 목을 정확히 인용하세요\n",
    "2. 전문 용어를 사용할 때는 쉬운 설명을 함께 제공하세요\n",
    "3. 관련 조항들 간의 연관성을 설명하세요\n",
    "4. 실무적 적용 방법도 함께 설명하세요\n",
    "5. 불확실한 내용이 있으면 명시적으로 언급하세요\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "위 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해주세요. 관련 조항을 인용하며 설명해주세요.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"\n",
    "    하이브리드 검색과 고성능 LLM을 결합한 질문 응답 시스템\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        \n",
    "    Returns:\n",
    "        dict: 답변, 참고 문서, 사용된 컨텍스트를 포함한 응답 딕셔너리\n",
    "        \n",
    "    처리 과정:\n",
    "        1. GPT-4o-mini 모델로 LLM 초기화 (높은 정확도)\n",
    "        2. 하이브리드 검색으로 관련 문서 7개 검색\n",
    "        3. 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        4. 법률 문서 전용 프롬프트 적용\n",
    "        5. LLM으로 최종 답변 생성\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 고성능 LLM 모델 설정\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\",  # 정확도가 높은 모델 사용\n",
    "            temperature=0.1,  # 낮은 온도로 일관성 있는 답변 생성\n",
    "            max_tokens=1000   # 충분한 답변 길이 허용\n",
    "        )\n",
    "        \n",
    "        print(f\"질의: {query}\")\n",
    "        print(\"하이브리드 검색 수행 중...\")\n",
    "        \n",
    "        # 하이브리드 검색으로 관련 문서 검색\n",
    "        # k=7로 설정하여 충분한 컨텍스트 확보\n",
    "        # alpha=0.7로 설정하여 벡터 검색을 더 중시 (의미적 유사도 우선)\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"검색된 관련 문서: {len(relevant_docs)}개\")\n",
    "        \n",
    "        # 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        # 각 문서에 번호를 매겨 구분하기 쉽게 구성\n",
    "        context = \"\\n\\n\".join([f\"[문서 {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # 한국어 법률 문서에 특화된 프롬프트 사용\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # 최종 프롬프트 생성 (컨텍스트와 질문 삽입)\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM 응답 생성 중...\")\n",
    "        \n",
    "        # LLM에 프롬프트 전달하여 답변 생성\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # 결과를 딕셔너리 형태로 반환\n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "# 10. 검색 품질 평가 함수\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    검색된 문서들의 품질을 정량적으로 평가\n",
    "    \n",
    "    Args:\n",
    "        query (str): 원본 질의\n",
    "        retrieved_docs (list): 검색된 Document 객체들\n",
    "        \n",
    "    Returns:\n",
    "        float: 0~1 범위의 품질 점수 (1에 가까울수록 높은 품질)\n",
    "        \n",
    "    평가 기준:\n",
    "        1. 키워드 매칭률 (70% 가중치): 질의 단어가 문서에 포함된 비율\n",
    "        2. 문서 길이 적절성 (30% 가중치): 너무 짧거나 길지 않은 적절한 길이\n",
    "    \"\"\"\n",
    "    # 질의를 단어 단위로 분할하고 소문자 변환\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서별 품질 점수 계산\n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        # 문서 내용을 단어 단위로 분할하고 소문자 변환\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 키워드 매칭 점수 계산\n",
    "        # 질의 단어 중 문서에 포함된 단어의 비율\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # 문서 길이 점수 계산\n",
    "        # 1000자를 기준으로 정규화 (1000자 이상이면 1.0점)\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # 종합 점수 계산 (키워드 매칭 70% + 길이 적절성 30%)\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    # 전체 문서의 평균 품질 점수 반환\n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5c6147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개선된 RAG 파이프라인 실행\n",
      "==================================================\n",
      "1. 문서 로드 중...\n",
      "   문서 로드 완료: 120,207 문자\n",
      "\n",
      "2. 문서 분할 중...\n",
      "   문서 분할 완료: 273개 청크 생성\n",
      "   평균 청크 길이: 477자, 최대 길이: 600자\n",
      "\n",
      "3. 임베딩 모델 초기화...\n",
      "   임베딩 모델 초기화 완료\n",
      "\n",
      "4. 벡터 저장소 생성 중...\n",
      "   총 273개 청크를 30개씩 배치 처리...\n",
      "   배치 1/10 처리 중... (30개 문서)\n",
      "   배치 2/10 처리 중... (30개 문서)\n",
      "   배치 3/10 처리 중... (30개 문서)\n",
      "   배치 4/10 처리 중... (30개 문서)\n",
      "   배치 5/10 처리 중... (30개 문서)\n",
      "   배치 6/10 처리 중... (30개 문서)\n",
      "   배치 7/10 처리 중... (30개 문서)\n",
      "   배치 8/10 처리 중... (30개 문서)\n",
      "   배치 9/10 처리 중... (30개 문서)\n",
      "   배치 10/10 처리 중... (3개 문서)\n",
      "   모든 배치 처리 완료\n",
      "   벡터 저장소 생성 완료\n",
      "\n",
      "5. 질의 실행 중...\n",
      "질의: 비과세소득의 종류에 대하여 설명해 주세요.\n",
      "하이브리드 검색 수행 중...\n",
      "검색된 관련 문서: 7개\n",
      "LLM 응답 생성 중...\n",
      "\n",
      "============================================================\n",
      "AI의 답변:\n",
      "============================================================\n",
      "비과세소득의 종류에 대해서는 주어진 법률 문서에서 직접적으로 언급된 내용은 없지만, 일반적으로 한국 세법에서 비과세소득으로 분류되는 항목들은 다음과 같습니다. 이와 관련된 조항들을 인용하여 설명하겠습니다.\n",
      "\n",
      "### 1. 비과세소득의 정의\n",
      "비과세소득이란 세법에서 과세대상 소득으로 간주되지 않는 소득을 의미합니다. 즉, 이러한 소득은 세금을 부과받지 않으며, 이는 특정한 법률 조항에 의해 명시됩니다.\n",
      "\n",
      "### 2. 비과세소득의 예시\n",
      "비과세소득의 종류는 다음과 같습니다:\n",
      "\n",
      "- **상속세 및 증여세**: 상속이나 증여를 통해 받은 재산은 상속세 및 증여세의 과세대상이나, 특정한 조건을 충족할 경우 비과세가 될 수 있습니다.\n",
      "- **기부금**: 공익을 위한 기부금은 비과세 소득으로 인정될 수 있습니다.\n",
      "- **특정한 정부 보조금**: 정부에서 지급하는 특정 보조금이나 지원금은 비과세 소득으로 간주될 수 있습니다.\n",
      "\n",
      "### 3. 관련 조항\n",
      "주어진 문서에서 비과세소득에 대한 직접적인 언급은 없지만, 제3조(과세소득의 범위) 제1항에서는 거주자에게 과세되는 모든 소득을 규정하고 있습니다. 이 조항은 비과세소득의 개념을 이해하는 데 중요한 기초가 됩니다. 즉, 모든 소득이 과세되는 것이 아니라, 특정 소득은 비과세로 분류될 수 있다는 점을 암시합니다.\n",
      "\n",
      "### 4. 실무적 적용 방법\n",
      "비과세소득을 적용하기 위해서는 다음과 같은 절차를 따릅니다:\n",
      "\n",
      "1. **소득의 성격 파악**: 소득이 비과세에 해당하는지 여부를 판단하기 위해 해당 소득의 성격을 분석합니다.\n",
      "2. **관련 법령 확인**: 비과세소득으로 인정받기 위해서는 관련 법령을 확인하고, 필요한 서류를 준비해야 합니다.\n",
      "3. **세무신고**: 비과세소득이 발생한 경우, 세무신고 시 해당 소득을 제외하고 신고해야 합니다.\n",
      "\n",
      "### 5. 불확실한 내용\n",
      "비과세소득의 종류와 적용에 대한 구체적인 내용은 법령의 개정이나 해석에 따라 달라질 수 있습니다. 따라서, 특정 소득이 비과세에 해당하는지 여부는 세무 전문가와 상담하거나 최신 법령을 확인하는 것이 중요합니다.\n",
      "\n",
      "결론적으로, 비과세소득은 특정 조건을 충족하는 소득으로, 세법에서 명시된 규정에 따라 과세되지 않으며, 이를 정확히 이해하고 적용하기 위해서는 관련 법령을 면밀히 검토해야 합니다.\n",
      "\n",
      "============================================================\n",
      "검색 결과 요약:\n",
      "============================================================\n",
      "참고한 문서 조각 수: 7개\n",
      "컨텍스트 품질 점수: 0.23/1.00\n",
      "총 컨텍스트 길이: 3,956 문자\n",
      "\n",
      "============================================================\n",
      "참고한 문서 미리보기:\n",
      "============================================================\n",
      "\n",
      "[문서 1] . 제2항 제1항에도 불구하고 위탁자가 신탁재산을 실질적으로 통제하는 등 대통령령으로 정하는 요건을 충족하는 신탁의 경우에는 그 신탁재산에 귀속되는 소득은 위탁자에게 귀속되는 것으로 본다.<개정 제2023호 제12호 31.> [본조신설 제2020호 제12호 29.] 제3조(과세소득의 범위) 제1항 거주자에게는 이 법에서 규정하는 모든 소득에 대해서 과세한다...\n",
      "\n",
      "[문서 2] . 이자소득 나. 배당소득 다. 사업소득 라. 근로소득 마. 연금소득 바. 기타소득 제2호 퇴직소득 2의제2호 금융투자소득 제3호 양도소득 제2항 제1항에 따른 소득을 구분할 때 다음 각 호의 신탁을 제외한 신탁의 이익은 「신탁법」 제2조에 따라 수탁자에게 이전되거나 그 밖에 처분된 재산권에서 발생하는 소득의 내용별로 구분한다.<개정 제2011호 제7호 2...\n",
      "\n",
      "[문서 3] . <개정 제2010호 제12호 27., 제2014호 제1호 1., 제2018호 제12호 31., 제2020호 제12호 29., 제2022호 제12호 31.> 제1호 소득세(제57조 및 제57조의2에 따라 세액공제를 적용하는 경우의 외국소득세액을 포함한다)와 개인지방소득세 제2호 벌금ㆍ과료(통고처분에 따른 벌금 또는 과료에 해당하는 금액을 포함한다)와 과태...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # 처리할 DOCX 파일 경로 설정\n",
    "    docx_path = \"data/tax_with_table_short.docx\"\n",
    "    \n",
    "    print(\"개선된 RAG 파이프라인 실행\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1단계: 문서 로드\n",
    "    print(\"1. 문서 로드 중...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   문서 로드 완료: {len(text):,} 문자\")\n",
    "    \n",
    "    # 2단계: 문서 분할\n",
    "    print(\"\\n2. 문서 분할 중...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=600, chunk_overlap=100)\n",
    "    print(f\"   문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 청크 크기 통계 분석 및 출력\n",
    "    chunk_lengths = [len(chunk) for chunk in text_chunks]\n",
    "    avg_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "    max_length = max(chunk_lengths)\n",
    "    print(f\"   평균 청크 길이: {avg_length:.0f}자, 최대 길이: {max_length}자\")\n",
    "    \n",
    "    # 3단계: 임베딩 모델 초기화\n",
    "    print(\"\\n3. 임베딩 모델 초기화...\")\n",
    "    # 성능이 우수한 text-embedding-3-large 모델 사용\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-large\",\n",
    "    )\n",
    "    print(\"   임베딩 모델 초기화 완료\")\n",
    "    \n",
    "    # 4단계: 벡터 저장소 생성\n",
    "    print(\"\\n4. 벡터 저장소 생성 중...\")\n",
    "    # 배치 크기 30으로 설정하여 토큰 한도 문제 방지\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model, batch_size=30)\n",
    "    print(\"   벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5단계: 질의 실행\n",
    "    print(\"\\n5. 질의 실행 중...\")\n",
    "   #query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    query = \"비과세소득의 종류에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6단계: 검색 품질 평가\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7단계: 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AI의 답변:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"검색 결과 요약:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "    print(f\"컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "    print(f\"총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"참고한 문서 미리보기:\")\n",
    "    print(\"=\" * 60)\n",
    "    # 상위 3개 문서의 일부만 미리보기로 출력\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200자까지만 미리보기로 표시\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
