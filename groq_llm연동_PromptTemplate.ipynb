{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5434d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4b78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70202ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"human\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"LangServe는 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff33d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002078E36A510> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002078E36AF90> root_client=<openai.OpenAI object at 0x000002078E207620> root_async_client=<openai.AsyncOpenAI object at 0x000002078E36ACF0> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1a5303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangServe는 개발자가 대규모 언어 모델(LLM)을 쉽게 배포하고 관리할 수 있도록 지원하는 오픈 소스 라이브러리입니다. LangServe를 사용하면 개발자는 언어 모델을 API 서버로 쉽게 전환하여 다양한 애플리케이션에서 모델을 사용할 수 있습니다.\\n\\nLangServe의 주요 기능은 다음과 같습니다:\\n\\n1. **모델 배포**: LangServe를 사용하면 개발자는 훈련된 언어 모델을 쉽게 배포할 수 있습니다. 모델을 API 서버로 전환하면 다양한 애플리케이션에서 모델을 사용할 수 있습니다.\\n\\n2. **API 서버**: LangServe는 언어 모델을 API 서버로 전환하여 클라이언트 애플리케이션이 모델에 쉽게 접근할 수 있도록 합니다. 이를 통해 개발자는 모델을 웹 애플리케이션, 모바일 애플리케이션, 또는 다른 시스템과 통합할 수 있습니다.\\n\\n3. **모델 관리**: LangServe는 모델의 버전 관리, 업데이트, 모니터링 등을 지원하여 모델을 효율적으로 관리할 수 있습니다.\\n\\n4. **확장성**: LangServe는 수평 확장성을 지원하여 대규모 트래픽을 처리할 수 있습니다.\\n\\n5. **보안**: LangServe는 데이터 암호화, 접근 제어 등의 보안 기능을 제공하여 모델과 데이터를 안전하게 보호합니다.\\n\\n6. **다양한 모델 지원**: LangServe는 다양한 언어 모델을 지원하며, 특히 Hugging Face의 Transformers 라이브러리와 긴밀하게 통합됩니다.\\n\\nLangServe를 사용하는 이유는 다음과 같습니다:\\n\\n* 대규모 언어 모델을 쉽게 배포하고 관리할 수 있습니다.\\n* 언어 모델을 API 서버로 전환하여 다양한 애플리케이션에서 사용할 수 있습니다.\\n* 모델의 버전 관리, 업데이트, 모니터링 등을 쉽게 할 수 있습니다.\\n* 확장성과 보안이 뛰어납니다.\\n\\nLangServe의 사용 사례는 다음과 같습니다:\\n\\n* 챗봇 개발: LangServe를 사용하여 언어 모델을 배포하고 관리함으로써 챗봇을 개발할 수 있습니다.\\n* 언어 번역: LangServe를 사용하여 언어 번역 모델을 배포하고 관리함으로써 번역 서비스를 제공할 수 있습니다.\\n* 텍스트 요약: LangServe를 사용하여 텍스트 요약 모델을 배포하고 관리함으로써 기사 또는 문서의 요약을 제공할 수 있습니다.\\n\\n결론적으로, LangServe는 대규모 언어 모델을 쉽게 배포하고 관리할 수 있는 오픈 소스 라이브러리입니다. 이를 통해 개발자는 언어 모델을 API 서버로 전환하여 다양한 애플리케이션에서 사용할 수 있으며, 모델의 버전 관리, 업데이트, 모니터링 등을 쉽게 할 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 475, 'prompt_tokens': 30, 'total_tokens': 505, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.300551306, 'prompt_time': 0.003034687, 'completion_time': 1.154676738, 'total_time': 1.157711425}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-ac5bad85-9f10-4e4b-b446-5a908d65bdb7', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--3b699040-9e27-47c8-8a24-698fbeed6ec9-0' usage_metadata={'input_tokens': 30, 'output_tokens': 475, 'total_tokens': 505, 'input_token_details': {}, 'output_token_details': {}}\n",
      "응답: LangServe는 개발자가 대규모 언어 모델(LLM)을 쉽게 배포하고 관리할 수 있도록 지원하는 오픈 소스 라이브러리입니다. LangServe를 사용하면 개발자는 언어 모델을 API 서버로 쉽게 전환하여 다양한 애플리케이션에서 모델을 사용할 수 있습니다.\n",
      "\n",
      "LangServe의 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **모델 배포**: LangServe를 사용하면 개발자는 훈련된 언어 모델을 쉽게 배포할 수 있습니다. 모델을 API 서버로 전환하면 다양한 애플리케이션에서 모델을 사용할 수 있습니다.\n",
      "\n",
      "2. **API 서버**: LangServe는 언어 모델을 API 서버로 전환하여 클라이언트 애플리케이션이 모델에 쉽게 접근할 수 있도록 합니다. 이를 통해 개발자는 모델을 웹 애플리케이션, 모바일 애플리케이션, 또는 다른 시스템과 통합할 수 있습니다.\n",
      "\n",
      "3. **모델 관리**: LangServe는 모델의 버전 관리, 업데이트, 모니터링 등을 지원하여 모델을 효율적으로 관리할 수 있습니다.\n",
      "\n",
      "4. **확장성**: LangServe는 수평 확장성을 지원하여 대규모 트래픽을 처리할 수 있습니다.\n",
      "\n",
      "5. **보안**: LangServe는 데이터 암호화, 접근 제어 등의 보안 기능을 제공하여 모델과 데이터를 안전하게 보호합니다.\n",
      "\n",
      "6. **다양한 모델 지원**: LangServe는 다양한 언어 모델을 지원하며, 특히 Hugging Face의 Transformers 라이브러리와 긴밀하게 통합됩니다.\n",
      "\n",
      "LangServe를 사용하는 이유는 다음과 같습니다:\n",
      "\n",
      "* 대규모 언어 모델을 쉽게 배포하고 관리할 수 있습니다.\n",
      "* 언어 모델을 API 서버로 전환하여 다양한 애플리케이션에서 사용할 수 있습니다.\n",
      "* 모델의 버전 관리, 업데이트, 모니터링 등을 쉽게 할 수 있습니다.\n",
      "* 확장성과 보안이 뛰어납니다.\n",
      "\n",
      "LangServe의 사용 사례는 다음과 같습니다:\n",
      "\n",
      "* 챗봇 개발: LangServe를 사용하여 언어 모델을 배포하고 관리함으로써 챗봇을 개발할 수 있습니다.\n",
      "* 언어 번역: LangServe를 사용하여 언어 번역 모델을 배포하고 관리함으로써 번역 서비스를 제공할 수 있습니다.\n",
      "* 텍스트 요약: LangServe를 사용하여 텍스트 요약 모델을 배포하고 관리함으로써 기사 또는 문서의 요약을 제공할 수 있습니다.\n",
      "\n",
      "결론적으로, LangServe는 대규모 언어 모델을 쉽게 배포하고 관리할 수 있는 오픈 소스 라이브러리입니다. 이를 통해 개발자는 언어 모델을 API 서버로 전환하여 다양한 애플리케이션에서 사용할 수 있으며, 모델의 버전 관리, 업데이트, 모니터링 등을 쉽게 할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(response)\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafbd5a1",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c34c649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}에 대해 쉽게 반드시 한글로 설명해주세요.\")\\n    ')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 반드시 한글로 설명해주세요.\")\n",
    "    \"\"\")                                     \n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d392dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbba4772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain 연결 (LCEL)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084bfec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "인공지능 모델의 학습 원리는 컴퓨터가 데이터를 통해 스스로 학습하고, 이를 바탕으로 미래의 새로운 데이터에 대해 예측하거나 결정을 내릴 수 있도록 하는 것입니다. 이를 위해 인공지능 모델은 주어진 데이터를 분석하고, 패턴을 발견하며, 규칙을 학습합니다.\n",
      "\n",
      "가장 기본적인 학습 원리는 다음과 같습니다.\n",
      "\n",
      "1. **데이터 수집**: 인공지능 모델을 학습시키기 위해서는 많은 양의 데이터가 필요합니다. 이 데이터는 문제에 따라 달라지며, 이미지, 텍스트, 오디오 등 다양한 형태일 수 있습니다.\n",
      "\n",
      "2. **데이터 전처리**: 수집된 데이터는 모델에 입력되기 전에 전처리 과정을 거칩니다. 이 과정에서는 데이터의 잡음을 제거하거나, 데이터를 정규화하는 등의 작업이 이루어집니다.\n",
      "\n",
      "3. **모델 선택**: 인공지능 모델에는 여러 가지 종류가 있습니다. 예를 들어, 이미지 인식에는 합성곱 신경망(CNN), 자연어 처리에는 순환 신경망(RNN) 또는 트랜스포머 등이 사용됩니다. 적합한 모델을 선택하는 것이 중요합니다.\n",
      "\n",
      "4. **학습**: 선택된 모델에 전처리된 데이터를 입력하여 학습을 시작합니다. 이 과정에서는 모델이 데이터의 패턴을 인식하고, 이에 맞춰 파라미터를 조정합니다. 학습 과정은 보통 최적화 알고리즘을 통해 이루어지며, 모델의 예측 결과와 실제 값 사이의 오류를 최소화하는 방향으로 파라미터가 조정됩니다.\n",
      "\n",
      "5. **평가**: 학습이 완료된 후, 모델의 성능을 평가합니다. 이를 위해 별도의 테스트 데이터를 사용하며, 모델의 예측 정확도, 오차율 등을 계산합니다.\n",
      "\n",
      "6. **튜닝**: 모델의 성능이 만족스럽지 않을 경우, 하이퍼파라미터를 조정하거나 모델 구조를 변경하는 등의 튜닝 과정을 거칩니다.\n",
      "\n",
      "예를 들어, 어린 아이에게 사과와 바나나의 사진을 보여주고 이 둘을 구별하라고 하면, 처음에는 구별하지 못할 것입니다. 하지만 여러 번 사과와 바나나를 보여주고, 이것이 사과이고, 이것이 바나나라고 설명해 주면, 아이는 어느 순간부터는 사과와 바나나를 구별할 수 있게 됩니다. 인공지능 모델의 학습 원리도 이와 유사합니다. 모델에게 많은 데이터를 제공하고, 이것이 어떤 것인지를 설명해 주면, 모델은 스스로 학습하여 새로운 사과와 바나나의 사진을 구별할 수 있게 됩니다.\n",
      "\n",
      "이러한 학습 원리는 다양한 분야에서 활용되고 있으며, 자율 주행 자동차, 의료 진단, 언어 번역 등 많은 응용 분야에서 인공지능 기술이 사용되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33153ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \": LangChain의 Products(제품)는 어떤 것들이 있나요? 예를 들어 LangSmith, LangServe 같은 Product가 있어\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d50087",
   "metadata": {},
   "source": [
    "\n",
    "###Runnable의 stream() 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfabd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해 주세요.\"})\n",
    "    \n",
    "    # 스트리밍 출력\n",
    "    #print(answer)\n",
    "    for token in answer:\n",
    "        # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "        print(token, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a759f26",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "* 첫번째 Chain의 출력이, 두번째 Chain의 입력이 된다.\n",
    "* 두개의 Chain과 Prompt + OutputParser를 LCEL로 연결하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acf368a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001BD5B7D96D0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001BD5B7DB750> root_client=<openai.OpenAI object at 0x000001BD5B7DB110> root_async_client=<openai.AsyncOpenAI object at 0x000001BD5B7DB890> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 15문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbbc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1의 출력을 movie 입력 변수로 전달\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "    response = chain2.invoke({\"genre\": \"성인\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eab14",
   "metadata": {},
   "source": [
    "## PromptTemplate 여러개 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1595b579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 2 문장으로 한국어로 답변해 주세요.', 'Gemma 모델의 학습 원리를 3 문장으로 한국어로 답변해 주세요.', 'llama*4 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama*4\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3747ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt) #AIMessage\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da13a8",
   "metadata": {},
   "source": [
    "### System_messagePromptTemplate\n",
    "* HumanMessagePromptTemplate, AIMessagePromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092be190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"당신은 {topic} 전문가 입니다다. 명확하고 자세하게 설명해 주세요.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"딥러닝은 무엇인가요?\")\n",
    "\n",
    "# LLM 호출\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efed56",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "* 프롬프트에서 예시를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39112b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 태양계의 행성\n",
      "\n",
      "1. **수성**: 태양과 가장 가까운 행성으로, 매우 작은 크기와 높은 온도를 가지고 있습니다.\n",
      "2. **금성**: 밝고 뜨거운 행성으로, 강한 온실 효과로 인해 표면 온도가 매우 높습니다.\n",
      "3. **지구**: 생명체가 존재하는 유일한 행성으로, 대기 구성과 물이 있어 생명 유지에 적합합니다.\n",
      "4. **화성**: 붉은 행성으로, 과거에 물이 있었을 것으로 추정되며, 현재는 암석과 모래로 덮여 있습니다.\n",
      "5. **목성**: 태양계에서 가장 큰 행성으로, 가스 거인이며 수많은 위성을 가지고 있습니다.\n",
      "6. **토성**: 아름다운 반지로 유명한 가스 거인으로, 많은 위성을 가지고 있습니다.\n",
      "7. **천왕성**: 얼음 거인으로, 자전축이 기울어져 있어 극단적인 계절 변화를 경험합니다.\n",
      "8. **해왕성**: 가장 먼 행성으로, 강한 바람과 깊은 푸른색을 띠는 대기층을 가지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"뉴턴의 운동 법칙을 요약해 주세요.\",\n",
    "        \"output\": \"\"\"### 뉴턴의 운동 법칙\n",
    "1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\n",
    "2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\n",
    "3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"지구의 대기 구성 요소를 알려주세요.\",\n",
    "        \"output\": \"\"\"### 지구 대기의 구성\n",
    "- **질소 (78%)**: 대기의 대부분을 차지합니다.\n",
    "- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\n",
    "- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\n",
    "- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate 적용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 구성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 초등학생도 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 생성 및 체인 구성\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# 테스트 실행\n",
    "result = chain.invoke({\"input\": \"태양계의 행성들을 간략히 정리해 주세요.\"})\n",
    "#result = chain.invoke({\"input\": \"양자 얽힘이 무엇인가요?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf5e1d",
   "metadata": {},
   "source": [
    "### PartialPromptTemplate\n",
    "* 프롬프트의 입력 값에 함수 호출 이나 외부 API를 호출한 동적인 값을 대입할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ceb891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 계절: 겨울\n",
      "🔹 프롬프트: <bound method ChatPromptTemplate.from_template of <class 'langchain_core.prompts.chat.ChatPromptTemplate'>>\n",
      "🔹 모델 응답: 겨울에 발생하는 자연 현상 : \n",
      " 겨울에 주로 발생하는 대표적인 지구과학 현상은 다음과 같습니다.\n",
      "\n",
      "1.  **극광**: 극광은 태양풍이 지구 자기장에 의해 극지방으로 끌려가면서 대기 입자와 충돌하여 발생하는 현상입니다. 이 충돌로 인해 대기 입자들이 에너지를 얻고 빛을 방출하게 되는데, 이 빛이 극광으로 나타납니다. 극광은 주로 북극과 남극 지역에서 볼 수 있으며, 겨울에 더욱 선명하게 나타납니다.\n",
      "2.  **빙하**: 빙하는 극지방이나 고산 지역에서 발생하는 현상으로, 눈이 쌓여 얼어붙은 얼음 덩어리입니다. 빙하는 지구의 기후 변화와 관련이 있으며, 겨울에 더욱 커지고 두꺼워집니다.\n",
      "3.  **성층권 오존 구멍**: 겨울에 오존 구멍이 형성되는 이유는 성층권에서 오존을 파괴하는 물질들이 극지방에서 집중되어 있기 때문입니다. 이러한 물질들은 주로 염소와 브롬 화합물로, 이들은 극지방에서 극저온으로 인해 얼어붙어 있는 구름 입자 위에 응축됩니다. 이 구름 입자들은 극지방의 밤 동안 극심한 추위로 인해 형성되며, 이 구름 입자 위에서 오존을 파괴하는 화학 반응이 활발하게 일어나 오존 구멍이 형성됩니다. 오존 구멍은 자외선 차단 역할을 하는 오존층에 구멍이 뚫리는 현상으로, 자외선에 민감한 생물에게 해를 끼칠 수 있습니다.\n",
      "\n",
      "이러한 현상들은 지구과학에서 중요한 연구 주제이며, 기후 변화와 지구 환경에 대한 이해를 높이는 데 도움이 됩니다.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# 계절을 결정하는 함수 (남반구/북반구 고려)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "    \n",
    "    if hemisphere == \"north\":  # 북반구 (기본값)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"봄\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"여름\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"가을\"\n",
    "        else:\n",
    "            return \"겨울\"\n",
    "    else:  # 남반구 (계절 반대)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"가을\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"겨울\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"봄\"\n",
    "        else:\n",
    "            return \"여름\"\n",
    "\n",
    "# 프롬프트 템플릿 정의 (부분 변수 적용)\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"{season}에 일어나는 대표적인 지구과학 현상은 {phenomenon}입니다.\",\n",
    "#     input_variables=[\"phenomenon\"],  # 사용자 입력 필요\n",
    "#     partial_variables={\"season\": get_current_season(\"south\")}  # 동적으로 계절 값 할당\n",
    "# )\n",
    "\n",
    "season = get_current_season(\"south\")\n",
    "print(f\"현재 계절: {season}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"{season}에 주로 발생하는 대표적인 지구과학 현상 3가지를 알려주세요. \"\n",
    "    \"각 현상에 대해 간단한 설명을 포함해주세요.\"\n",
    ")\n",
    "\n",
    "# OpenAI 모델 초기화\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# 특정 계절의 현상 질의\n",
    "chain = (\n",
    "    {\"season\": lambda x: season}\n",
    "    | prompt\n",
    "    | model\n",
    "    |StrOutputParser()\n",
    ")\n",
    "result = chain.invoke({})\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"🔹 프롬프트: {prompt}\") #동일 -> {prompt.from_template\"}\n",
    "print(f\"🔹 모델 응답: {season}에 발생하는 자연 현상 : \\n {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b0e06",
   "metadata": {},
   "source": [
    "### 2. API를 호출하여 실시간 정보를 동적인 값을  partial variable로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b32b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={'info': '1달러 = 1365.14원'} template='현재 {info} 기준으로 환율 정보를 알려드립니다. 현재 환율을 기준으로 한국경제에 미치는 영향 및 향후대책에 대한 분석을 제공해 주세요.'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 실시간 환율을 가져오는 함수\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1달러 = {data['rates']['KRW']}원\"\n",
    "\n",
    "# {info} 변수에 API에서 받은 환율 정보를 동적으로 반영\n",
    "prompt = PromptTemplate(\n",
    "    template=\"현재 {info} 기준으로 환율 정보를 알려드립니다. 현재 환율을 기준으로 한국경제에 미치는 영향 및 향후대책에 대한 분석을 제공해 주세요.\",\n",
    "    input_variables=[],  # 사용자 입력 없음\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # API에서 가져온 데이터 자동 반영\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1da566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 프롬프트: 현재 1달러 = 1365.14원 기준으로 환율 정보를 알려드립니다. 현재 환율을 기준으로 한국경제에 미치는 영향 및 향후대책에 대한 분석을 제공해 주세요.\n",
      "🔹 모델 응답: ## 1. 현재 환율 상황\n",
      "2024년 4월 5일, 1달러 = 1,365.14원입니다. 최근 환율은 글로벌 경제 상황, 미국의 통화 정책, 한국의 수출입 동향 등 다양한 요인에 의해 영향을 받고 있습니다.\n",
      "\n",
      "### 2. 한국 경제에 미치는 영향\n",
      "\n",
      "#### (1) 수출입\n",
      "- **수출**: 높은 환율은 한국의 수출을 촉진할 수 있습니다. 원화 약세로 인해 한국 상품의 가격이 해외 시장에서 상대적으로 저렴해지기 때문입니다. 이는 특히 자동차, 반도체, 철강 등 주요 수출 산업에 긍정적인 영향을 미칠 수 있습니다.\n",
      "- **수입**: 반대로, 높은 환율은 수입 물가를 상승시켜 국내 물가 상승 압력을 가중시킬 수 있습니다. 원유, 원자재, 전자제품 등 수입에 의존하는 산업의 비용 부담이 증가할 수 있습니다.\n",
      "\n",
      "#### (2) 물가 상승\n",
      "- 높은 환율은 수입 물가 상승을 통해 소비자 물가 지수를 상승시킬 수 있습니다. 이는 가계의 실질소득 감소를 초래하고, 소비 심리를 위축시킬 수 있습니다.\n",
      "\n",
      "#### (3) 금융 시장\n",
      "- 원화 약세는 외국인 투자자들에게 한국 자산(주식, 채권 등)의 매력을 감소시킬 수 있습니다. 이는 외국인 자금의 유출로 이어질 수 있으며, 국내 금융 시장의 변동성을 증가시킬 수 있습니다.\n",
      "\n",
      "#### (4) 기업 실적\n",
      "- **수출 기업**: 환율 상승으로 인해 수출 기업의 실적이 개선될 수 있습니다. 그러나 원자재를 수입하는 기업들은 비용 부담이 증가할 수 있습니다.\n",
      "- **내수 기업**: 내수 중심 기업들은 높은 물가에 따른 소비 위축으로 부정적인 영향을 받을 수 있습니다.\n",
      "\n",
      "### 3. 향후 대책\n",
      "\n",
      "#### (1) 통화 정책\n",
      "- **한국은행**: 높은 물가 상승률과 환율 변동성을 고려하여, 금리 인상 등 긴축 통화 정책을 시행할 수 있습니다. 이는 물가 안정과 금융 시장의 안정성을 확보하는 데 도움이 될 수 있습니다.\n",
      "\n",
      "#### (2) 재정 정책\n",
      "- **정부**: 수출 지원 정책을 강화하거나, 물가 안정화 대책을 마련할 수 있습니다. 예를 들어, 수출 보조금 지원, 원자재 가격 안정화 대책, 생필품 가격 통제 등 다양한 정책을 시행할 수 있습니다.\n",
      "\n",
      "#### (3) 외환 시장 개입\n",
      "- 한국은행은 외환 시장에서 달러를 매입하거나 매도하여 환율을 안정화시키는 방안을 고려할 수 있습니다. 그러나 이는 한계가 있으며, 시장 원리에 의해 환율이 결정되는 글로벌 금융 시장에서 효과적인지 여부는 논란의 여지가 있습니다.\n",
      "\n",
      "#### (4) 구조적 개선\n",
      "- 장기적으로는 한국 경제의 구조적 문제를 해결해야 합니다. 예를 들어, 수출 의존도를 낮추고, 내수 시장을 활성화하는 정책이 필요합니다. 또한, 원자재 수입 의존도를 줄이기 위해 국내 생산을 확대하고, 대체 수입처를 모색하는 등의 전략이 중요합니다.\n",
      "\n",
      "### 4. 결론\n",
      "현재의 높은 환율은 한국 경제에 복합적인 영향을 미치고 있습니다. 수출에는 긍정적이지만, 물가 상승과 금융 시장의 변동성 증가 등 부정적인 영향도 존재합니다. 향후 대책으로는 통화 정책, 재정 정책, 외환 시장 개입, 구조적 개선 등 다각적인 접근이 필요합니다. 경제의 안정성과 지속 가능한 성장을 위해, 정부와 한국은행의 적극적인 대응이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "# LLM 모델 설정 (GPT-4o-mini 사용)\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 모델에 프롬프트 전달 및 응답 받기\n",
    "response = model.invoke(prompt.format())\n",
    "\n",
    "# 결과 출력\n",
    "print(\"🔹 프롬프트:\", prompt.format())\n",
    "print(\"🔹 모델 응답:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
