{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c90167bb",
   "metadata": {},
   "source": [
    "### 1.Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9719b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 원본 텍스트:\n",
      "--------------------------------------------------\n",
      "RAG는 검색 기반의 텍스트 생성 모델입니다. 기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다. \n",
      "RAG는 검색과 생성 단계를 포함합니다. 먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다.\n",
      "이 방식은 환상(hallucination) 문제를 크게 줄여줍니다. 또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다.\n",
      "\n",
      " 원본 길이: 235자\n",
      "\n",
      "============================================================\n",
      " 다양한 CharacterTextSplitter 설정 비교\n",
      "============================================================\n",
      "\n",
      " 마침표(.) 기준 분할:\n",
      "------------------------------\n",
      "청크 1: 'RAG는 검색 기반의 텍스트 생성 모델입니다' (길이: 24자)\n",
      "청크 2: '기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다' (길이: 32자)\n",
      "청크 3: '특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다' (길이: 32자)\n",
      "청크 4: 'RAG는 검색과 생성 단계를 포함합니다' (길이: 21자)\n",
      "청크 5: '먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다' (길이: 43자)\n",
      "청크 6: '이 방식은 환상(hallucination) 문제를 크게 줄여줍니다' (길이: 36자)\n",
      "청크 7: '또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다' (길이: 33자)\n",
      "\n",
      " 문장 기준 분할 (큰 청크):\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# ===================================\n",
    "# 예제 텍스트\n",
    "# ===================================\n",
    "text = \"\"\"RAG는 검색 기반의 텍스트 생성 모델입니다. 기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다.\n",
    "특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다. \n",
    "RAG는 검색과 생성 단계를 포함합니다. 먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다.\n",
    "이 방식은 환상(hallucination) 문제를 크게 줄여줍니다. 또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다.\"\"\"\n",
    "\n",
    "print(\" 원본 텍스트:\")\n",
    "print(\"-\" * 50)\n",
    "print(text)\n",
    "print(f\"\\n 원본 길이: {len(text)}자\")\n",
    "\n",
    "# ===================================\n",
    "#  다양한 분할 방식 비교\n",
    "# ===================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 다양한 CharacterTextSplitter 설정 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 기본 설정 (마침표 기준)\n",
    "print(\"\\n 마침표(.) 기준 분할:\")\n",
    "print(\"-\" * 30)\n",
    "splitter1 = CharacterTextSplitter(\n",
    "    chunk_size=50,      # 청크 최대 크기\n",
    "    chunk_overlap=10,   # 청크 간 중복\n",
    "    separator=\".\"       # 분할 기준\n",
    ")\n",
    "chunks1 = splitter1.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks1, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "\n",
    "#  문장 기준 (좀 더 큰 청크)\n",
    "print(\"\\n 문장 기준 분할 (큰 청크):\")\n",
    "print(\"-\" * 30)\n",
    "splitter2 = CharacterTextSplitter(\n",
    "    chunk_size=100,     # 더 큰 청크\n",
    "    chunk_overlap=20,   # 더 많은 중복\n",
    "    separator=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f811adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청크 1: 'RAG는 검색 기반의 텍스트 생성 모델입니다. 기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다' (길이: 92자)\n",
      "청크 2: 'RAG는 검색과 생성 단계를 포함합니다. 먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다' (길이: 66자)\n",
      "청크 3: '이 방식은 환상(hallucination) 문제를 크게 줄여줍니다. 또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다' (길이: 71자)\n",
      "\n",
      " 줄바꿈(\\n) 기준 분할:\n",
      "------------------------------\n",
      "청크 1: 'RAG는 검색 기반의 텍스트 생성 모델입니다. 기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다.' (길이: 59자)\n",
      "청크 2: '특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다.' (길이: 33자)\n",
      "청크 3: 'RAG는 검색과 생성 단계를 포함합니다. 먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다.' (길이: 67자)\n",
      "청크 4: '이 방식은 환상(hallucination) 문제를 크게 줄여줍니다. 또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다.' (길이: 72자)\n",
      "\n",
      " 공백(' ') 기준 분할 (단어 단위):\n",
      "------------------------------\n",
      "청크 1: 'RAG는 검색 기반의 텍스트 생성 모델입니다. 기존' (길이: 28자)\n",
      "청크 2: '기존 언어 모델의 단점을 보완하고, 최신 정보를' (길이: 26자)\n",
      "청크 3: '정보를 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데' (길이: 29자)\n",
      "청크 4: '데 강력한 기능을 제공합니다. \n",
      "RAG는 검색과 생성' (길이: 29자)\n",
      "청크 5: '생성 단계를 포함합니다. 먼저 관련 문서를 검색하고,' (길이: 29자)\n",
      "... 총 10개 청크 생성됨\n",
      "\n",
      "============================================================\n",
      " 설정별 결과 요약\n",
      "============================================================\n",
      "마침표 기준 (50자)   :  7개 청크, 평균 31.6자\n",
      "마침표 기준 (100자)  :  3개 청크, 평균 76.3자\n",
      "줄바꿈 기준         :  4개 청크, 평균 57.8자\n",
      "공백 기준          : 10개 청크, 평균 25.9자\n",
      "\n",
      "============================================================\n",
      " chunk_overlap 효과 확인\n",
      "============================================================\n",
      "\n",
      " 중복 없음 (overlap=0):\n",
      "청크 1: 'RAG는 검색 기반의 텍스트 생성 모델입니다'\n",
      "청크 2: '기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다'\n",
      "청크 3: '특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다'\n",
      "청크 4: 'RAG는 검색과 생성 단계를 포함합니다'\n",
      "청크 5: '먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다'\n",
      "청크 6: '이 방식은 환상(hallucination) 문제를 크게 줄여줍니다'\n",
      "청크 7: '또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다'\n",
      "\n",
      " 중복 있음 (overlap=15):\n",
      "청크 1: 'RAG는 검색 기반의 텍스트 생성 모델입니다'\n",
      "청크 2: '기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다'\n",
      "청크 3: '특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다'\n",
      "청크 4: 'RAG는 검색과 생성 단계를 포함합니다'\n",
      "청크 5: '먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다'\n",
      "청크 6: '이 방식은 환상(hallucination) 문제를 크게 줄여줍니다'\n",
      "청크 7: '또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다'\n",
      "\n",
      "============================================================\n",
      " 실무 활용 팁\n",
      "============================================================\n",
      "\n",
      " 청크 크기 가이드:\n",
      "   • 짧은 문서: 200-500자\n",
      "   • 긴 문서: 500-1000자\n",
      "   • 매우 긴 문서: 1000-2000자\n",
      "\n",
      " chunk_overlap 가이드:\n",
      "   • 일반적: 청크 크기의 10-20%\n",
      "   • 맥락 중요: 청크 크기의 20-30%\n",
      "   • 속도 중요: 0-10%\n",
      "\n",
      " separator 선택:\n",
      "   • 문서: 문단(\n",
      "\n",
      ") 또는 문장(.)\n",
      "   • 코드: 함수나 클래스 단위\n",
      "   • 대화: 발화자 변경 지점\n",
      "\n",
      " RAG 최적화:\n",
      "   • 너무 작으면 맥락 손실\n",
      "   • 너무 크면 관련성 저하\n",
      "   • 적절한 중복으로 연결성 유지\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks2 = splitter2.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks2, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "\n",
    "#  줄바꿈 기준\n",
    "print(\"\\n 줄바꿈(\\\\n) 기준 분할:\")\n",
    "print(\"-\" * 30)\n",
    "splitter3 = CharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=0,    # 중복 없음\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "chunks3 = splitter3.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks3, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "\n",
    "#  공백 기준 (단어 단위)\n",
    "print(\"\\n 공백(' ') 기준 분할 (단어 단위):\")\n",
    "print(\"-\" * 30)\n",
    "splitter4 = CharacterTextSplitter(\n",
    "    chunk_size=30,      # 작은 청크\n",
    "    chunk_overlap=5,\n",
    "    separator=\" \"       # 공백으로 분할\n",
    ")\n",
    "chunks4 = splitter4.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks4[:5], 1):  # 처음 5개만 출력\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "print(f\"... 총 {len(chunks4)}개 청크 생성됨\")\n",
    "\n",
    "# ===================================\n",
    "#  설정별 결과 비교\n",
    "# ===================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 설정별 결과 요약\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    (\"마침표 기준 (50자)\", len(chunks1), chunks1),\n",
    "    (\"마침표 기준 (100자)\", len(chunks2), chunks2),\n",
    "    (\"줄바꿈 기준\", len(chunks3), chunks3),\n",
    "    (\"공백 기준\", len(chunks4), chunks4)\n",
    "]\n",
    "\n",
    "for name, count, chunks in results:\n",
    "    avg_length = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "    print(f\"{name:15}: {count:2}개 청크, 평균 {avg_length:.1f}자\")\n",
    "\n",
    "# ===================================\n",
    "#  chunk_overlap 효과 확인\n",
    "# ===================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" chunk_overlap 효과 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 중복 없음\n",
    "splitter_no_overlap = CharacterTextSplitter(\n",
    "    chunk_size=50, chunk_overlap=0, separator=\".\"\n",
    ")\n",
    "chunks_no_overlap = splitter_no_overlap.split_text(text)\n",
    "\n",
    "# 중복 있음\n",
    "splitter_with_overlap = CharacterTextSplitter(\n",
    "    chunk_size=50, chunk_overlap=15, separator=\".\"\n",
    ")\n",
    "chunks_with_overlap = splitter_with_overlap.split_text(text)\n",
    "\n",
    "print(\"\\n 중복 없음 (overlap=0):\")\n",
    "for i, chunk in enumerate(chunks_no_overlap, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}'\")\n",
    "\n",
    "print(\"\\n 중복 있음 (overlap=15):\")\n",
    "for i, chunk in enumerate(chunks_with_overlap, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}'\")\n",
    "    if i > 1:  # 두 번째 청크부터 중복 부분 표시\n",
    "        prev_chunk = chunks_with_overlap[i-2].strip()\n",
    "        curr_chunk = chunk.strip()\n",
    "        # 간단한 중복 확인\n",
    "        if len(prev_chunk) > 10 and len(curr_chunk) > 10:\n",
    "            if prev_chunk[-10:] in curr_chunk:\n",
    "                print(f\"    이전 청크와 중복: '{prev_chunk[-10:]}'\")\n",
    "\n",
    "# ===================================\n",
    "#  실무 팁\n",
    "# ===================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 실무 활용 팁\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21160041",
   "metadata": {},
   "source": [
    "\n",
    "### 2. RicursiveCharacter TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7a783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트:\n",
      "--------------------------------------------------\n",
      "RAG는 검색과 생성 단계를 포함하는 모델입니다.\n",
      "\n",
      "이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다.\n",
      "\n",
      "Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다.\n",
      "\n",
      "RAG의 핵심은 검색과 생성의 결합입니다! 먼저 관련 문서를 찾고, 그 정보를 바탕으로 답변을 만듭니다.\n",
      "\n",
      "텍스트 길이: 230자\n",
      "\n",
      "============================================================\n",
      "RecursiveCharacterTextSplitter vs CharacterTextSplitter 비교\n",
      "============================================================\n",
      "\n",
      "1. RecursiveCharacterTextSplitter (계층적 분할):\n",
      "---------------------------------------------\n",
      "Chunk 1: 'RAG는 검색과 생성 단계를 포함하는 모델입니다.'\n",
      "길이: 27자\n",
      "\n",
      "Chunk 2: '이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다.'\n",
      "길이: 67자\n",
      "\n",
      "Chunk 3: 'Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다.'\n",
      "길이: 72자\n",
      "\n",
      "Chunk 4: 'RAG의 핵심은 검색과 생성의 결합입니다! 먼저 관련 문서를 찾고, 그 정보를 바탕으로 답변을 만듭니다.'\n",
      "길이: 58자\n",
      "\n",
      "2. CharacterTextSplitter (단순 분할):\n",
      "-----------------------------------\n",
      "Chunk 1: 'RAG는 검색과 생성 단계를 포함하는 모델입니다.\n",
      "\n",
      "이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다'\n",
      "길이: 58자\n",
      "\n",
      "Chunk 2: '특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다'\n",
      "길이: 35자\n",
      "\n",
      "Chunk 3: 'Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다'\n",
      "길이: 71자\n",
      "\n",
      "Chunk 4: 'RAG의 핵심은 검색과 생성의 결합입니다! 먼저 관련 문서를 찾고, 그 정보를 바탕으로 답변을 만듭니다'\n",
      "길이: 57자\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 예제 텍스트\n",
    "text = \"\"\"RAG는 검색과 생성 단계를 포함하는 모델입니다.\n",
    "\n",
    "이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다.\n",
    "특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다.\n",
    "\n",
    "Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다.\n",
    "\n",
    "RAG의 핵심은 검색과 생성의 결합입니다! 먼저 관련 문서를 찾고, 그 정보를 바탕으로 답변을 만듭니다.\"\"\"\n",
    "\n",
    "print(\"원본 텍스트:\")\n",
    "print(\"-\" * 50)\n",
    "print(text)\n",
    "print(f\"\\n텍스트 길이: {len(text)}자\")\n",
    "\n",
    "# ===========================================\n",
    "# Recursive vs Character 비교\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RecursiveCharacterTextSplitter vs CharacterTextSplitter 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. RecursiveCharacterTextSplitter (추천)\n",
    "print(\"\\n1. RecursiveCharacterTextSplitter (계층적 분할):\")\n",
    "print(\"-\" * 45)\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]  # 우선순위 순서\n",
    ")\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"Chunk {i+1}: '{chunk.strip()}'\")\n",
    "    print(f\"길이: {len(chunk)}자\")\n",
    "    print()\n",
    "\n",
    "# 2. CharacterTextSplitter (비교용)\n",
    "print(\"2. CharacterTextSplitter (단순 분할):\")\n",
    "print(\"-\" * 35)\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "simple_splitter = CharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    separator=\".\"  # 하나의 구분자만 사용\n",
    ")\n",
    "simple_chunks = simple_splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(simple_chunks):\n",
    "    print(f\"Chunk {i+1}: '{chunk.strip()}'\")\n",
    "    print(f\"길이: {len(chunk)}자\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# separators 우선순위 테스트\n",
    "# ===========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"separators 우선순위 동작 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_text = \"\"\"첫 번째 문단입니다.\n",
    "\n",
    "두 번째 문단입니다.\n",
    "이 문단은 여러 문장으로 구성됩니다! 정말 흥미롭죠?\n",
    "\n",
    "세 번째 문단입니다.\"\"\"\n",
    "\n",
    "print(\"테스트 텍스트:\")\n",
    "print(repr(test_text))  # 줄바꿈 문자까지 보이도록\n",
    "\n",
    "# 다양한 separators 설정 테스트\n",
    "separators_configs = [\n",
    "    ([\"\\n\\n\", \"\\n\", \".\", \" \"], \"문단 우선\"),\n",
    "    ([\"\\n\", \".\", \" \"], \"줄바꿈 우선\"),\n",
    "    ([\".\", \"!\", \"?\", \" \"], \"문장 우선\"),\n",
    "    ([\" \"], \"단어 단위\")\n",
    "]\n",
    "\n",
    "for separators, description in separators_configs:\n",
    "    print(f\"\\n{description} separators={separators}:\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=40,\n",
    "        chunk_overlap=10,\n",
    "        separators=separators\n",
    "    )\n",
    "    chunks = splitter.split_text(test_text)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i}: '{chunk.strip()}'\")\n",
    "\n",
    "# ===========================================\n",
    "# chunk_size별 결과 비교\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"chunk_size별 분할 결과 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "chunk_sizes = [50, 100, 150]\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={size}:\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=20,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    print(f\"총 {len(chunks)}개 청크 생성\")\n",
    "    avg_length = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "    print(f\"평균 청크 길이: {avg_length:.1f}자\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i}: '{chunk.strip()[:30]}...' (길이: {len(chunk)}자)\")\n",
    "\n",
    "# ===========================================\n",
    "# chunk_overlap 효과 확인\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"chunk_overlap 효과 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overlap_values = [0, 10, 30]\n",
    "\n",
    "for overlap in overlap_values:\n",
    "    print(f\"\\nchunk_overlap={overlap}:\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=80,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \".\", \" \"]\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    print(f\"총 {len(chunks)}개 청크 생성\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i}: '{chunk.strip()}'\")\n",
    "        \n",
    "        # 중복 부분 확인\n",
    "        if i > 1 and overlap > 0:\n",
    "            prev_chunk = chunks[i-2].strip()\n",
    "            curr_chunk = chunk.strip()\n",
    "            # 간단한 중복 확인 (마지막 10자와 첫 10자 비교)\n",
    "            if len(prev_chunk) >= 10 and len(curr_chunk) >= 10:\n",
    "                prev_end = prev_chunk[-10:]\n",
    "                curr_start = curr_chunk[:10]\n",
    "                if any(word in curr_start for word in prev_end.split() if len(word) > 2):\n",
    "                    print(f\"    중복 감지: 이전 청크와 겹치는 부분 있음\")\n",
    "\n",
    "# ===========================================\n",
    "# 활용 가이드\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"실무 활용 가이드\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "RecursiveCharacterTextSplitter 사용 가이드:\n",
    "\n",
    "1. 기본 설정 (일반적 문서):\n",
    "   chunk_size=1000, chunk_overlap=200\n",
    "   separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "\n",
    "2. 한국어 문서 최적화:\n",
    "   chunk_size=500-1000, chunk_overlap=100-200\n",
    "   separators=[\"\\n\\n\", \"\\n\", \".\", \"。\", \" \"]\n",
    "\n",
    "3. 코드 문서:\n",
    "   separators=[\"\\n\\n\", \"\\n\", \"\\t\", \" \"]\n",
    "\n",
    "4. 대화/채팅 로그:\n",
    "   separators=[\"\\n\\n\", \"\\n\", \":\", \" \"]\n",
    "\n",
    "장점:\n",
    "- 의미 단위로 자연스러운 분할\n",
    "- 계층적 구분자로 최적화된 분할점 찾기\n",
    "- 텍스트 특성에 맞는 유연한 설정\n",
    "\n",
    "주의사항:\n",
    "- chunk_size는 LLM 토큰 제한 고려\n",
    "- chunk_overlap은 맥락 보존과 비용의 균형\n",
    "- separators 순서가 분할 품질 결정\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n프로그램 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9effbb93",
   "metadata": {},
   "source": [
    "### 3. TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d18c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 읽기 성공: ./data/ai-terminology.txt\n",
      "원본 텍스트 미리보기:\n",
      "--------------------------------------------------\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 ...\n",
      "\n",
      "전체 텍스트 길이: 3036자\n",
      "\n",
      "============================================================\n",
      "TokenTextSplitter 특징 및 다른 Splitter와 비교\n",
      "============================================================\n",
      "\n",
      "원본 텍스트의 토큰 개수: 2407개\n",
      "문자 대 토큰 비율: 1.26 (문자/토큰)\n",
      "\n",
      "1. TokenTextSplitter (토큰 기반 분할):\n",
      "----------------------------------------\n",
      "총 14개 청크 생성\n",
      "\n",
      "Chunk 1:\n",
      "  토큰 수: 200개\n",
      "  문자 수: 270자\n",
      "  내용: Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를...\n",
      "\n",
      "Chunk 2:\n",
      "  토큰 수: 200개\n",
      "  문자 수: 229자\n",
      "  내용: 성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 ...\n",
      "\n",
      "Chunk 3:\n",
      "  토큰 수: 200개\n",
      "  문자 수: 233자\n",
      "  내용: 까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "정의: 텍스트 데이터...\n",
      "\n",
      "2. RecursiveCharacterTextSplitter (문자 기반 분할):\n",
      "---------------------------------------------\n",
      "총 5개 청크 생성\n",
      "\n",
      "Chunk 1:\n",
      "  토큰 수: 556개\n",
      "  문자 수: 685자\n",
      "  내용: Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를...\n",
      "\n",
      "Chunk 2:\n",
      "  토큰 수: 608개\n",
      "  문자 수: 730자\n",
      "  내용: Transformer (트랜스포머)\n",
      "\n",
      "정의: 자연어 처리에서 사용되는 신경망 아키텍처로, 병렬 연산과 장기 의존성 처리가 강점.\n",
      "예시: GPT, BERT 등의 모델이 트랜스포머 기...\n",
      "\n",
      "Chunk 3:\n",
      "  토큰 수: 621개\n",
      "  문자 수: 794자\n",
      "  내용: Reinforcement Learning (강화 학습)\n",
      "\n",
      "정의: 보상을 극대화하는 방향으로 행동을 학습하는 AI 기법.\n",
      "예시: 알파고가 바둑에서 최적의 수를 찾기 위해 강화 학습을...\n",
      "\n",
      "============================================================\n",
      "토큰 기반 분할의 장점\n",
      "============================================================\n",
      "\n",
      "LLM 토큰 제한: 200개 토큰\n",
      "\n",
      "토큰 기반 분할 결과:\n",
      "  Chunk 1: 200개 토큰 OK\n",
      "  Chunk 2: 200개 토큰 OK\n",
      "  Chunk 3: 200개 토큰 OK\n",
      "  Chunk 4: 200개 토큰 OK\n",
      "  Chunk 5: 200개 토큰 OK\n",
      "  Chunk 6: 200개 토큰 OK\n",
      "  Chunk 7: 200개 토큰 OK\n",
      "  Chunk 8: 200개 토큰 OK\n",
      "  Chunk 9: 200개 토큰 OK\n",
      "  Chunk 10: 200개 토큰 OK\n",
      "  Chunk 11: 200개 토큰 OK\n",
      "  Chunk 12: 200개 토큰 OK\n",
      "  Chunk 13: 200개 토큰 OK\n",
      "  Chunk 14: 67개 토큰 OK\n",
      "\n",
      "토큰 제한 초과 청크: 0개\n",
      "\n",
      "문자 기반 분할 결과:\n",
      "  Chunk 1: 556개 토큰 초과\n",
      "  Chunk 2: 608개 토큰 초과\n",
      "  Chunk 3: 621개 토큰 초과\n",
      "  Chunk 4: 522개 토큰 초과\n",
      "  Chunk 5: 140개 토큰 OK\n",
      "\n",
      "토큰 제한 초과 청크: 4개\n",
      "\n",
      "============================================================\n",
      "다양한 인코딩 방식 비교\n",
      "============================================================\n",
      "\n",
      "cl100k_base (GPT-4, GPT-3.5-turbo):\n",
      "  토큰 수: 37개\n",
      "  토큰 예시: [31495, 230, 75265, 243, 92245, 0, 15592, 81673, 5251, 101]...\n",
      "\n",
      "p50k_base (GPT-3 (davinci)):\n",
      "  토큰 수: 69개\n",
      "  토큰 예시: [168, 243, 230, 167, 227, 243, 47991, 246, 168, 226]...\n",
      "\n",
      "r50k_base (GPT-3 (ada, babbage, curie)):\n",
      "  토큰 수: 69개\n",
      "  토큰 예시: [168, 243, 230, 167, 227, 243, 47991, 246, 168, 226]...\n",
      "\n",
      "============================================================\n",
      "실무 활용 예제\n",
      "============================================================\n",
      "\n",
      "chunk_size=100 토큰:\n",
      "  총 청크 수: 27개\n",
      "  평균 토큰 수: 98.8개\n",
      "  첫 번째 청크 토큰 수: 100개\n",
      "\n",
      "chunk_size=200 토큰:\n",
      "  총 청크 수: 14개\n",
      "  평균 토큰 수: 190.5개\n",
      "  첫 번째 청크 토큰 수: 200개\n",
      "\n",
      "chunk_size=500 토큰:\n",
      "  총 청크 수: 6개\n",
      "  평균 토큰 수: 442.8개\n",
      "  첫 번째 청크 토큰 수: 500개\n",
      "\n",
      "============================================================\n",
      "메타데이터 포함 Document 분할\n",
      "============================================================\n",
      "Document 분할 결과: 14개 청크\n",
      "\n",
      "Document Chunk 1:\n",
      "  토큰 수: 200개\n",
      "  메타데이터: {'source': 'ai-terminology.txt', 'type': 'glossary', 'start_index': -1}\n",
      "  내용: Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를...\n",
      "\n",
      "Document Chunk 2:\n",
      "  토큰 수: 200개\n",
      "  메타데이터: {'source': 'ai-terminology.txt', 'type': 'glossary', 'start_index': 252}\n",
      "  내용: 성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# 파일 읽기\n",
    "file_path = \"./data/ai-terminology.txt\"\n",
    "\n",
    "# 파일 읽기\n",
    "try:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        ai_terminology_text = f.read()\n",
    "    print(f\"파일 읽기 성공: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"파일 읽기 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"원본 텍스트 미리보기:\")\n",
    "print(\"-\" * 50)\n",
    "print(ai_terminology_text[:500] + \"...\")\n",
    "print(f\"\\n전체 텍스트 길이: {len(ai_terminology_text)}자\")\n",
    "\n",
    "# ==========================================\n",
    "# TokenTextSplitter vs 다른 Splitter 비교\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TokenTextSplitter 특징 및 다른 Splitter와 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 토큰 개수 확인 (tiktoken 사용)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens = encoding.encode(ai_terminology_text)\n",
    "print(f\"\\n원본 텍스트의 토큰 개수: {len(tokens)}개\")\n",
    "print(f\"문자 대 토큰 비율: {len(ai_terminology_text)/len(tokens):.2f} (문자/토큰)\")\n",
    "\n",
    "# 2. TokenTextSplitter 설정 및 실행\n",
    "print(\"\\n1. TokenTextSplitter (토큰 기반 분할):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "token_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,              # 토큰 개수로 크기 지정\n",
    "    chunk_overlap=20,            # 토큰 단위 중복\n",
    "    encoding_name=\"cl100k_base\", # OpenAI GPT 모델용 인코딩\n",
    "    add_start_index=True         # 시작 인덱스 정보 포함\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(ai_terminology_text)\n",
    "print(f\"총 {len(token_chunks)}개 청크 생성\")\n",
    "\n",
    "for i, chunk in enumerate(token_chunks[:3], 1):\n",
    "    chunk_tokens = encoding.encode(chunk)\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens)}개\")\n",
    "    print(f\"  문자 수: {len(chunk)}자\")\n",
    "    print(f\"  내용: {chunk[:100]}...\")\n",
    "\n",
    "# 3. 다른 Splitter와 비교\n",
    "print(\"\\n2. RecursiveCharacterTextSplitter (문자 기반 분할):\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # 문자 개수로 크기 지정 (토큰 200개 ≈ 문자 800개)\n",
    "    chunk_overlap=80,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(ai_terminology_text)\n",
    "print(f\"총 {len(char_chunks)}개 청크 생성\")\n",
    "\n",
    "for i, chunk in enumerate(char_chunks[:3], 1):\n",
    "    chunk_tokens = encoding.encode(chunk)\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens)}개\")\n",
    "    print(f\"  문자 수: {len(chunk)}자\")\n",
    "    print(f\"  내용: {chunk[:100]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 토큰 기반 분할의 장점 확인\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토큰 기반 분할의 장점\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LLM 토큰 제한 시뮬레이션\n",
    "max_tokens = 200  # 가상의 토큰 제한\n",
    "\n",
    "print(f\"\\nLLM 토큰 제한: {max_tokens}개 토큰\")\n",
    "print(\"\\n토큰 기반 분할 결과:\")\n",
    "over_limit_count = 0\n",
    "for i, chunk in enumerate(token_chunks, 1):\n",
    "    chunk_tokens = len(encoding.encode(chunk))\n",
    "    status = \"OK\" if chunk_tokens <= max_tokens else \"초과\"\n",
    "    if chunk_tokens > max_tokens:\n",
    "        over_limit_count += 1\n",
    "    print(f\"  Chunk {i}: {chunk_tokens}개 토큰 {status}\")\n",
    "\n",
    "print(f\"\\n토큰 제한 초과 청크: {over_limit_count}개\")\n",
    "\n",
    "print(\"\\n문자 기반 분할 결과:\")\n",
    "over_limit_count = 0\n",
    "for i, chunk in enumerate(char_chunks, 1):\n",
    "    chunk_tokens = len(encoding.encode(chunk))\n",
    "    status = \"OK\" if chunk_tokens <= max_tokens else \"초과\"\n",
    "    if chunk_tokens > max_tokens:\n",
    "        over_limit_count += 1\n",
    "    print(f\"  Chunk {i}: {chunk_tokens}개 토큰 {status}\")\n",
    "\n",
    "print(f\"\\n토큰 제한 초과 청크: {over_limit_count}개\")\n",
    "\n",
    "# ==========================================\n",
    "# 다양한 encoding 테스트\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"다양한 인코딩 방식 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encodings = [\n",
    "    (\"cl100k_base\", \"GPT-4, GPT-3.5-turbo\"),\n",
    "    (\"p50k_base\", \"GPT-3 (davinci)\"),\n",
    "    (\"r50k_base\", \"GPT-3 (ada, babbage, curie)\")\n",
    "]\n",
    "\n",
    "test_text = \"안녕하세요! AI와 머신러닝에 대해 배워보겠습니다. Hello, let's learn about AI and Machine Learning!\"\n",
    "\n",
    "for encoding_name, description in encodings:\n",
    "    enc = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = enc.encode(test_text)\n",
    "    print(f\"\\n{encoding_name} ({description}):\")\n",
    "    print(f\"  토큰 수: {len(tokens)}개\")\n",
    "    print(f\"  토큰 예시: {tokens[:10]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 활용 예제\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"실무 활용 예제\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 다양한 chunk_size로 테스트\n",
    "chunk_sizes = [100, 200, 500]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={chunk_size} 토큰:\")\n",
    "    splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//10,  # 10% 중복\n",
    "        encoding_name=\"cl100k_base\"\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(ai_terminology_text)\n",
    "    avg_tokens = sum(len(encoding.encode(chunk)) for chunk in chunks) / len(chunks)\n",
    "    \n",
    "    print(f\"  총 청크 수: {len(chunks)}개\")\n",
    "    print(f\"  평균 토큰 수: {avg_tokens:.1f}개\")\n",
    "    print(f\"  첫 번째 청크 토큰 수: {len(encoding.encode(chunks[0]))}개\")\n",
    "\n",
    "# ==========================================\n",
    "# 메타데이터 포함 문서 분할\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"메타데이터 포함 Document 분할\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Document 객체 생성\n",
    "document = Document(\n",
    "    page_content=ai_terminology_text,\n",
    "    metadata={\"source\": \"ai-terminology.txt\", \"type\": \"glossary\"}\n",
    ")\n",
    "\n",
    "# Document 분할\n",
    "doc_chunks = token_splitter.split_documents([document])\n",
    "\n",
    "print(f\"Document 분할 결과: {len(doc_chunks)}개 청크\")\n",
    "for i, doc_chunk in enumerate(doc_chunks[:2], 1):\n",
    "    tokens_count = len(encoding.encode(doc_chunk.page_content))\n",
    "    print(f\"\\nDocument Chunk {i}:\")\n",
    "    print(f\"  토큰 수: {tokens_count}개\")\n",
    "    print(f\"  메타데이터: {doc_chunk.metadata}\")\n",
    "    print(f\"  내용: {doc_chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f89d53",
   "metadata": {},
   "source": [
    "### 4. HugginFace Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc24f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.13\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 총 23개의 청크로 분할됨\n",
      "\n",
      " Chunk 1 (25자):\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      " Chunk 2 (157자):\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      " Chunk 3 (37자):\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      " Chunk 4 (176자):\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      " Chunk 5 (143자):\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "\n",
      " 첫 번째 청크의 토큰 개수: 23\n",
      " 첫 번째 청크의 토큰 리스트: ['Sem', 'antic', 'ĠSearch', 'Ġ(', 'ìĿ', 'ĺ', 'ë', '¯', '¸', 'ë', '¡', 'ł', 'ì', 'ł', 'ģ', 'Ġ', 'ê', '²', 'Ģ', 'ì']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"./data/ai-terminology.txt\"\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "#print(\" 원본 텍스트 미리보기:\\n\", file_content[:200])\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,  # 각 청크 크기 (토큰 기준 아님)\n",
    "    chunk_overlap=50,  # 청크 간 중복 부분\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "# 분할된 텍스트 출력\n",
    "print(f\"\\n 총 {len(split_texts)}개의 청크로 분할됨\\n\")\n",
    "for i, chunk in enumerate(split_texts[:5]):  # 처음 5개만 출력\n",
    "    print(f\" Chunk {i+1} ({len(chunk)}자):\\n{chunk}\\n\")\n",
    "\n",
    "# 토크나이저로 텍스트를 토큰 단위로 변환하여 확인\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts[0])\n",
    "print(f\"\\n 첫 번째 청크의 토큰 개수: {len(tokenized_example)}\")\n",
    "print(\" 첫 번째 청크의 토큰 리스트:\", tokenized_example[:20])  # 앞 20개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297720e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tiktoken\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# 파일 읽기\n",
    "with open(\"./data/ai-terminology.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일 내용을 읽어오기\n",
    "\n",
    "#print(\"원본 텍스트 미리보기:\\n\", file[:500])  # 앞 500자 출력\n",
    "\n",
    "# TokenTextSplitter 설정\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,  # 청크 크기\n",
    "    chunk_overlap=20,  # 청크 간 겹치는 부분 추가하여 문맥 유지\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI tiktoken 기본 인코딩 사용 (한글 처리 개선)\n",
    "    add_start_index=True  # 각 청크의 시작 인덱스 반환\n",
    ")\n",
    "\n",
    "# 텍스트 분할 실행\n",
    "texts = text_splitter.split_text(file)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n 총 {len(texts)}개의 청크로 분할됨.\")\n",
    "print(\"\\n 첫 번째 청크:\\n\", texts[0])\n",
    "\n",
    "# 청크 길이 확인\n",
    "for i, chunk in enumerate(texts[:5]):  # 처음 5개만 확인\n",
    "    print(f\"\\n Chunk {i+1} (길이: {len(chunk)}):\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68a19ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 토크나이저 로딩 중...\n",
      "토크나이저 로딩 완료\n",
      "파일 읽기 성공: ./data/ai-terminology.txt\n",
      "원본 텍스트 미리보기:\n",
      "--------------------------------------------------\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Faceboo...\n",
      "\n",
      "전체 텍스트 길이: 3036자\n",
      "\n",
      "============================================================\n",
      "토크나이저 동작 확인\n",
      "============================================================\n",
      "테스트 텍스트: RAG는 검색 기반의 텍스트 생성 모델입니다. Hello, AI!\n",
      "\n",
      "1. 토큰화 결과: ['RAG', 'ë', 'Ĭ', 'Ķ', 'Ġ', 'ê', '²', 'Ģ', 'ì', 'ĥ', 'ī', 'Ġ', 'ê', '¸', '°', 'ë', '°', 'ĺ', 'ìĿ', 'ĺ', 'Ġ', 'í', 'ħ', 'į', 'ì', 'Ĭ', '¤', 'í', 'Ĭ', '¸', 'Ġì', 'ĥ', 'Ŀ', 'ì', 'Ħ', '±', 'Ġë', 'ª', '¨', 'ë', 'į', '¸', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'ĠHello', ',', 'ĠAI', '!']\n",
      "   토큰 개수: 54개\n",
      "\n",
      "2. 인코딩 결과: [33202, 167, 232, 242, 220, 166, 110, 222, 168, 225, 231, 220, 166, 116, 108, 167, 108, 246, 35975, 246, 220, 169, 227, 235, 168, 232, 97, 169, 232, 116, 23821, 225, 251, 168, 226, 109, 31619, 103, 101, 167, 235, 116, 168, 252, 227, 46695, 230, 46695, 97, 13, 18435, 11, 9552, 0]\n",
      "   토큰 ID 개수: 54개\n",
      "\n",
      "3. 디코딩 결과: RAG는 검색 기반의 텍스트 생성 모델입니다. Hello, AI!\n",
      "\n",
      "4. 토큰별 ID 매핑:\n",
      "    1. 'RAG' -> 33202\n",
      "    2. 'ë' -> 167\n",
      "    3. 'Ĭ' -> 232\n",
      "    4. 'Ķ' -> 242\n",
      "    5. 'Ġ' -> 220\n",
      "    6. 'ê' -> 166\n",
      "    7. '²' -> 110\n",
      "    8. 'Ģ' -> 222\n",
      "    9. 'ì' -> 168\n",
      "   10. 'ĥ' -> 225\n",
      "   11. 'ī' -> 231\n",
      "   12. 'Ġ' -> 220\n",
      "   13. 'ê' -> 166\n",
      "   14. '¸' -> 116\n",
      "   15. '°' -> 108\n",
      "   16. 'ë' -> 167\n",
      "   17. '°' -> 108\n",
      "   18. 'ĺ' -> 246\n",
      "   19. 'ìĿ' -> 35975\n",
      "   20. 'ĺ' -> 246\n",
      "   21. 'Ġ' -> 220\n",
      "   22. 'í' -> 169\n",
      "   23. 'ħ' -> 227\n",
      "   24. 'į' -> 235\n",
      "   25. 'ì' -> 168\n",
      "   26. 'Ĭ' -> 232\n",
      "   27. '¤' -> 97\n",
      "   28. 'í' -> 169\n",
      "   29. 'Ĭ' -> 232\n",
      "   30. '¸' -> 116\n",
      "   31. 'Ġì' -> 23821\n",
      "   32. 'ĥ' -> 225\n",
      "   33. 'Ŀ' -> 251\n",
      "   34. 'ì' -> 168\n",
      "   35. 'Ħ' -> 226\n",
      "   36. '±' -> 109\n",
      "   37. 'Ġë' -> 31619\n",
      "   38. 'ª' -> 103\n",
      "   39. '¨' -> 101\n",
      "   40. 'ë' -> 167\n",
      "   41. 'į' -> 235\n",
      "   42. '¸' -> 116\n",
      "   43. 'ì' -> 168\n",
      "   44. 'ŀ' -> 252\n",
      "   45. 'ħ' -> 227\n",
      "   46. 'ëĭ' -> 46695\n",
      "   47. 'Ī' -> 230\n",
      "   48. 'ëĭ' -> 46695\n",
      "   49. '¤' -> 97\n",
      "   50. '.' -> 13\n",
      "   51. 'ĠHello' -> 18435\n",
      "   52. ',' -> 11\n",
      "   53. 'ĠAI' -> 9552\n",
      "   54. '!' -> 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "print(\"GPT-2 토크나이저 로딩 중...\")\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(\"토크나이저 로딩 완료\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"./data/ai-terminology.txt\"\n",
    "\n",
    "# 파일 읽기\n",
    "try:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        file_content = f.read()\n",
    "    print(f\"파일 읽기 성공: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"파일 읽기 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"원본 텍스트 미리보기:\")\n",
    "print(\"-\" * 50)\n",
    "print(file_content[:200] + \"...\")\n",
    "print(f\"\\n전체 텍스트 길이: {len(file_content)}자\")\n",
    "\n",
    "# ==========================================\n",
    "# 토크나이저 동작 확인\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토크나이저 동작 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 예제 텍스트로 토크나이저 테스트\n",
    "test_text = \"RAG는 검색 기반의 텍스트 생성 모델입니다. Hello, AI!\"\n",
    "print(f\"테스트 텍스트: {test_text}\")\n",
    "\n",
    "# 1. 토큰화 (tokenize)\n",
    "tokens = hf_tokenizer.tokenize(test_text)\n",
    "print(f\"\\n1. 토큰화 결과: {tokens}\")\n",
    "print(f\"   토큰 개수: {len(tokens)}개\")\n",
    "\n",
    "# 2. 인코딩 (encode) - 토큰을 ID로 변환\n",
    "token_ids = hf_tokenizer.encode(test_text)\n",
    "print(f\"\\n2. 인코딩 결과: {token_ids}\")\n",
    "print(f\"   토큰 ID 개수: {len(token_ids)}개\")\n",
    "\n",
    "# 3. 디코딩 (decode) - ID를 다시 텍스트로 변환\n",
    "decoded_text = hf_tokenizer.decode(token_ids)\n",
    "print(f\"\\n3. 디코딩 결과: {decoded_text}\")\n",
    "\n",
    "# 4. 개별 토큰 ID 확인\n",
    "print(f\"\\n4. 토큰별 ID 매핑:\")\n",
    "for i, (token, token_id) in enumerate(zip(tokens, token_ids), 1):\n",
    "    print(f\"   {i:2d}. '{token}' -> {token_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60d3da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n",
      "Created a chunk of size 321, which is longer than the specified 100\n",
      "Created a chunk of size 362, which is longer than the specified 100\n",
      "Created a chunk of size 237, which is longer than the specified 100\n",
      "Created a chunk of size 201, which is longer than the specified 100\n",
      "Created a chunk of size 226, which is longer than the specified 100\n",
      "Created a chunk of size 236, which is longer than the specified 100\n",
      "Created a chunk of size 180, which is longer than the specified 100\n",
      "Created a chunk of size 240, which is longer than the specified 100\n",
      "Created a chunk of size 226, which is longer than the specified 100\n",
      "Created a chunk of size 205, which is longer than the specified 100\n",
      "Created a chunk of size 202, which is longer than the specified 100\n",
      "Created a chunk of size 170, which is longer than the specified 100\n",
      "Created a chunk of size 193, which is longer than the specified 100\n",
      "Created a chunk of size 230, which is longer than the specified 100\n",
      "Created a chunk of size 199, which is longer than the specified 100\n",
      "Created a chunk of size 206, which is longer than the specified 100\n",
      "Created a chunk of size 185, which is longer than the specified 100\n",
      "Created a chunk of size 192, which is longer than the specified 100\n",
      "Created a chunk of size 207, which is longer than the specified 100\n",
      "Created a chunk of size 202, which is longer than the specified 100\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CharacterTextSplitter with HuggingFace 토크나이저\n",
      "============================================================\n",
      "총 23개의 청크로 분할됨\n",
      "\n",
      "처음 3개 청크 분석:\n",
      "\n",
      "Chunk 1:\n",
      "  문자 수: 25자\n",
      "  토큰 수: 23개\n",
      "  토큰 ID 수: 23개\n",
      "  내용 미리보기: Semantic Search (의미론적 검색)...\n",
      "\n",
      "Chunk 2:\n",
      "  문자 수: 157자\n",
      "  토큰 수: 321개\n",
      "  토큰 ID 수: 321개\n",
      "  내용 미리보기: 정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"...\n",
      "\n",
      "Chunk 3:\n",
      "  문자 수: 37자\n",
      "  토큰 수: 9개\n",
      "  토큰 ID 수: 9개\n",
      "  내용 미리보기: FAISS (Facebook AI Similarity Search)...\n",
      "\n",
      "============================================================\n",
      "다양한 chunk_size 설정 비교\n",
      "============================================================\n",
      "\n",
      "chunk_size=100 토큰:\n",
      "  총 청크 수: 42개\n",
      "  평균 토큰 수: 121.2개\n",
      "  전체 토큰 수: 5,090개\n",
      "  토큰 수 범위: 9~362개\n",
      "\n",
      "chunk_size=300 토큰:\n",
      "  총 청크 수: 23개\n",
      "  평균 토큰 수: 238.0개\n",
      "  전체 토큰 수: 5,475개\n",
      "  토큰 수 범위: 9~362개\n",
      "\n",
      "chunk_size=500 토큰:\n",
      "  총 청크 수: 13개\n",
      "  평균 토큰 수: 413.6개\n",
      "  전체 토큰 수: 5,377개\n",
      "  토큰 수 범위: 276~491개\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CharacterTextSplitter with HuggingFace 토크나이저\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CharacterTextSplitter with HuggingFace 토크나이저\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,     # 토큰 개수 기준\n",
    "    chunk_overlap=50,   # 토큰 단위 중복\n",
    "    separator=\"\\n\\n\"    # 구분자 설정\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "print(f\"총 {len(split_texts)}개의 청크로 분할됨\")\n",
    "print(\"\\n처음 3개 청크 분석:\")\n",
    "\n",
    "for i, chunk in enumerate(split_texts[:3], 1):\n",
    "    chunk_tokens = hf_tokenizer.tokenize(chunk)\n",
    "    chunk_token_ids = hf_tokenizer.encode(chunk)\n",
    "    \n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  문자 수: {len(chunk):,}자\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens):,}개\")\n",
    "    print(f\"  토큰 ID 수: {len(chunk_token_ids):,}개\")\n",
    "    print(f\"  내용 미리보기: {chunk[:100]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 다양한 chunk_size 비교\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"다양한 chunk_size 설정 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "chunk_sizes = [100, 300, 500]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={chunk_size} 토큰:\")\n",
    "    \n",
    "    splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        hf_tokenizer,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//10,  # 10% 중복\n",
    "        separator=\"\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(file_content)\n",
    "    \n",
    "    # 통계 계산\n",
    "    total_tokens = sum(len(hf_tokenizer.tokenize(chunk)) for chunk in chunks)\n",
    "    avg_tokens = total_tokens / len(chunks) if chunks else 0\n",
    "    \n",
    "    print(f\"  총 청크 수: {len(chunks)}개\")\n",
    "    print(f\"  평균 토큰 수: {avg_tokens:.1f}개\")\n",
    "    print(f\"  전체 토큰 수: {total_tokens:,}개\")\n",
    "    \n",
    "    # 토큰 수 분포 확인\n",
    "    token_counts = [len(hf_tokenizer.tokenize(chunk)) for chunk in chunks]\n",
    "    min_tokens = min(token_counts) if token_counts else 0\n",
    "    max_tokens = max(token_counts) if token_counts else 0\n",
    "    \n",
    "    print(f\"  토큰 수 범위: {min_tokens}~{max_tokens}개\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426e5fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "토크나이저별 비교\n",
      "============================================================\n",
      "비교 텍스트: RAG는 검색 기반의 텍스트 생성 모델입니다. AI와 머신러닝을 활용합니다.\n",
      "\n",
      "GPT-2 토크나이저:\n",
      "  토큰 수: 81개\n",
      "  토큰: ['RAG', 'ë', 'Ĭ', 'Ķ', 'Ġ', 'ê', '²', 'Ģ', 'ì', 'ĥ', 'ī', 'Ġ', 'ê', '¸', '°', 'ë', '°', 'ĺ', 'ìĿ', 'ĺ', 'Ġ', 'í', 'ħ', 'į', 'ì', 'Ĭ', '¤', 'í', 'Ĭ', '¸', 'Ġì', 'ĥ', 'Ŀ', 'ì', 'Ħ', '±', 'Ġë', 'ª', '¨', 'ë', 'į', '¸', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'ĠAI', 'ì', 'Ļ', 'Ģ', 'Ġë', '¨', '¸', 'ì', 'ĭ', 'ł', 'ë', 'Ł', '¬', 'ëĭ', 'Ŀ', 'ìĿ', 'Ħ', 'Ġ', 'í', 'Ļ', 'ľ', 'ì', 'ļ', '©', 'íķ', '©', 'ëĭ', 'Ī', 'ëĭ', '¤', '.']\n",
      "\n",
      "OpenAI 토크나이저 (cl100k_base):\n",
      "  토큰 수: 33개\n",
      "  토큰 ID: [49, 1929, 16969, 86422, 78326, 55216, 39277, 246, 21028, 10997, 45204, 54289, 53017, 55170, 69697, 116, 80052, 13, 15592, 81673, 5251, 101, 116, 83628, 61394, 9019, 251, 18359, 47932, 250, 27797, 61938, 13]\n",
      "\n",
      "토큰 수 차이: 48개\n",
      "\n",
      "============================================================\n",
      "한국어 처리 성능 테스트\n",
      "============================================================\n",
      "한국어 텍스트별 토큰화 결과:\n",
      "\n",
      "1. '안녕하세요'\n",
      "   토큰: ['ì', 'ķ', 'Ī', 'ë', 'ħ', 'ķ', 'íķ', 'ĺ', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ']\n",
      "   토큰 수: 14개\n",
      "   문자당 토큰: 2.80\n",
      "\n",
      "2. '인공지능과 머신러닝'\n",
      "   토큰: ['ìĿ', '¸', 'ê', '³', 'µ', 'ì', '§', 'Ģ', 'ë', 'Ĭ', '¥', 'ê', '³', '¼', 'Ġë', '¨', '¸', 'ì', 'ĭ', 'ł', 'ë', 'Ł', '¬', 'ëĭ', 'Ŀ']\n",
      "   토큰 수: 25개\n",
      "   문자당 토큰: 2.50\n",
      "\n",
      "3. '자연어 처리 기술'\n",
      "   토큰: ['ì', 'ŀ', 'Ĳ', 'ì', 'Ĺ', '°', 'ì', 'ĸ', '´', 'Ġì', '²', 'ĺ', 'ë', '¦', '¬', 'Ġ', 'ê', '¸', '°', 'ì', 'Ī', 'ł']\n",
      "   토큰 수: 22개\n",
      "   문자당 토큰: 2.44\n",
      "\n",
      "4. '딥러닝 모델 학습'\n",
      "   토큰: ['ë', 'Ķ', '¥', 'ë', 'Ł', '¬', 'ëĭ', 'Ŀ', 'Ġë', 'ª', '¨', 'ë', 'į', '¸', 'Ġ', 'íķ', 'Ļ', 'ì', 'Ĭ', 'µ']\n",
      "   토큰 수: 20개\n",
      "   문자당 토큰: 2.22\n",
      "\n",
      "5. '검색 기반 생성 모델'\n",
      "   토큰: ['ê', '²', 'Ģ', 'ì', 'ĥ', 'ī', 'Ġ', 'ê', '¸', '°', 'ë', '°', 'ĺ', 'Ġì', 'ĥ', 'Ŀ', 'ì', 'Ħ', '±', 'Ġë', 'ª', '¨', 'ë', 'į', '¸']\n",
      "   토큰 수: 25개\n",
      "   문자당 토큰: 2.27\n",
      "\n",
      "============================================================\n",
      "Document 객체와 함께 사용\n",
      "============================================================\n",
      "Document 분할 결과: 23개 청크\n",
      "\n",
      "Document Chunk 1:\n",
      "  토큰 수: 23개\n",
      "  메타데이터: {'source': 'ai-terminology.txt', 'type': 'glossary', 'tokenizer': 'gpt2'}\n",
      "  내용: Semantic Search (의미론적 검색)...\n",
      "\n",
      "Document Chunk 2:\n",
      "  토큰 수: 321개\n",
      "  메타데이터: {'source': 'ai-terminology.txt', 'type': 'glossary', 'tokenizer': 'gpt2'}\n",
      "  내용: 정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 토크나이저별 비교 (GPT-2 vs tiktoken)\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토크나이저별 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# tiktoken과 비교 (OpenAI 토크나이저)\n",
    "try:\n",
    "    # OpenAI 토크나이저\n",
    "    openai_encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # 같은 텍스트로 비교\n",
    "    comparison_text = \"RAG는 검색 기반의 텍스트 생성 모델입니다. AI와 머신러닝을 활용합니다.\"\n",
    "    \n",
    "    # GPT-2 토큰화\n",
    "    gpt2_tokens = hf_tokenizer.tokenize(comparison_text)\n",
    "    gpt2_token_count = len(gpt2_tokens)\n",
    "    \n",
    "    # OpenAI 토큰화\n",
    "    openai_tokens = openai_encoding.encode(comparison_text)\n",
    "    openai_token_count = len(openai_tokens)\n",
    "    \n",
    "    print(f\"비교 텍스트: {comparison_text}\")\n",
    "    print(f\"\\nGPT-2 토크나이저:\")\n",
    "    print(f\"  토큰 수: {gpt2_token_count}개\")\n",
    "    print(f\"  토큰: {gpt2_tokens}\")\n",
    "    \n",
    "    print(f\"\\nOpenAI 토크나이저 (cl100k_base):\")\n",
    "    print(f\"  토큰 수: {openai_token_count}개\")\n",
    "    print(f\"  토큰 ID: {openai_tokens}\")\n",
    "    \n",
    "    print(f\"\\n토큰 수 차이: {abs(gpt2_token_count - openai_token_count)}개\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"tiktoken이 설치되지 않음. GPT-2 토크나이저만 사용합니다.\")\n",
    "\n",
    "# ==========================================\n",
    "# 한국어 처리 성능 테스트\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"한국어 처리 성능 테스트\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "korean_texts = [\n",
    "    \"안녕하세요\",\n",
    "    \"인공지능과 머신러닝\",\n",
    "    \"자연어 처리 기술\",\n",
    "    \"딥러닝 모델 학습\",\n",
    "    \"검색 기반 생성 모델\"\n",
    "]\n",
    "\n",
    "print(\"한국어 텍스트별 토큰화 결과:\")\n",
    "for i, text in enumerate(korean_texts, 1):\n",
    "    tokens = hf_tokenizer.tokenize(text)\n",
    "    token_ids = hf_tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"\\n{i}. '{text}'\")\n",
    "    print(f\"   토큰: {tokens}\")\n",
    "    print(f\"   토큰 수: {len(tokens)}개\")\n",
    "    print(f\"   문자당 토큰: {len(tokens)/len(text):.2f}\")\n",
    "\n",
    "# ==========================================\n",
    "# Document 객체와 함께 사용\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Document 객체와 함께 사용\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Document 객체 생성\n",
    "document = Document(\n",
    "    page_content=file_content,\n",
    "    metadata={\n",
    "        \"source\": \"ai-terminology.txt\",\n",
    "        \"type\": \"glossary\",\n",
    "        \"tokenizer\": \"gpt2\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Document 분할\n",
    "doc_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_chunks = doc_splitter.split_documents([document])\n",
    "\n",
    "print(f\"Document 분할 결과: {len(doc_chunks)}개 청크\")\n",
    "\n",
    "for i, doc_chunk in enumerate(doc_chunks[:2], 1):\n",
    "    token_count = len(hf_tokenizer.tokenize(doc_chunk.page_content))\n",
    "    print(f\"\\nDocument Chunk {i}:\")\n",
    "    print(f\"  토큰 수: {token_count}개\")\n",
    "    print(f\"  메타데이터: {doc_chunk.metadata}\")\n",
    "    print(f\"  내용: {doc_chunk.page_content[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
