{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9eda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Okay, so I'm trying to understand what LangChain is. From the initial \"\n",
      " \"explanation, it seems like it's a tool used in programming or some sort of \"\n",
      " 'automation, but with a twist involving language modeling. \\n'\n",
      " '\\n'\n",
      " 'I remember that when you talk about tools that help automate tasks, they '\n",
      " 'often involve libraries, modules, or even frameworks. So maybe LangChain is '\n",
      " 'something similar but with a focus on handling human language and '\n",
      " 'understanding.\\n'\n",
      " '\\n'\n",
      " 'The initial answer mentioned that LangChain uses the \"langchain\" library, '\n",
      " 'which I believe is an open-source project for integrating language models '\n",
      " 'into applications. That makes sense because without a proper language model, '\n",
      " \"you can't really handle tasks related to natural language processing or text \"\n",
      " 'generation.\\n'\n",
      " '\\n'\n",
      " 'Let me think about how this would work in practice. If someone wants to '\n",
      " 'automate a task that involves understanding and responding to a '\n",
      " 'conversation, they might use LangChain with a language model provided by '\n",
      " '\"langchain\". The user would set up their application, connect the model, and '\n",
      " 'then feed it data or commands as input. \\n'\n",
      " '\\n'\n",
      " \"For example, if I wanted an app that translates languages, I'd probably \"\n",
      " 'build it using LangChain. I could choose a specific model like BPE or '\n",
      " \"GPT-3.1B, which are popular because they're widely used and can handle \"\n",
      " 'different types of text tasks.\\n'\n",
      " '\\n'\n",
      " 'But wait, how does the communication happen between the model and the app? '\n",
      " 'Does the app send the data to the model, process it, and then return a '\n",
      " 'response back to the user? That sounds logical. So in my translation '\n",
      " \"example, I'd input some text into the app, it would analyze it using the \"\n",
      " 'language model, understand the context, and translate it appropriately.\\n'\n",
      " '\\n'\n",
      " 'I also wonder about the limitations or dependencies of LangChain. It needs '\n",
      " \"access to the chosen language model's parameters, which can be set when \"\n",
      " 'installing it. There might be constraints on how much data it can handle at '\n",
      " \"once or whether certain tasks require specific configurations that aren't \"\n",
      " 'flexible enough for all use cases.\\n'\n",
      " '\\n'\n",
      " 'Another thing is integration with other tools and frameworks. For instance, '\n",
      " 'if I wanted to use a web framework like Flask with my LangChain app, would I '\n",
      " 'need to integrate external libraries like FastAPI? Or could I just include '\n",
      " 'the necessary code directly into my project?\\n'\n",
      " '\\n'\n",
      " \"I'm curious about how efficient LangChain's performance is compared to other \"\n",
      " 'libraries for language modeling. I know that some frameworks might have '\n",
      " 'better community support or more resources available, but maybe \"langchain\" '\n",
      " 'is simpler and easier to get started with if someone just wants basic tasks '\n",
      " 'involving natural language processing.\\n'\n",
      " '\\n'\n",
      " 'Also, considering the future of this tool, are there plans to improve it in '\n",
      " 'the near future? Like adding new models, better data handling methods, or '\n",
      " 'enhanced integration capabilities with other technologies?\\n'\n",
      " '\\n'\n",
      " 'Overall, I think LangChain is a solid starting point for someone looking to '\n",
      " 'leverage language modeling in their applications. It seems versatile enough '\n",
      " 'to handle a variety of tasks, from simple translations and text '\n",
      " 'summarization to more complex interactions where understanding context is '\n",
      " 'crucial.\\n'\n",
      " '\\n'\n",
      " 'I should also check if there are any tutorials or documentation available on '\n",
      " 'the \"langchain\" project\\'s website or community forums. That would give me a '\n",
      " 'clearer idea of how to set up and use this tool effectively. Additionally, '\n",
      " 'perhaps looking at case studies where others have successfully used '\n",
      " 'LangChain could provide practical insights into its application across '\n",
      " 'different industries.\\n'\n",
      " '\\n'\n",
      " \"One more thing I'm thinking about is the potential for integrating with \"\n",
      " 'other AI frameworks or tools. If LangChain can work seamlessly with '\n",
      " 'something like TensorFlow or PyTorch, that would make it even more powerful '\n",
      " \"for building complex models. But right now, from what I know, it's focused \"\n",
      " 'on language modeling tasks without integration points to other machine '\n",
      " 'learning environments.\\n'\n",
      " '\\n'\n",
      " 'In summary, LangChain appears to be a good tool for anyone who wants to '\n",
      " 'automate tasks involving natural language processing, especially when they '\n",
      " \"can't afford the cost or complexity of using more advanced frameworks. It \"\n",
      " 'provides a straightforward setup and flexible options for different types of '\n",
      " 'applications.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a powerful and versatile tool designed for automating tasks '\n",
      " \"that involve natural language processing (NLP). Here's an organized overview \"\n",
      " 'of its key aspects:\\n'\n",
      " '\\n'\n",
      " '1. **Objective**: LangChain aims to simplify the process of applying '\n",
      " 'language models to various NLP tasks, making it accessible to users without '\n",
      " 'extensive technical expertise.\\n'\n",
      " '\\n'\n",
      " '2. **Foundation**: It leverages the \"langchain\" library, which provides a '\n",
      " 'robust implementation for integrating language models into applications. '\n",
      " 'This library is open-source and widely used in both research and industry.\\n'\n",
      " '\\n'\n",
      " '3. **Functionality**:\\n'\n",
      " '   - **Integration with Language Models**: LangChain requires users to '\n",
      " 'select a specific language model (e.g., BPE, GPT-3.1B) through configuration '\n",
      " 'when installing the \"langchain\" package.\\n'\n",
      " '   - **Data Handling**: The tool can handle various NLP tasks such as text '\n",
      " 'translation, summarization, and generation, allowing users to input data or '\n",
      " 'commands directly into the application.\\n'\n",
      " '\\n'\n",
      " '4. **User Interaction**:\\n'\n",
      " '   - Users set up their applications by connecting a language model instance '\n",
      " 'to an app.\\n'\n",
      " '   - Data is processed through the model, context understanding, and '\n",
      " 'response generation occur seamlessly between the model and the user.\\n'\n",
      " '\\n'\n",
      " '5. **Limitations and Considerations**:\\n'\n",
      " '   - **Integration Challenges**: While integration with web frameworks like '\n",
      " 'Flask can be achieved via external libraries, there may be constraints on '\n",
      " 'data handling or flexibility in certain use cases.\\n'\n",
      " '   - **Community Support**: The \"langchain\" project is known for its '\n",
      " 'simplicity and ease of setup, though users might need additional resources '\n",
      " 'for more complex tasks.\\n'\n",
      " '\\n'\n",
      " '6. **Future Enhancements**: Plans for future improvements could include '\n",
      " 'broader model support, enhanced data handling methods, and better '\n",
      " 'integration with other AI frameworks like TensorFlow or PyTorch.\\n'\n",
      " '\\n'\n",
      " '7. **Practical Applications**:\\n'\n",
      " '   - **Translation Tools**: Utilizing \"langchain\" can lead to efficient '\n",
      " 'translation services.\\n'\n",
      " '   - **NLP Tasks**: Beyond translation, LangChain supports tasks like text '\n",
      " 'summarization and question answering.\\n'\n",
      " '\\n'\n",
      " \"8. **Learning Resources**: While the project's documentation isn't \"\n",
      " 'extensively available, community tutorials and case studies provide '\n",
      " \"practical insights into LangChain's usage across various industries.\\n\"\n",
      " '\\n'\n",
      " 'In conclusion, LangChain is a suitable starting point for users looking to '\n",
      " 'leverage language modeling in their applications. It offers simplicity and '\n",
      " 'flexibility, making it ideal for tasks that require natural language '\n",
      " 'processing without requiring advanced integration capabilities.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ deepseek-r1:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0860666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, and they want the answer in Korean. Let me start by recalling the basic definition of Python. Python is a programming language, right? It's known for being easy to learn and versatile. I should mention that it was created by Guido van Rossum in 1991. \n",
      "\n",
      "Wait, the user might be a beginner, so I should explain it in simple terms. Maybe start with the key points: it's a high-level language, interpreted, open-source, and used for web development, data analysis, automation, etc. Also, mention that it's widely used in various fields like AI, machine learning, and scientific computing.\n",
      "\n",
      "I should check if there are any important features or benefits to highlight. Oh, right, the \"Pythonic\" way of writing code, the extensive library ecosystem, and how it's beginner-friendly. Maybe include some examples, like how it's used in projects or popular frameworks.\n",
      "\n",
      "Wait, the user might not know the full scope, so I should balance between the basics and some applications. Avoid technical jargon unless necessary. Also, make sure the answer is clear and not too lengthy. Let me structure it with a definition, key features, and applications. Check for any typos or mistakes. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ê³ ê¸‰ ìˆ˜ì¤€ì˜ ì½”ë“œë¥¼ ì‰½ê²Œ ì‘ì„±í•  ìˆ˜ ìˆëŠ” **ê°„ê²°í•˜ê³  ì§ê´€ì ì¸ ì–¸ì–´**ì…ë‹ˆë‹¤. 1991ë…„ì— í”„ë¡œì íŠ¸ íŒ€ìœ¼ë¡œë¶€í„° Guido van Rossumì— ì˜í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "**íŠ¹ì§•**:  \n",
      "1. **ê°„ê²°í•œ ë¬¸ë²•**: ë³µì¡í•œ êµ¬ë¬¸ì´ ì—†ì–´ ì‹ ì… ê°œë°œìë„ ì‰½ê²Œ ë°°ì›Œì§‘ë‹ˆë‹¤.  \n",
      "2. **ê³ ì† ì‹¤í–‰**: ì½”ë“œ ì‹¤í–‰ ì†ë„ê°€ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í”„ë¡œê·¸ë˜ë°ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.  \n",
      "3. **ë‹¤ì–‘í•œ í™œìš© ë¶„ì•¼**: ì›¹ ê°œë°œ, ë°ì´í„° ë¶„ì„, ìë™í™”, AI, ë¨¸ì‹ ëŸ¬ë‹, ê³¼í•™ ê³„ì‚° ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
      "4. **ê°œë°©ì  ì†Œí”„íŠ¸ì›¨ì–´**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì œê³µë˜ì–´ ëˆ„êµ¬ë‚˜ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "**ì‚¬ìš© ì˜ˆ**:  \n",
      "- ì›¹ì‚¬ì´íŠ¸ ê°œë°œ (Django, Flask)  \n",
      "- ë°ì´í„° ë¶„ì„ (Pandas, NumPy)  \n",
      "- ìë™í™” (ì˜ˆ: ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±)  \n",
      "- AI ë° ë¨¸ì‹ ëŸ¬ë‹ (TensorFlow, PyTorch)  \n",
      "\n",
      "íŒŒì´ì¬ì€ **\"Pythonic\"** ë°©ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆì–´, í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ qwen2.5:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "#llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "#qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? í•œê¸€ë¡œ ë‹µë³€í•´ ì¤˜\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f5fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To determine which number is larger, I'll compare the whole numbers first.\n",
      "\n",
      "Both numbers have a whole number part of 9, so they are equal in that regard.\n",
      "\n",
      "Next, I'll examine the decimal parts:\n",
      "- The tenths place: 9.9 has a 9, while 9.11 has a 1.\n",
      "Since 9 is greater than 1, 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Compare the Whole Numbers\n",
      "Both numbers have the same whole number part:\n",
      "- **9**\n",
      "\n",
      "Since they are equal, we move to the decimal parts.\n",
      "\n",
      "### Step 2: Compare the Tenths Place\n",
      "- **9.9** has a **9** in the tenths place.\n",
      "- **9.11** has a **1** in the tenths place.\n",
      "\n",
      "**Conclusion:**  \n",
      "The tenths place of **9.9** (which is **9**) is greater than that of **9.11** (which is **1**).\n",
      "\n",
      "### Final Answer\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174fec31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger, I'll compare the whole numbers first.\n",
       "\n",
       "Both numbers have a whole number part of 9, so they are equal in that regard.\n",
       "\n",
       "Next, I'll examine the decimal parts:\n",
       "- The tenths place: 9.9 has a 9, while 9.11 has a 1.\n",
       "Since 9 is greater than 1, 9.9 is larger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Compare the Whole Numbers\n",
       "Both numbers have the same whole number part:\n",
       "- **9**\n",
       "\n",
       "Since they are equal, we move to the decimal parts.\n",
       "\n",
       "### Step 2: Compare the Tenths Place\n",
       "- **9.9** has a **9** in the tenths place.\n",
       "- **9.11** has a **1** in the tenths place.\n",
       "\n",
       "**Conclusion:**  \n",
       "The tenths place of **9.9** (which is **9**) is greater than that of **9.11** (which is **1**).\n",
       "\n",
       "### Final Answer\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0af4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have the same whole number part, which is 9. Then the decimal parts are 0.9 and 0.11. \n",
      "\n",
      "Hmm, I need to compare the decimal parts. Let's break it down. The first number is 9.9, which is 9 + 0.9. The second number is 9.11, which is 9 + 0.11. So the difference between the two decimal parts is 0.9 versus 0.11. \n",
      "\n",
      "Wait, 0.9 is larger than 0.11 because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So even though the whole numbers are the same, the decimal part of 9.9 is bigger. Therefore, 9.9 is larger than 9.11. \n",
      "\n",
      "But let me double-check to make sure I didn't make a mistake. Sometimes when comparing decimals, people might confuse the places. Let's write them out:\n",
      "\n",
      "9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place first: 9 vs 1. Since 9 is larger than 1, 9.90 is larger than 9.11. Yeah, that makes sense. So even if the second number has more decimal places, the tenths place is already different. \n",
      "\n",
      "Another way to think about it: 9.9 is 9.90, and 9.11 is 9.11. If you line them up:\n",
      "\n",
      "9.90\n",
      "9.11\n",
      "\n",
      "Comparing digit by digit from the left:\n",
      "\n",
      "- The first digit after the decimal is 9 vs 1. Since 9 is greater than 1, the first number is larger. \n",
      "\n",
      "So definitely, 9.9 is bigger than 9.11. I don't think there's any other way to interpret this. The whole numbers are the same, but the decimal parts are different. The decimal part of 9.9 is larger, so the entire number is larger. \n",
      "\n",
      "I guess that's it. The answer should be 9.9.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "\n",
      "**ì´ìœ :**  \n",
      "- ë‘ ìˆ˜ ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.  \n",
      "- 9.9ì˜ ì†Œìˆ˜ ë¶€ë¶„ì€ 0.9, 9.11ì˜ ì†Œìˆ˜ ë¶€ë¶„ì€ 0.11ì…ë‹ˆë‹¤.  \n",
      "- 0.9ëŠ” 0.11ë³´ë‹¤ í¬ë¯€ë¡œ, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
      "\n",
      "ë”°ë¼ì„œ, **9.9**ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "#model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3623342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4cdc8",
   "metadata": {},
   "source": [
    "### LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ DeepSeek ëª¨ë¸(ì¶”ë¡ )ê³¼ Qwen ëª¨ë¸(í•œê¸€ì‘ë‹µ)ì„ ì—°ë™í•˜ê¸°\n",
    "* poetry add langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f146748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.5\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\\n\\n        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\\n        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\\n        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\\n        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\\n\\n        ì§€ì¹¨:\\n        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\\n        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\\n        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\\n        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\\n\\n        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        ì§ˆë¬¸: {question}\\n        ì¶”ë¡ : {thinking}\\n        '), additional_kwargs={})]\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. Both numbers are decimals, right? So, the whole numbers are the same, 9. Then the decimal parts. \n",
      "\n",
      "Wait, 9.9 is the same as 9.90 when you add a zero. So comparing 9.90 and 9.11. Let's break it down digit by digit. The units place is 9 in both. Then the tenths place: 9 vs 1. Oh, right, 9 is greater than 1, so even though the second number has a 1 in the tenths place, the first one has a 9. So 9.9 is larger.\n",
      "\n",
      "I need to make sure I didn't mix up the places. The tenths are the first decimal place, so 9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place, 9 vs 1, so 9.9 is bigger. Yeah, that makes sense. So the answer is 9.9.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ«ìëŠ” ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•˜ë‚˜, ì†Œìˆ˜ì  ì•„ë˜ì˜ ìˆ«ìë¥¼ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¾¸ë©´ 9.90, 9.11ì„ ë¹„êµí•˜ë©´ 9.90ì´ 9.11ë³´ë‹¤ ë” ì»¤ìš”.  \n",
      "ë”°ë¼ì„œ 9.9ê°€ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\n",
      "{'question': '9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\\n\\nNow, comparing each digit from left to right:\\n\\n- The units place for both numbers is 9.\\n- In the tenths place, 9 has a 9 and 9.11 has a 1.\\n  \\nSince 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': \"<think>\\nOkay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. Both numbers are decimals, right? So, the whole numbers are the same, 9. Then the decimal parts. \\n\\nWait, 9.9 is the same as 9.90 when you add a zero. So comparing 9.90 and 9.11. Let's break it down digit by digit. The units place is 9 in both. Then the tenths place: 9 vs 1. Oh, right, 9 is greater than 1, so even though the second number has a 1 in the tenths place, the first one has a 9. So 9.9 is larger.\\n\\nI need to make sure I didn't mix up the places. The tenths are the first decimal place, so 9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place, 9 vs 1, so 9.9 is bigger. Yeah, that makes sense. So the answer is 9.9.\\n</think>\\n\\n9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \\në‘ ìˆ«ìëŠ” ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•˜ë‚˜, ì†Œìˆ˜ì  ì•„ë˜ì˜ ìˆ«ìë¥¼ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \\n9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¾¸ë©´ 9.90, 9.11ì„ ë¹„êµí•˜ë©´ 9.90ì´ 9.11ë³´ë‹¤ ë” ì»¤ìš”.  \\në”°ë¼ì„œ 9.9ê°€ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\"}\n",
      "==> ìƒì„±ëœ ë‹µë³€: \n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. Both numbers are decimals, right? So, the whole numbers are the same, 9. Then the decimal parts. \n",
      "\n",
      "Wait, 9.9 is the same as 9.90 when you add a zero. So comparing 9.90 and 9.11. Let's break it down digit by digit. The units place is 9 in both. Then the tenths place: 9 vs 1. Oh, right, 9 is greater than 1, so even though the second number has a 1 in the tenths place, the first one has a 9. So 9.9 is larger.\n",
      "\n",
      "I need to make sure I didn't mix up the places. The tenths are the first decimal place, so 9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place, 9 vs 1, so 9.9 is bigger. Yeah, that makes sense. So the answer is 9.9.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ«ìëŠ” ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•˜ë‚˜, ì†Œìˆ˜ì  ì•„ë˜ì˜ ìˆ«ìë¥¼ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¾¸ë©´ 9.90, 9.11ì„ ë¹„êµí•˜ë©´ 9.90ì´ 9.11ë³´ë‹¤ ë” ì»¤ìš”.  \n",
      "ë”°ë¼ì„œ 9.9ê°€ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë¸\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "#ì‘ë‹µ ëª¨ë¸ (í•œê¸€ì²˜ë¦¬ ê°€ëŠ¥)\n",
    "#generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "#qwen3:1.7b\n",
    "print(generation_model)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        ì§ˆë¬¸: {question}\n",
    "        ì¶”ë¡ : {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n",
    "\n",
    "#LangGraphì—ì„œ State ì‚¬ìš©ìì •ì˜ í´ë˜ìŠ¤ëŠ” ë…¸ë“œ ê°„ì˜ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” í‹€ì…ë‹ˆë‹¤. \n",
    "#ë…¸ë“œ ê°„ì— ê³„ì† ì „ë‹¬í•˜ê³  ì‹¶ê±°ë‚˜, ê·¸ë˜í”„ ë‚´ì—ì„œ ìœ ì§€í•´ì•¼ í•  ì •ë³´ë¥¼ ë¯¸ë¦¬ ì •ì˜í™ë‹ˆë‹¤. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "#DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwenë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„°\n",
    "inputs = {\"question\": \"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"}\n",
    "\n",
    "# invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ í˜¸ì¶œ\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"==> ìƒì„±ëœ ë‹µë³€: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a6834b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9a58b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "To make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. This means converting 9.9 into 9.90.\n",
      "\n",
      "Now that both numbers have two decimal places, I can directly compare them digit by digit from left to right.\n",
      "\n",
      "Starting with the units place: Both numbers have a 9 in the units place, so they are equal there.\n",
      "\n",
      "Next, looking at the tenths place: The first number has a 9, and the second number has a 1. Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. So first, I need to compare these two numbers. \n",
      "\n",
      "I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Then, moving to the decimal part. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \n",
      "\n",
      "So, breaking it down: the first number has 9 in the tenths place, and the second has 1. Since 9 is greater than 1, even though the second number has more decimal places, the tenths place is where the difference lies. \n",
      "\n",
      "Wait, but the second number is 9.11, which is 9.11. So, 9.90 is 9.90, which is more than 9.11. So the answer is 9.9 is bigger. \n",
      "\n",
      "I should make sure there's no trick here. Both numbers are positive, and the tenths place is where the difference is. The first number's tenths digit is 9, the second is 1. So yeah, 9.9 is larger. \n",
      "\n",
      "I think that's it. The key was to convert them to the same decimal places and compare digit by digit.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ë™ì¼í•˜ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¿”ì„œ 9.90ê³¼ 9.11ì„ ë¹„êµí•˜ë©´, ì†Œìˆ˜ì  ì²« ë²ˆì§¸ ìë¦¬ì—ì„œ 9ê°€ 1ë³´ë‹¤ í° ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚˜ë¯€ë¡œ, **9.90 > 9.11**ì…ë‹ˆë‹¤.  \n",
      "ë”°ë¼ì„œ, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. So first, I need to compare these two numbers. \n",
      "\n",
      "I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Then, moving to the decimal part. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \n",
      "\n",
      "So, breaking it down: the first number has 9 in the tenths place, and the second has 1. Since 9 is greater than 1, even though the second number has more decimal places, the tenths place is where the difference lies. \n",
      "\n",
      "Wait, but the second number is 9.11, which is 9.11. So, 9.90 is 9.90, which is more than 9.11. So the answer is 9.9 is bigger. \n",
      "\n",
      "I should make sure there's no trick here. Both numbers are positive, and the tenths place is where the difference is. The first number's tenths digit is 9, the second is 1. So yeah, 9.9 is larger. \n",
      "\n",
      "I think that's it. The key was to convert them to the same decimal places and compare digit by digit.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ë™ì¼í•˜ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ë°”ê¿”ì„œ 9.90ê³¼ 9.11ì„ ë¹„êµí•˜ë©´, ì†Œìˆ˜ì  ì²« ë²ˆì§¸ ìë¦¬ì—ì„œ 9ê°€ 1ë³´ë‹¤ í° ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚˜ë¯€ë¡œ, **9.90 > 9.11**ì…ë‹ˆë‹¤.  \n",
      "ë”°ë¼ì„œ, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cec5a4",
   "metadata": {},
   "source": [
    "### 2ê°œì˜ ëª¨ë¸ì„ ì—°ë™í•œ ì½”ë“œë¥¼ Gradio ë¥¼ ì‚¬ìš©í•˜ì—¬ UIë¡œ ì‹¤í–‰í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f03026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ì…ë ¥ ì§ˆë¬¸: 9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\n",
      "[DEBUG] ì§ˆë¬¸ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: 461\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: 9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: 461\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: 237\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: 9.9ê³¼ 9.11 ì¤‘ì— ë” í° ìˆ˜ëŠ” 9.9ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë¨¼ì € ë‘ ìˆ«ìë¥¼ ë¹„êµí•˜ê² ìŠµë‹ˆë‹¤: 9.9ì™€ 9.11ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë‘ ìˆ«ì ëª¨ë‘ ëª‡ ê°€ì§€ ë¶€ìœ„ê°€ ë™ì¼í•˜ê²Œ 9ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ì œ í¬ì†Œìˆ˜ ìë¦¬ì—ì„œ ê° ìë¦¿ìˆ˜ë¥¼ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤:\n",
      "\n",
      "- ìœ ë‹ˆí‹° í”Œë«ì€ ë‘ ìˆ«ì ëª¨ë‘ 9ì…ë‹ˆë‹¤.\n",
      "- ì‹­ ë¶„ì˜ í•˜ë‚˜ ìë¦¬ì—ì„œëŠ” 9.11ì— 1ì´ ë” í¬ë‹¤.\n",
      "\n",
      "ë”°ë¼ì„œ, 9.90ì€ 9.11ë³´ë‹¤ í° ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ë”°ë¼ì„œ 9.9ëŠ” 9.11ë³´ë‹¤ í¬ê²Œ í•©ë‹ˆë‹¤.\n",
      "[DEBUG] ì…ë ¥ ì§ˆë¬¸: Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "[DEBUG] ì§ˆë¬¸ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: 9\n",
      "[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: 9\n",
      "[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: <class 'str'>\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: 290\n",
      "[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: Pythonì€ í•˜ë‚˜ì˜ ì–¸ì–´ì…ë‹ˆë‹¤. ì´ëŠ” ì—¬ëŸ¬ ê°€ì§€ ì´ìœ ë¡œ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "1. Pythonì€ ë†’ì€ ê°€ë…ì„±ê³¼ í¸í–¥ì„±ì„ ìë‘í•©ë‹ˆë‹¤.\n",
      "2. ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ëª¨ë²” ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "3. ê°œë°œìë“¤ì´ ì‰½ê²Œ ë°°ìš¸ ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë˜í•œ, Pythonì´ ëŒ€ì¤‘ì ì¸ ì–¸ì–´ë¡œ ì•Œë ¤ì§€ë©´ì„œ ì—¬ëŸ¬ ê°€ì§€ ì´ìœ ë¡œ ì‚¬ìš©ë˜ë©°:\n",
      "- ì›¹ ê°œë°œ\n",
      "- ì•± ë§Œë“¤ê¸°\n",
      "- ë°ì´í„° ë¶„ì„ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.\n",
      "- êµìœ¡ ë¶„ì•¼ì— ìˆì–´ì„œë„ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. \n",
      "\n",
      "ë”°ë¼ì„œ, Pythonì€ ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë° íƒì›”í•œ ì–¸ì–´ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì • (Jupyter ë…¸íŠ¸ë¶ í˜¸í™˜)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter í™˜ê²½ì—ì„œëŠ” reconfigure ëŒ€ì‹  í™˜ê²½ë³€ìˆ˜ë¡œ ì²˜ë¦¬\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter ë…¸íŠ¸ë¶ì´ë‚˜ ë‹¤ë¥¸ í™˜ê²½ì—ì„œëŠ” íŒ¨ìŠ¤\n",
    "    pass\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •: ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ê³¼ ë‹µë³€ ìƒì„±ì„ ìˆ˜í–‰\n",
    "# - reasoning_model: ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë‚®ìŒ, ì •í™•í•œ ë¶„ì„ìš©)\n",
    "# - generation_model: ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë†’ìŒ, ì°½ì˜ì  ì‘ë‹µìš©)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen2.5:1.5b\", \n",
    "    #model=\"qwen3:1.7b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ìƒíƒœ(State) ì •ì˜: ê·¸ë˜í”„ì—ì„œ ìƒíƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ë°ì´í„° êµ¬ì¡°\n",
    "class State(TypedDict):\n",
    "    question: str   # ì‚¬ìš©ìì˜ ì§ˆë¬¸\n",
    "    thinking: str   # ì¶”ë¡  ê²°ê³¼\n",
    "    answer: str     # ìµœì¢… ë‹µë³€\n",
    "\n",
    "# ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \n",
    "        ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ í•œêµ­ì–´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "        \n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ì¤‘ìš”: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"ì§ˆë¬¸: {question}\n",
    "        \n",
    "        ì¶”ë¡  ê³¼ì •: {thinking}\n",
    "        \n",
    "        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"ë¬¸ìì—´ì´ UTF-8ë¡œ ì œëŒ€ë¡œ ì¸ì½”ë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë³€í™˜\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # ë¬¸ìì—´ì´ì§€ë§Œ ì¸ì½”ë”© ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # ë¬¸ìì—´ì„ UTF-8ë¡œ ì¸ì½”ë”©í–ˆë‹¤ê°€ ë‹¤ì‹œ ë””ì½”ë”©í•˜ì—¬ ì •ë¦¬\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] ì…ë ¥ ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] ì§ˆë¬¸ íƒ€ì…: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    #thinking_content = ensure_utf8_string(response.content)\n",
    "    thinking_content = response.content\n",
    "    \n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5ë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±\n",
    "def generate(state: State):\n",
    "    # question = ensure_utf8_string(state[\"question\"])\n",
    "    # thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    thinking = state[\"thinking\"]\n",
    "    \n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    #answer_content = ensure_utf8_string(response.content)\n",
    "    answer_content = response.content\n",
    "    \n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜: ìƒíƒœ(State) ê°„ì˜ íë¦„ì„ ì •ì˜\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "# def launch_gradio():\n",
    "#     iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "#     iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n",
    "    #launch_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
